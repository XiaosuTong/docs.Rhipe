<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>RHIPE: Nuggets</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <link href="assets/bootstrap/css/bootstrap.css" rel="stylesheet">
    <link href="assets/custom/custom.css" rel="stylesheet">
    <!-- font-awesome -->
    <link href="assets/font-awesome/css/font-awesome.min.css" rel="stylesheet">

    <!-- prism -->
    <link href="assets/prism/prism.css" rel="stylesheet">
    <link href="assets/prism/prism.r.css" rel="stylesheet">
    <script type='text/javascript' src='assets/prism/prism.js'></script>
    <script type='text/javascript' src='assets/prism/prism.r.js'></script>
    
    
    
    <script type="text/javascript" src="assets/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
   MathJax.Hub.Config({    
     extensions: ["tex2jax.js"],    
     "HTML-CSS": { scale: 100}    
   });
   </script>
    
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="js/html5shiv.js"></script>
    <![endif]-->
    
    <link href='http://fonts.googleapis.com/css?family=Lato' rel='stylesheet' type='text/css'>
    <!-- <link href='http://fonts.googleapis.com/css?family=Lustria' rel='stylesheet' type='text/css'> -->
    <link href='http://fonts.googleapis.com/css?family=Bitter' rel='stylesheet' type='text/css'>
    

    <!-- Fav and touch icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="ico/apple-touch-icon-114-precomposed.png">
      <link rel="apple-touch-icon-precomposed" sizes="72x72" href="ico/apple-touch-icon-72-precomposed.png">
                    <link rel="apple-touch-icon-precomposed" href="ico/apple-touch-icon-57-precomposed.png">
                                   <!-- <link rel="shortcut icon" href="ico/favicon.png"> -->
  </head>

  <body>

    <div class="container-narrow">

      <div class="masthead">
        <ul class="nav nav-pills pull-right">
           
        </ul>
        <p class="myHeader">RHIPE: Nuggets</p>
      </div>

      <hr>

<div class="container-fluid">
   <div class="row-fluid">
   
   <div class="col-md-3 well">
   <ul class = "nav nav-list" id="toc">
   <li class='nav-header unselectable' data-edit-href='01.intro.Rmd'>Introduction</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#outline-and-reference'>Outline and Reference</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#background'>Background</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#getting-started'>Getting Started</a>
      </li>


<li class='nav-header unselectable' data-edit-href='02.simpleexample.Rmd'>Simple Example</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#key-value-paris-in-rhipe'>Key-value paris in RHIPE</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#without-reduce-function'>Without reduce function</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#with-reduce-function'>With reduce function</a>
      </li>


<li class='nav-header unselectable' data-edit-href='03.basicsimulation.Rmd'>Basic Simulation with Same Parameters</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#problem'>Problem</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#simple-r-version'>Simple R version</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#rhipe-version'>RHIPE version</a>
      </li>


<li class='nav-header unselectable' data-edit-href='04.pRNG.Rmd'>Random Number Generating</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#to-be-added'>To be added...</a>
      </li>


<li class='nav-header unselectable' data-edit-href='05.advancedsimulation.Rmd'>Advanced Simulation with Different Parameters</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#necessity-of-rhipe'>Necessity of RHIPE</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#simulation-example-ar2'>Simulation Example: AR(2)</a>
      </li>


<li class='nav-header unselectable' data-edit-href='06.writedata.Rmd'>Generating and Writing Data into HDFS</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#generating-data-r-vs-hdfs'>Generating Data: R vs. HDFS</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#generating-and-writing-data-into-hdfs'>Generating and Writing Data into HDFS</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#observing-the-location-of-the-hdfs-data'>Observing the Location of the HDFS Data</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#reading-the-data-stored-in-the-hdfs-into-your-local-environment'>Reading the Data Stored in the HDFS into your Local Environment</a>
      </li>


<li class='nav-header unselectable' data-edit-href='07.shared.Rmd'>Shared Objects</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#to-be-added'>To be added...</a>
      </li>


<li class='nav-header unselectable' data-edit-href='08.timing.Rmd'>Elapsed Timing Experiment</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#the-problem-description-'>The Problem Description </a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#data-structure'>Data Structure</a>
      </li>


<li class='nav-header unselectable' data-edit-href='09.readstructured.Rmd'>Reading Structured Data </li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#structured-data'>Structured Data</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#copying-the-data-to-the-hdfs'>Copying the Data to the HDFS</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#converting-to-r-objects'>Converting to R Objects</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#example-set-up-for-counter'>Example set up for counter</a>
      </li>


<li class='nav-header unselectable' data-edit-href='10.transformtext.Rmd'>Manipulating Text Data</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#subsetting-data-under-construction'>Subsetting Data (Under Construction)</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#transforming-data-under-construction'>Transforming Data (Under Construction)</a>
      </li>


<li class='nav-header unselectable' data-edit-href='11.combiner.Rmd'>Using a Combiner</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#the-combiner'>The Combiner</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#airline-data-overall-distance-by-carrier'>Airline Data: Overall Distance by Carrier</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#airline-data-average-distance-per-flight'>Airline Data: Average Distance per Flight</a>
      </li>


<li class='nav-header unselectable' data-edit-href='12.counters.Rmd'>Using a Counter</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#function-rhcounter'>Function rhcounter()</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#airline-data-download-status'>Airline Data: Download Status</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#airline-data-delay-rate'>Airline Data: Delay Rate</a>
      </li>


<li class='nav-header unselectable' data-edit-href='13.mapfile.Rmd'>Map Files for Queryable Database</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#to-be-added'>To be added...</a>
      </li>


<li class='nav-header unselectable' data-edit-href='14.wordcount.Rmd'>Word Count Example</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#the-problem'>The Problem</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#preparation'>Preparation</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#entirety'>Entirety</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#map'>Map</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#reduce'>Reduce</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#execution-function'>Execution Function</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#combiner'>Combiner</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#multiple-input-files'>Multiple input files</a>
      </li>

   </ul>
   </div>

<div class="col-md-9 tab-content" id="main-content">

<div class='tab-pane active' id='outline-and-reference'>
<h3>Outline and Reference</h3>

<p>This tutorial covers examples of how to use R package <code>Rhipe</code>.</p>

<h4>Outline</h4>

<ul>
<li>First, we </li>
<li>Next, we </li>
<li>Then we </li>
<li>We also provide R source files for all of the examples throughout the documentation.</li>
</ul>

<h4>Reference</h4>

<p>References:</p>

<ul>
<li><a href="http://datadr.org">datadr.org</a>: Divide and Recombine (D&amp;R) with `Rhipe</li>
<li><a href="http://github.com/saptarshiguha/RHIPE">RHIPE</a>: the engine that makes D&amp;R work for large datasets</li>
<li><a href="http://github.com/hafen/datadr">datadr</a>: R package providing the D&amp;R framework</li>
<li><a href="http://github.com/hafen/trelliscope">trelliscope</a>: the visualization companion to <code>datadr</code></li>
<li><a href="http://onlinelibrary.wiley.com/doi/10.1002/sta4.7/full">Large complex data: divide and recombine (D&amp;R) with RHIPE. <em>Stat</em>, 1(1), 53-67</a></li>
</ul>

</div>


<div class='tab-pane' id='background'>
<h3>Background</h3>

<!--
#### Hadoop 
Hadoop is an open source programming framework for distributed computing with massive data sets 
using a cluster of networked computers. It has changed the way many web companies work, bringing 
cluster computing to people with little knowledge of the intricacies of concurrent/distributed 
programming. Part of the reason for its success is that it has a fixed programming paradigm. It 
somewhat restricts what the user can parallelize but once an algorithm has been written the 
‘MapReduce way’, concurrency and distribution over a cluster comes for free.

It consists of two components: the Hadoop Distributed Filesystem and Hadoop MapReduce. They are 
based on the Google Filesystem and Google MapReduce respectively. Companies using these include 
Amazon, Ebay, New York Times, Facebook to name a few. 

#### Hadoop Distributed Filesystem

The Hadoop Distributed Filesystem (HDFS) sits on top of the file system of a computer (called the 
local filesystem). It pools the hard drive space of a cluster or heterogenous computers (e.g. 
different hardware and operating systems) and provides a unified view to the user. For example, with
 a cluster of 10 computers each with 1TB hard drive space available to Hadoop, the HDFS provides a 
user 10 TB of hard drive space. A single file can be bigger than maximum size on the local filesystem 
e.g. 2TB files can be saved on the HDFS. The HDFS is catered to large files and high throughput reads.
 Appends to files are not allowed. Files written to the HDFS are chunked into blocks, each block is 
replicated and saved on different cluster computers. This provides a measure of safety in case of 
transient or permanent computer failures. When a file is written to the HDFS, the client contacts the 
Namenode, a computer that serves as the gateway to the HDFS. It also performs a lot of administrative 
tasks, such as saving the mapping between a file and the location of its block across the cluster and
 so on. The Namenode tells the client which Datanodes (the computers that make up the HDFS) to store 
the data onto. It also tells the client which Datanodes to read the data from when a read request is 
performed. 
#### Hadoop MapReduce

Concurrent programming is difficult to get right. As Herb Sutter put it:

    ... humans are quickly overwhelmed by concurrency and find it much more difficult
        to reason about concurrent than sequential code.

A statistician attempting concurrent programming needs to be aware of race conditions, deadlocks and 
tools to prevent this: locks, semaphores, and mutually exclusive regions etc. An approach suggested by 
Sutter et al (Software and the concurrency revolution, H. Sutter and J. Larus, ACM Queue, Volume 3,
Number 7 2005) is to provide programming models not functions that force the programmer to approach 
her algorithms differently. Once the programmer constructs the algorithm using this model, concurrency 
comes for free. The MapReduce programming model is one example. Correctly coded Condor DAGS are another 
example.

MapReduce (MapReduce: Simplified Data Processing on Large Clusters, Jeffrey Dean and Sanjay Ghemawat,
*Communications of the ACM*, 2008) consists of several embarrassingly parallel splits which are 
evaluated in parallel. This is called the Map. There is a synchronization guard where intermediate 
data created at the end of the Map is exchanged between nodes and another round of parallel computing
 starts, called the Reduce phase. In effect large scale simulation trials in which the programmer 
launches several thousands of independent computations is an example of a Map. Retrieving and collating 
the results (usually done in the R console) is an example of a manual reduce.

In detail, the input to a MapReduce computation is a set of N key,value pairs. The N pairs are 
partitioned into S arbitrary splits. Each split is a unit of computation and is assigned to one 
computing unit on the cluster. Thus the processing of the S splits occurs in parallel. Each split is
processed by a user given function M, that takes a sequence of input key,value pairs and outputs 
(one or many) intermediate key,value pairs. The Hadoop framework will partition the intermediate 
values by the intermediate key. That is intermediate values sharing the same intermediate key are 
grouped together. Once the map is complete, the if there are M distinct intermediate keys, a user 
given function R, will be given an intermediate key and all intermediate values associated with the 
same key. Each processing core is assigned a subset of intermediate keys to reduce and the reduction 
of the M intermediate keys occurs in parallel. The function R, takes an intermediate key, a stream of 
associated intermediate values and returns a final key,value pair or pairs.

The R programmer has used MapReduce ideas. For example, the tapply command splits a vector by a list
 of factors. This the map equivalent: each row of the vector is the value and the keys are the distinct
 levels of the list of factors. The reduce is the user given function applied to the partitions of the 
vector. The xyplot function in lattice takes a formula e.g. $F\sim Y|A*B$, subsets the the data frame by 
the cartesian product of the levels of A and B (the map) and displays each subset (the reduce). Hadoop 
MapReduce generalizes this to a distributed level.
-->

<h4>RHIPE</h4>

<p>RHIPE is the R and Hadoop Integrated Programming Environment. It provides a way to execute Hadoop MapReduce
 jobs completely from within R and with R data structures.</p>

</div>


<div class='tab-pane' id='getting-started'>
<h3>Getting Started</h3>

<p>This tutorial assumes RHIPE has been installed on the server.
Please refer to <a href="http://www.datadr.org/install.html">Install RHIPE</a> for installation guide.</p>

<p>We can load and initialize the package:</p>

<pre><code class="r">library(Rhipe)
rhinit()
</code></pre>

<p>and we are ready to go.</p>

</div>


<div class='tab-pane' id='key-value-paris-in-rhipe'>
<h3>Key-value paris in RHIPE</h3>

<h4>Key and value</h4>

<p>In <code>RHIPE</code>, key/value pairs are R lists with two elements, one for the key and on for the value. If
we have a data frame object in R which we would like to analyze using <code>RHIPE</code>, we have to first 
convert it to be a list object.  As an example, consider the iris data set, which consists of 
measurements of 4 aspects for 50 flowers from each of 3 species of iris. Suppose we would like to 
split the sepal measurements of the <code>iris</code> data into key-value pairs by species:</p>

<pre><code class="r"># create by-species key-value pairs
irisKV &lt;- list(
   list(&quot;setosa&quot;, subset(iris, Species == &quot;setosa&quot;)[,c(1:2, 5)]),
   list(&quot;versicolor&quot;, subset(iris, Species == &quot;versicolor&quot;)[,c(1:2, 5)]),
   list(&quot;virginica&quot;, subset(iris, Species == &quot;virginica&quot;)[,c(1:2, 5)])
)
str(irisKV)
</code></pre>

<pre><code>List of 3
 $ :List of 2
  ..$ : chr &quot;setosa&quot;
  ..$ :&#39;data.frame&#39;:    50 obs. of  3 variables:
  .. ..$ Sepal.Length: num [1:50] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
  .. ..$ Sepal.Width : num [1:50] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
  .. ..$ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
 $ :List of 2
  ..$ : chr &quot;versicolor&quot;
  ..$ :&#39;data.frame&#39;:    50 obs. of  3 variables:
  .. ..$ Sepal.Length: num [1:50] 7 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 ...
  .. ..$ Sepal.Width : num [1:50] 3.2 3.2 3.1 2.3 2.8 2.8 3.3 2.4 2.9 2.7 ...
  .. ..$ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 2 2 2 2 2 2 2 2 2 2 ...
 $ :List of 2
  ..$ : chr &quot;virginica&quot;
  ..$ :&#39;data.frame&#39;:    50 obs. of  3 variables:
  .. ..$ Sepal.Length: num [1:50] 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 ...
  .. ..$ Sepal.Width : num [1:50] 3.3 2.7 3 2.9 3 3 2.5 2.9 2.5 3.6 ...
  .. ..$ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 3 3 3 3 3 3 3 3 3 3 ...
</code></pre>

<p>The result is a list of 3 key-value pairs. We chose the species to be the key and the corresponding
data frame of sepal measurements to be the value for each pair.</p>

<h4>Write key-vakue pairs onto HDFS</h4>

<p>We use <code>rhwrite()</code> to write R object onto HDFS.</p>

<pre><code class="r">rhwrite(irisKV, file = &quot;/tmp/iris/irisbyspecies&quot;)
rhls(&quot;/tmp/iris/irisbyspecies&quot;)
</code></pre>

<pre><code>  permission owner      group     size          modtime
1 -rw-r--r-- tongx supergroup  3.44 kb 2014-06-21 10:28
                                file
1 /tmp/keyvalue/irisbyspecies/part_1
</code></pre>

<p>It is possible that when we are writing R object, like a data frame, we only would like to divide 
the data frame to subsets with given number of rows. In this case, the keys are <code>NULL</code></p>

<pre><code class="r">rhwrite(iris, file = &quot;/tmp/keyvalue/irisbyrow&quot;, chunk = 1, numfiles = 3, kvpairs = FALSE)
rhls(&quot;/tmp/iris/irisbyrow&quot;)
</code></pre>

<pre><code>  permission owner      group     size          modtime
1 -rw-r--r-- tongx supergroup 13.71 kb 2014-06-21 10:49
2 -rw-r--r-- tongx supergroup 13.74 kb 2014-06-21 10:49
3 -rw-r--r-- tongx supergroup 13.75 kb 2014-06-21 10:49
                            file
1 /tmp/iris/irisbyrow/part_1
2 /tmp/iris/irisbyrow/part_2
3 /tmp/iris/irisbyrow/part_3
</code></pre>

<p>The reason that we have three files under <code>/tmp/iris/irisbyrow</code> is becasue we specify the <code>numfiles</code>
argument in <code>rhwrite()</code> to be 3.</p>

<pre><code class="r">byrow &lt;- rhread(&quot;/tmp/iris/irisbyrow&quot;)
head(byrow, 3)
</code></pre>

<pre><code>[[1]]
[[1]][[1]]
NULL

[[1]][[2]]
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa


[[2]]
[[2]][[1]]
NULL

[[2]][[2]]
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
2          4.9           3          1.4         0.2  setosa


[[3]]
[[3]][[1]]
NULL

[[3]][[2]]
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
3          4.7         3.2          1.3         0.2  setosa
</code></pre>

<p>We can see that every key is <code>NA</code> and every value is one row of the original data frame since we 
specify <code>chunk</code> argument to be 1, which is the default. </p>

</div>


<div class='tab-pane' id='without-reduce-function'>
<h3>Without reduce function</h3>

<p>First example will be about the keys are levels of <code>species</code> and values are corresponding data frame
of that species. For each species, we want to get the mean of <code>Sepal.Length</code> and <code>Sepal.Width</code>. Only 
map function is needed in this example.</p>

<pre><code class="r">map &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    Sepal.Mean &lt;- c(mean(map.values[[r]]$Sepal.Length), mean(map.values[[r]]$Sepal.Width))
    rhcollect(map.keys[[r]], Sepal.Mean)
  })
})
mr &lt;- rhwatch(
  map      = map,
  input    = rhfmt(&quot;/tmp/iris/irisbyspecies&quot;, type = &quot;sequence&quot;),
  output   = rhfmt(&quot;/tmp/iris/output/species.sepalmean&quot;, type = &quot;sequence&quot;),
  mapred   = list( mapred.reduce.tasks = 1 ),
  readback = FALSE
)
rst1 &lt;- rhread(&quot;/tmp/iris/output/species.sepalmean&quot;)
rst1 &lt;- data.frame(
  cbind(do.call(&quot;rbind&quot;, lapply(rst1, &quot;[[&quot;, 2)), unlist(lapply(rst1, &quot;[[&quot;, 1))), 
  stringsAsFactors = FALSE
)
names(rst1) &lt;- c(&quot;Sepal.length&quot;, &quot;Sepal.width&quot;, &quot;species&quot;)
rst1
</code></pre>

<pre><code>  Sepal.length Sepal.width    species
1        5.006       3.428     setosa
2        6.588       2.974  virginica
3        5.936        2.77 versicolor
</code></pre>

<p>The key/value pairs read in from input file will be saved as map.keys and map.values respectively.
Map.keys and map.values are two lists which are consist of all keys and all values that will be 
excuted in one task at one monment respectively. In map expression, we iterate over all key/value 
pairs. The length of map.keys and map.values are the same as the total number of key/value pairs,
which here is three here. map.keys[[r]] and map.values[[r]] is the r&#39;th key/value pair. For each 
map.values[[r]] which is the sub data frame for rth species, we calculate the mean of <code>Sepal.Length</code>
and <code>Sepal.Width</code>, save it as a numeric vector named as <code>Sepal.Mean</code>.</p>

</div>


<div class='tab-pane' id='with-reduce-function'>
<h3>With reduce function</h3>

<p>In the second example, we are going to access the <code>/tmp/iris/irisbyrow</code> file. In this file, keys are
all <code>NA</code> and values are each row of the <code>iris</code> data frame. In our map function, we have to collect
the new key which will be the species for each row, and value will be kept as the same. In reduce
function, we collect all rows that share same species and calculate the mean of <code>Sepal.Length</code> and 
<code>Sepal.Width</code>.</p>

<pre><code class="r">map &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    key &lt;- as.character(map.values[[r]]$Species)
    value &lt;- map.values[[r]][, 1:(ncol(map.values[[r]]) - 1)]
    rhcollect(key, value)
  })
})
reduce &lt;- expression(
  pre = {
    Length.sum = 0
    count = 0
    Width.sum = 0
  }, 
  reduce = {
    Length.sum = Length.sum + sum(sapply(reduce.values, &quot;[[&quot;, 1), na.rm = TRUE)
    count = count + length(reduce.values)
    Width.sum = Width.sum + sum(sapply(reduce.values, &quot;[[&quot;, 2), na.rm = TRUE)
  },
  post = {
    rhcollect(reduce.key, c(Length.sum/count, Width.sum/count))
  }
)
mr &lt;- rhwatch(
  map      = map,
  reduce   = reduce,
  input    = rhfmt(&quot;/tmp/iris/irisbyrow&quot;, type = &quot;sequence&quot;),
  output   = rhfmt(&quot;/tmp/iris/output/row.sepalmean&quot;, type = &quot;sequence&quot;),
  mapred   = list( mapred.reduce.tasks = 1 ),
  readback = FALSE
)
rst2 &lt;- rhread(&quot;/tmp/iris/output/row.sepalmean&quot;)
rst2 &lt;- data.frame(
  cbind(do.call(&quot;rbind&quot;, lapply(rst2, &quot;[[&quot;, 2)), unlist(lapply(rst2, &quot;[[&quot;, 1))), 
  stringsAsFactors = FALSE
)
names(rst2) &lt;- c(&quot;Sepal.length&quot;, &quot;Sepal.width&quot;, &quot;species&quot;)
identical(rst1, rst2)
</code></pre>

<pre><code>[1] TRUE
</code></pre>

<p>As shown by <code>identical()</code> function, the results from the first and second examples are the same.</p>

</div>


<div class='tab-pane' id='problem'>
<h3>Problem</h3>

<p>Suppose we are trying to generate data from a regression model. Suppose we have \(p\)-dimensional covariate vectors, and we are trying to simulate data based on the model \[ y = \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \cdots + \beta_p x_p + \epsilon \] 
Here \(y\) is the response variable, \(x=(x_1,x_2,x_3,\cdots,x_p)\) is the covariate vector and \(\epsilon\) is the error term normally distributed with mean \(0\) and variance \(1\). The \(p\) dimentional parameter for this model is \(\beta=(\beta_1,\beta_2,\beta_3,\cdots,\beta_p)\). We also generate \(x\) and \(\beta\) from \(N(0,100I_p)\). (We take \(p=10\) and we generate \(1000\) samples.)</p>

</div>


<div class='tab-pane' id='simple-r-version'>
<h3>Simple R version</h3>

<p>The simple R code to simulate for this model is </p>

<pre><code class="r">p &lt;- 10
n &lt;- 1000
beta &lt;- rnorm(p,0,100)
X &lt;- matrix(rnorm(n*p, 0, 100), ncol=n)
y &lt;- as.vector(beta%*%X)+rnorm(1)
</code></pre>

<p>The output will look like:( we are showing first 6 elements )</p>

<pre><code>&gt; beta
 [1]   56.353203    2.338886   61.136072  -58.929674 -153.136136  135.898400   79.834114  -27.213545   16.975711   -4.642392
&gt; X[1:10,1:6]
            [,1]       [,2]       [,3]       [,4]       [,5]      [,6]
 [1,]   75.95717  73.404384   59.01405   43.37698 251.675558 109.56923
 [2,]  -95.63060  42.738375   19.25537 -128.41214 127.471096 105.07078
 [3,] -273.95999 174.380333  112.85614  -12.75805  23.876184  34.18660
 [4,]  223.67577 -67.363519  -17.24695  -46.33213 -78.648826 -77.38899
 [5,]  -59.44221   5.706950   84.99961   25.47501 204.197139 -73.35753
 [6,]  -35.25866  -9.774056  -11.83463 -198.79612 -54.875868 -80.11310
 [7,]  -55.57189  31.650452  213.17751 -104.13461 -19.660030  71.25627
 [8,]  -83.30740 -52.625794  -25.58250   78.62923 -40.380024 -67.60982
 [9,] -127.69558   5.512040 -107.32493  127.12900  -6.894852  77.64534
[10,]   21.87018  28.493423  -15.57081 -214.57355   2.937460 -84.90639
&gt; head(y)
[1] -26000.50  20585.43  12627.44 -34121.54 -18753.27  22658.41
&gt; 
</code></pre>

</div>


<div class='tab-pane' id='rhipe-version'>
<h3>RHIPE version</h3>

<p>The entire code of RHIPE for word count is:</p>

<pre><code class="r">account &lt;- &quot;chakrav0&quot;
N &lt;- 1000
R &lt;- 10
p &lt;- 10
beta &lt;- rnorm(p,0,100)

##Generating Data
map1 &lt;- expression({
  lapply(seq_along(map.keys), function(r){
    set.seed(100)
        n &lt;- N/R #number of rows for each subset
        X &lt;- matrix(rnorm(p*n,0,100), nrow=p) 
        y &lt;- beta%*%X+rnorm(n)
        value &lt;- rbind(X,y)
        rhcollect(r, value)
        rm(value)
    })
})
job1 &lt;- rhwatch(
    map = map1,
    input = R,
    output = rhfmt(file.path(&quot;/ln&quot;, account, &quot;Bas_Sim&quot;, &quot;data&quot;), type=&quot;sequence&quot;),
    jobname = &quot;Generating Data&quot;,
    readback = FALSE,
    parameters = list(N = N, R = R, p = p),
    noeval = TRUE
)
ex = rhex(job1, async=FALSE)

##read result from HDFS
result &lt;- rhread(file.path(&quot;/ln&quot;, account, &quot;Bas_sim&quot;, &quot;data&quot;))
result
</code></pre>

<p>The mapreduce job in RHIPE is consist of a <code>map</code> expression, an optional <code>reduce</code> expression, and a execution funtion 
<code>rhwatch()</code>.</p>

</div>


<div class='tab-pane' id='to-be-added'>
<h3>To be added...</h3>

</div>


<div class='tab-pane' id='necessity-of-rhipe'>
<h3>Necessity of RHIPE</h3>

<p>The goal of statistics is to utilize data to make inferences about a scientific question of interest. To make a problem tractable, assumptions are made and in most scientific cases, appropriate parametric assumptions apply. This can reduce the scope of the problem to a handful of summary statistics gleaned from the data. However, this is not the case of every model. Some models require access to all of the data values for the most accurate conclusions.</p>

<p>For problems in Big Data, this is where R Hadoop Integrated Programming Environment (RHIPE) is absolutely critical. Although placing all the data in a single location to be utilized as necessary is not possible, the Hadoop platform still allows us access to all of the data broken up into subsets across multiple nodes. In this format, analysis on each subset of the data to be combined is still better than not being able to utilize all of the data.</p>

</div>


<div class='tab-pane' id='simulation-example-ar2'>
<h3>Simulation Example: AR(2)</h3>

<p>An example of such a case is estimating the parameter(s) to an Auto Regressive Integrated Moving Average (ARIMA) Model to Time Series Data. For the sake of simplicity, but still abiding by the section title of different parameters, I will use RHIPE to estimate an AR(2) (2 AR parameter terms) Model through simulation.</p>

<p>The RHIPE code needed to run this simulation is not all that much more extensive than the R code necessary to run this simulation. The reason for RHIPE is one of computational storage limitations and speed improvements through parallelization.</p>

<pre><code class="r">N.exp &lt;- 20
N &lt;- 2^N.exp
m.exp &lt;- 10
m &lt;- 2^m.exp
w.vec &lt;- 2*pi*0:(m - 1)/m
rho.true &lt;- c(2/3, -1/3)
rho.init &lt;- c(0, 0)
ginv &lt;- function(lambda, rho1, rho2) 1 + rho1^2 + rho2^2 - 2*rho1*(1 - rho2)*cos(lambda) - 2*rho2*cos(2*lambda))
map &lt;- expression({
  for(i in seq_along(map.values)){
    AR2 &lt;- arima.sim(n = m, model = list(ar = rho.true))
    fft &lt;- fft(AR2)
    per &lt;- Re(fft)^2 + Im(fft)^2
    lW &lt;- function(rho1, rho2) m*log(sum(ginv(w.vec, rho1, rho2)*per)/m) + sum(log(1/g(w.vec, rho1, rho2)))
    rho.est &lt;- optim(rho.init, lW)
    #With a simulation of total size of m we would be done here excluding the map &amp; for lines.
    rhcollect(1, rho.est)
  }
})
reduce &lt;- expression(
  pre &lt;- {x &lt;- 0}
  reduce &lt;- {x &lt;- x + app}y(unlist(reduce.values), 2, mean)}
  post &lt;- {rhcollect(1, x)}
  )
mr &lt;- rhwatch(
  map      = map,
  reduce   = reduce,
  input    = c(N/m, 10),
  output   = &quot;/tmp/advsim/rho.est&quot;,
  mapred   = list(mapred.reduce.tasks = 1),
  readback = FALSE
  )
</code></pre>

<p>The reduce expression will further reduce final data storage, as a simple averaging of the parameter estimates over the subsets will provide the best estimate over the entire data set provided our data access constraint due to the Central Limit Theorem (CLT). If data storage restraints did not exist, the R numerical optimization method <code>optim(&#39;initial parameter estimate&#39;, function)</code> over the entire data set is more likely to be the better estimate. However, the mean over all the subsets of the data still abides by a Normal Distribution with mean the actual parameter value(s) and variance scaling down by a multiplicative factor of \(r^{-1}, r = \frac{N}{m}\) being the number of subsets. Thus, provided the computational difficulties, RHIPE provides a more than satisfactory solution in thei advanced simulation with different parameters example as well as numerous other cases.</p>

</div>


<div class='tab-pane' id='generating-data-r-vs-hdfs'>
<h3>Generating Data: R vs. HDFS</h3>

<p>Generating random numbers from essentially every distribution imaginable in R is typically a cinch with simple calls, like <code>Z &lt;- rnorm(2^10)</code>. However, in the HaDoop File System (HDFS), a call of that nature will not have the generated data reach the HDFS and will instead have it sit in your local environment (whereever that may happen to be when you open R). Not to worry though, it is not too extensive to get randomly generated data into the HDFS.</p>

</div>


<div class='tab-pane' id='generating-and-writing-data-into-hdfs'>
<h3>Generating and Writing Data into HDFS</h3>

<p>The main purpose of the HDFS is to handle data sizes that are too large for your local environment. Thus, we are going to generate <code>m &lt;- 2^10</code> random standard normal values as hinted at in the previous subsection. However, we are not only going to do this once, but <code>2^10 = 1,024</code> times for a total of <code>N &lt;- 2^20</code> random standard normal values.</p>

<p>This should take up a little more than 8 Mega bytes (Mbs) of space, a fairly large local environment storage quantity merely for a collection of random numbers. Further, the code below will do this in the simplest way possible with the caveat of a few additional intelligent input variations from the default to improve the efficiency and plyability of the code to altering parameters.</p>

<pre><code class="r">N &lt;- 2^20
map &lt;- expression({
  m &lt;- 2^10
  for(i in seq_along(map.values)){
    Z &lt;- rnorm(m)
    rhcollect(map.keys[[i]], Z)
  }
})
mr &lt;- rhwatch(
  map      = map,
  input    = c(N/m, 10), 
  output   = paste(&quot;/tmp/rnorm/Nexp&quot;, log2(N), &quot;mexp&quot;, log2(m), sep=&quot;&quot;),
  mapred   = list(mapred.reduce.tasks = 0),
  readback = FALSE
  )
</code></pre>

<p>Note, the few, but subtle differences when transitioning from generating data in R vs. HDFS. We need to keep track of the map, key pair mechanism with commands such as <code>seq_along(map.values)</code>, <code>rhcollect(map.keys[[i]], Z)</code>, and <code>mapred = list(mapred.reduce.tasks=0)</code>.</p>

<p>A special note for <code>mapred.reduce.tasks=0</code>, the default choice is <code>1</code>, so that a reducer is available and operational should you only desire some summary statistic(s) from your data. However, if this task is not being utilized, it is more efficient to set this quantity to <code>0</code>, so you are not tying up resources to an operation not being utilized.</p>

<p>Finally, <code>readback = FALSE</code> asserts that the data is not immediately read into R as it is very easy to read any to all of your data into R later. However, usually there is no need to read the data into R and it is faster not to perform this action. Should you find a need to do this later, the subsections to follow go through the steps necessary to read your randomly generated data.</p>

</div>


<div class='tab-pane' id='observing-the-location-of-the-hdfs-data'>
<h3>Observing the Location of the HDFS Data</h3>

<p>A simple call to list the available files (ls) within your HDFS directory through R utilizing the R Hadoop Integrated Programming Environment (RHIPE) library command <code>rhls()</code> will display where the random values are stored.</p>

<pre><code class="r">head(rhls(&quot;/tmp/rnorm/Nexp20mexp10&quot;))
</code></pre>

<pre><code>   permission   owner      group size          modtime
1  -rwr---r-- jtroisi supergroup    0 2014-06-17 18:24
2  drwxrwxrwt jtroisi supergroup    0 2014-06-17 18:14
3  -rw-r--r-- jtroisi supergroup 8315 2014-06-17 18:14
4  -rw-r--r-- jtroisi supergroup 8315 2014-06-17 18:14
5  -rw-r--r-- jtroisi supergroup 8315 2014-06-17 18:14
6  -rw-r--r-- jtroisi supergroup 8315 2014-06-17 18:14
                                   file
1      /tmp/rnorm/Nexp20mexp10/_SUCCESS
2         /tmp/rnorm/Nexp20mexp10/_logs
3  /tmp/rnorm/Nexp20mexp10/part-m-00000
4  /tmp/rnorm/Nexp20mexp10/part-m-00001
5  /tmp/rnorm/Nexp20mexp10/part-m-00002
6  /tmp/rnorm/Nexp20mexp10/part-m-00003
</code></pre>

<p>Files are list as above.</p>

</div>


<div class='tab-pane' id='reading-the-data-stored-in-the-hdfs-into-your-local-environment'>
<h3>Reading the Data Stored in the HDFS into your Local Environment</h3>

<p>To call any of these sets of random numbers into your local environment it is as simple as a call to the RHIPE command <code>rhread()</code>.</p>

<pre><code class="r">Z0001of1024.list &lt;- rhread(&quot;/tmp/rnorm/Nexp20mexp10/part-m-00000&quot;)
Z0001of1024.key &lt;- Z0001of1024.list[[1]][[1]]
Z0001of1024.values &lt;- Z0001of1024.list[[1]][[2]]
head(Z0001of1024.values)
</code></pre>

<pre><code>[1] -0.2314  1.6313 -0.9020 -0.8584 -0.1432  1.0514
</code></pre>

</div>


<div class='tab-pane' id='to-be-added'>
<h3>To be added...</h3>

</div>


<div class='tab-pane' id='the-problem-description-'>
<h3>The Problem Description</h3>

<p>Divide and Recombine (D &amp; R) is a statistical framework for the analysis of large complex data. The Elapsed Timing Experiment is a very good example for embarrassingly parallel computing and it is designed to improve the perormance of D &amp; R Computations on a Cluster. The time depends on many factors, so it presents an opportunity for optimizing the computation by making the best choice of the factors. However, this exmaple here mainly serve to illustrate the usage of RHIPE functions, so we will only consider two statistical factors that measure characteristics of the dataset and the subsets.</p>

<p>The basic idea is to generate subsets first and then use logistic regression method to analyze each subset by R function <code>glm.fit</code> . There are two types of elapsed-time computation. The subsets are stored on the HDFS as R objects. The first computation type is <strong>O</strong>, the elapsed time to read the subsets from the HDFS and make them available to <code>glm.fit</code> in memory as an R objects. The other type, <strong>L</strong>, starts when <strong>O</strong> ends and it consists of <code>glm.fit</code> computations on the subsets by <strong>map</strong>, plus <strong>reduce</strong> gathering the subset estimates and computing the means. However, we cannot measure <strong>L</strong> directly. So we measure <strong>O</strong> in one run and <strong>T = O + L</strong> in another.</p>

</div>


<div class='tab-pane' id='data-structure'>
<h3>Data Structure</h3>

<table><thead>
<tr>
<th>Variables</th>
<th>Description</th>
<th>Values</th>
</tr>
</thead><tbody>
<tr>
<td>N</td>
<td>Sample size</td>
<td>2<sup>21</sup></td>
</tr>
<tr>
<td>V</td>
<td>Factor--Number of variables</td>
<td>2<sup>4</sup> , 2<sup>5</sup> , 2<sup>6</sup></td>
</tr>
<tr>
<td>M</td>
<td>Factor--Number of observations per subset</td>
<td>2<sup>8</sup> , 2<sup>9</sup> , 2<sup>10</sup> , ..., 2<sup>17</sup></td>
</tr>
<tr>
<td>O</td>
<td>Response variable--first type of elapsed time</td>
<td></td>
</tr>
<tr>
<td>T</td>
<td>Response varibale--whole elapsed time</td>
<td></td>
</tr>
</tbody></table>

</div>


<div class='tab-pane' id='structured-data'>
<h3>Structured Data</h3>

<p>Data that resides in a fixed field within a record or file is called structured data. This includes data contained in 
relational databases and spreadsheets. Structured data has the advantage of being easily entered, stored, queried and 
analyzed. At one time, because of the high cost and performance limitations of storage, memory and processing, relational 
databases and spreadsheets using structured data were the only way to effectively manage data. Tremendous amount of 
raw data now a day are stored as structured data in the form of text files. In this section, we are going to demostrate
how to read, store, and manipulate structure data on HDFS by using RHIPE.</p>

<p>The Airline data set consists of flight arrival and departure details for all commercial flights from 1987 to 2008. 
The approximately 120MM records (CSV format), occupy 120GB space.</p>

</div>


<div class='tab-pane' id='copying-the-data-to-the-hdfs'>
<h3>Copying the Data to the HDFS</h3>

<p>The Airline data can be found <a href="http://stat-computing.org/dataexpo/2009/the-data.html">at this site</a> . In this example, 
we download the data sets for the individual years and save them on the HDFS with the following code (with limited error checks)</p>

<pre><code class="r">map &lt;- expression({
  msys &lt;- function(on){
    system(sprintf(&quot;wget  %s --directory-prefix ./tmp 2&gt; ./errors&quot;, on))
    if(length(grep(&quot;(failed)|(unable)&quot;, readLines(&quot;./errors&quot;))) &gt; 0){
      stop(paste(readLines(&quot;./errors&quot;), collapse=&quot;\n&quot;))
    }
  }
  lapply(map.values, function(r){
    x = 1986 + r
    on &lt;- sprintf(&quot;http://stat-computing.org/dataexpo/2009/%s.csv.bz2&quot;, x)
    fn &lt;- sprintf(&quot;./tmp/%s.csv.bz2&quot;, x)
    msys(on)
    system(sprintf(&#39;bunzip2 %s&#39;, fn))
  })
})
z &lt;- rhwatch(
  map       = map,
  input     = rep(length(1987:1996),2),
  output    = &quot;/tmp/airline/data&quot;,
  mapred    = list( mapred.reduce.tasks = 0 ),
  copyFiles = TRUE,
  readback  = FALSE
)
</code></pre>

<p>In the map expression, we first define <code>msys()</code> function which grabs a download link as function input argument. Within <code>msys()</code> function, 
we call the system command <code>wget</code> to download the materials from the input <code>on</code> to a forlder named tmp, and also change the standard 
error output to a folder named errors under current directory. If there is any failed or unable in error message, there will be an error
message showing in R by calling <code>stop</code> function.</p>

<p>Here we only plan to download 10 years data to demonstrate our example, so the numbers 1 to N (which is equal to <code>length(1987:1996)</code>) are 
assigned as both keys and values. The variables map.values and map.keys will be both lists of numbers 1 to 10 respectively. 
The entries need not be in the order 1 to 10. For more information about <code>input=c(integer,integer)</code>, please refer to simulation section.</p>

<p>Then we iterate over each <code>map.values</code>. For a given <code>map.values</code>, we define the download link to be a string, and assign it to object <code>on</code>.
Assign anther string which is the name of zip file we downloaded. Then we apply our <code>msys()</code> function to download link <code>on</code>, and call system
command <code>bunzip2</code>.</p>

<p>In short, the expression downloads the CSV file, unzips its, and stores in the folder tmp located in the current directory. No copying is performed. 
The current directory is a temporary directory on the local filesystem of the compute node, not on the HDFS. Upon successful completion of the
split, the files stored in tmp (of the current directory) will be copied to the output folder specified by <code>output</code> in the call to <code>rhwatch()</code>. Files 
are copied only if <code>copyFiles</code> is set to TRUE.</p>

<pre><code class="r">rhls()
</code></pre>

<pre><code>   permission owner      group        size          modtime                           file
1  -rw-r--r-- tongx supergroup           0 2014-06-11 10:44     /tmp/airline/data/_SUCCESS
2  drwxrwxrwt tongx supergroup           0 2014-06-11 10:43        /tmp/airline/data/_logs
3  drwxr-xr-x tongx supergroup           0 2014-06-11 17:51     /tmp/airline/data/_outputs
4  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00000
5  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00001
6  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00002
7  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00003
8  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00004
9  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00005
10 -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00006
11 -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00007
12 -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00008
13 -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00009
</code></pre>

<p>All &#39;part-m-...&#39; files are empty since we did not have real output content from the mapreduce job. Downloaded files are actually coyied into a 
subdirectory named <code>_outputs</code></p>

<pre><code class="r">rhls(&quot;/tmp/airline/data/_outputs&quot;)
</code></pre>

<pre><code>   permission owner      group     size          modtime                                file
1  -rw-r--r-- tongx supergroup 121.3 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1987.csv
2  -rw-r--r-- tongx supergroup 477.8 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1988.csv
3  -rw-r--r-- tongx supergroup   464 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1989.csv
4  -rw-r--r-- tongx supergroup 485.6 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1990.csv
5  -rw-r--r-- tongx supergroup 468.5 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1991.csv
6  -rw-r--r-- tongx supergroup 469.5 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1992.csv
7  -rw-r--r-- tongx supergroup   468 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1993.csv
8  -rw-r--r-- tongx supergroup 478.3 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1994.csv
9  -rw-r--r-- tongx supergroup 506.2 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1995.csv
10 -rw-r--r-- tongx supergroup 509.2 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1996.csv
</code></pre>

</div>


<div class='tab-pane' id='converting-to-r-objects'>
<h3>Converting to R Objects</h3>

<p>The data needs to be converted to R objects. Since we will be doing repeated analyses on the data, it is better to spend time converting them 
to R objects making subsequent computations faster, rather than tokenizing strings and converting to R objects for every analysis.</p>

<p>A sample of the text file</p>

<pre><code>1987,10,14,3,741,730,912,849,PS,1451,NA,91,79,NA,23,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA
1987,10,15,4,729,730,903,849,PS,1451,NA,94,79,NA,14,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA
1987,10,17,6,741,730,918,849,PS,1451,NA,97,79,NA,29,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA
...
</code></pre>

<p>The meaning of the columns can be found <a href="http://stat-computing.org/dataexpo/2009/the-data.html">here</a> . Rather than store the entire 120MM rows 
as one big data frame, it is efficient to store it as rectangular blocks of R rows and M columns. We will not store all the above columns only the
following:</p>

<ul>
<li>Dates: day of week, date, month and year (1,2,3, and 4)</li>
<li>Arrival and departure times: actual and scheduled (5,6,7 and 8)</li>
<li>Flight time: actual and scheduled (12 and 13)</li>
<li>Origin and Destination: airport code, latitude and longitude (17 and 18)</li>
<li>Distance (19)</li>
<li>Carrier Name (9)
Since latitude and longitude are not present in the data sets, we will compute them later as required. Carrier names are located in a different R 
data set which will be used to do perform carrier code to carrier name translation.</li>
</ul>

<p>Before we start any mapreduce job for converting, there is one thing we have to do. As we already seen previously, the real text files are located 
in /tmp/airline/data/_outputs/ directory. The underscore at the beginning of a directory/file on HDFS makes the system to treat the directory/file 
as invisible. That&#39;s why when we read from a directory that is an output from a mapreduce job, those file &#39;_SUCCESS&#39; and &#39;_logs&#39;are skipped and only 
files &#39;part-m-...&#39; are read in. So in order to read in those csv files, we have to change the name of directory to be without underscore.</p>

<pre><code class="r">rhmv(&quot;/tmp/airline/data/_outputs&quot;, &quot;/tmp/airline/data/outputs&quot;)
rhls(&quot;/tmp/airline/data&quot;)
</code></pre>

<pre><code>   permission owner      group        size          modtime                           file
1  -rw-r--r-- tongx supergroup           0 2014-06-11 10:44     /tmp/airline/data/_SUCCESS
2  drwxrwxrwt tongx supergroup           0 2014-06-11 10:43        /tmp/airline/data/_logs
3  drwxr-xr-x tongx supergroup           0 2014-06-11 22:05      /tmp/airline/data/outputs
4  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00000
5  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00001
...
</code></pre>

<p>Now we are ready to go. First, we would like to store the data set as blocks of 5000 \(\times\) 12 rows 
and columns. These will be the values. The class of the values will be data.frame in R, and every value 
must be mapped to a key.</p>

<pre><code class="r">map &lt;- expression({
  convertHHMM &lt;- function(s){
    t(sapply(s, function(r){
      l = nchar(r)
      if(l == 4) c(substr(r, 1, 2), substr(r, 3, 4))
      else if(l == 3) c(substr(r, 1, 1), substr(r, 2, 3))
      else c(&#39;0&#39;, &#39;0&#39;)
    })
  )}
  y &lt;- do.call(&quot;rbind&quot;, lapply(map.values, function(r) {
    if(substr(r, 1, 4) != &#39;Year&#39;) strsplit(r, &quot;,&quot;)[[1]]
  }))
  mu &lt;- rep(1,nrow(y))
  yr &lt;- y[, 1]
  mn &lt;- y[, 2]
  dy &lt;- y[, 3]
  hr &lt;- convertHHMM(y[,5])
  depart &lt;- ISOdatetime(
    year  = yr,
    month = mn,
    day   = dy,
    hour  = hr[,1],
    min   = hr[,2],
    sec   = mu
  )
  hr &lt;- convertHHMM(y[,6])
  sdepart &lt;- ISOdatetime(
    year  = yr,
    month = mn,
    day   = dy,
    hour  = hr[,1],
    min   = hr[,2],
    sec   = mu
  )
  hr &lt;- convertHHMM(y[,7])
  arrive &lt;- ISOdatetime(
    year  = yr,
    month = mn,
    day   = dy,
    hour  = hr[,1],
    min   = hr[,2],
    sec   = mu
  )
  hr &lt;- convertHHMM(y[,8])
  sarrive &lt;- ISOdatetime(
    year  = yr,
    month = mn,
    day   = dy,
    hour  = hr[,1],
    min   = hr[,2],
    sec   = mu
  )
  d &lt;- data.frame(
    depart = depart,
    sdepart = sdepart,
    arrive = arrive,
    sarrive = sarrive,
    carrier = y[, 9],
    origin = y[, 17],
    dest = y[, 18],
    dist = as.numeric(y[, 19]),
    year = as.numeric(yr),
    month = as.numeric(mn),
    day = as.numeric(dy),
    cancelled = as.numeric(y[, 22]),
    stringsAsFactors = FALSE
  )
  d &lt;- d[order(d$sdepart),]
  rhcollect(d[c(1,nrow(d)), &quot;sdepart&quot;], d)
})
reduce &lt;- expression(
  reduce = {
    lapply(reduce.values, function(i) rhcollect(reduce.key, i))
  }
)
z &lt;- rhwatch(
  map      = map,
  reduce   = reduce,
  input    = rhfmt(&quot;/tmp/airline/data/outputs&quot;, type = &quot;text&quot;),
  output   = rhfmt(&quot;/tmp/airline/output/blocks&quot;, type = &quot;sequence&quot;),
  mapred   = list( rhipe_map_buff_size = 5000 ),
  orderby  = &quot;numeric&quot;,
  readback = FALSE
)
</code></pre>

<pre><code>...
[Thu Jun 12 10:43:12 2014] Name:2014-06-12 10:13:54 Job: job_201405301308_0878  State: RUNNING Duration: 1757.38
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_0878
             pct numtasks pending running complete killed failed_attempts killed_attempts
map    1.0000000       37       0       0       37      0               0               3
reduce 0.9915602        1       0       1        0      0               0               0
Waiting 5 seconds
[Thu Jun 12 10:43:17 2014] Name:2014-06-12 10:13:54 Job: job_201405301308_0878  State: RUNNING Duration: 1762.414
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_0878
             pct numtasks pending running complete killed failed_attempts killed_attempts
map    1.0000000       37       0       0       37      0               0               3
reduce 0.9936993        1       0       1        0      0               0               0
Waiting 5 seconds
...
</code></pre>

<p>There are 37 splits for the mapreduce job. A split can consist of many map.values that need to be processed. For text files as input, a split is 
128MB or whatever your Hadoop block size is. In map expression, we first define a R function <code>convertHHMM()</code>, which will be used to separate an
input of four digit time record (hhmm) to a vector with hour and minute separately. 
Then we iterate over the <code>map.values</code>, which are the lines in text files, and tokenizing them. The first line in each downloaded file is about the
variable names. The first line starts with column year which must be ignored. The lines of text are aggregated using rbind and time related columns 
converted to datetime objects. The data frame is sorted by scheduled departure and saved to disk indexed by the range of scheduled departures in the 
data frame. The size of the value (data frame) is important. RHIPE will can write any sized object but cannot read key/value pairs that are more than
256MB. A data frame of 5000 rows and 12 columns fits very well into 256MB.</p>

<p>Here maybe we can add a small session talking more about Sorting and Shuffling????...Just a thought</p>

</div>


<div class='tab-pane' id='example-set-up-for-counter'>
<h3>Example set up for counter</h3>

<p>It is very likely in the future analysis, we want to study the flights information for a specific day. So for this scenario we want to create new
key/value pairs by using RHIPE. The input files are the blocks of data we created previously, and the ouput will be &#39;sequence&#39; file with key is the
date, and corresponding value is a data frame of data for that particular day. For example, we would like to know what is the delay rate on everyday.</p>

<pre><code class="r">map &lt;- expression({
  a &lt;- do.call(&quot;rbind&quot;, map.values)
  a$delay.sec &lt;- as.vector(a[,&#39;arrive&#39;]) - as.vector(a[,&#39;sarrive&#39;])
  a &lt;- a[!is.na(a$delay.sec),]
  a$isdelayed &lt;- sapply(a$delay.sec, function(r) if(r &gt; 0) TRUE else FALSE)
  e &lt;- split(a, list(a$year, a$month, a$day))
  lapply(e, function(r){
    n &lt;- nrow(r)
    numdelayed &lt;- sum(r$isdelayed)
    rhcollect(as.vector(unlist(c(r[1, c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)]))), c(n, numdelayed))
  })
})
reduce &lt;- expression(
  pre = {
    sums &lt;- c(0, 0)
  },
  reduce = {
    sums &lt;- sums + apply(do.call(&quot;rbind&quot;, reduce.values), 2, sum)
  },
  post = { 
    rhcollect(reduce.key, sums) 
  }
)
mapred &lt;- list()
mapred$rhipe_map_buff_size &lt;- 5
z &lt;- rhwatch(
  map      = map,
  reduce   = reduce,
  combiner = TRUE,
  input    = rhfmt(&quot;/tmp/airline/output/blocks&quot;, type = &quot;sequence&quot;),
  output   = rhfmt(&quot;/tmp/airline/output/delaybyday&quot;, type = &quot;sequence&quot;),
  mapred   = mapred,
  readback = FALSE
)
</code></pre>

<p>In map expression, we still iterate over the <code>map.values</code>, which are number <code>rhipe_map_buff_size</code> of the data frame with 5000 rows and 12 columns
flights information. we define the second of delay as <code>delay.sec</code>. Of course, we have to remove all <code>NA</code> in <code>delay.sec</code> since there are some 
records of flight have <code>NA</code> as missing data of arriving time. Then create a flag variable <code>isdelayed</code> to identify if the flight is delayed. 
Object <code>e</code> is a list which come from the calling of <code>split()</code> function. What we get for <code>e</code> is a data frame for each day as elements of the 
list. At last, we collect the key which is the date, and value which is a vector with total number of flights and number of delayed flights 
for each element of <code>e</code>.</p>

<p>In reduce expression, we initialize the <code>sums</code> in <code>pre</code> of reduce, which will be the final total number of flights and number of delay for a given day.
And in <code>reduce</code> of reduce, we just cumulate all two numbers for same key. Finally, in <code>post</code> of reduce, collect the final key/value pairs. 
<code>reduce.key</code> here is one particular date of the day, and <code>reduce.values</code> is a list with all <code>c(numberofflight, numberofdelay)</code> as elements.</p>

<p>We read the output by using <code>rhread()</code>, and then grab all the keys assigned to <code>y1</code>, grab all the values assigned to <code>y2</code>. Based on keys and
values, we create a data frame named <code>results</code> with 6 columns. The delay rate is the number of delay divided by the number of total flights 
on that day. Finally, the data frame is sorted by the day.</p>

<pre><code class="r">b &lt;- rhread(&quot;/tmp/airline/output/delaybyday&quot;)
y1 &lt;- do.call(&quot;rbind&quot;, lapply(b, &quot;[[&quot;, 1))
y1 &lt;- y1[-1, ]
y2 &lt;- do.call(&quot;rbind&quot;, lapply(b, &quot;[[&quot;, 2))
y2 &lt;- y2[-1, ]
results &lt;- data.frame(
  year    = y1[, 1],
  month   = y1[, 2],
  day     = y1[, 3],
  nflight = y2[, 1],
  ndelay  = y2[, 2]
)
results$rate &lt;- results$ndelay/results$nflight
results &lt;- results[order(results$year, results$month, results$day), ]
head(results)
</code></pre>

<pre><code>   year month day nflight ndelay      rate
1  1987    10   1   14759   9067 0.6143370
10 1987    10  10   13417   7043 0.5249311
11 1987    10  11   14016   7790 0.5557934
12 1987    10  12   14792   8376 0.5662520
13 1987    10  13   14859   8623 0.5803217
14 1987    10  14   14799   8806 0.5950402
</code></pre>

<p>We can do very similar thing that calculating the delay rate, but for each hour, instead of doing that for each day. We only need to change
the key to be the hour variable in data.</p>

<pre><code class="r">map &lt;- expression({
  a &lt;- do.call(&quot;rbind&quot;,map.values)
  a$delay.sec &lt;- as.vector(a[, &#39;arrive&#39;])-as.vector(a[, &#39;sarrive&#39;])
  a &lt;- a[!is.na(a$delay.sec),]
  a$isdelayed &lt;- sapply(a$delay.sec, function(r) if(r &gt; 0) TRUE else FALSE)
  a$hrs &lt;- as.numeric(format(a[, &#39;sdepart&#39;], &quot;%H&quot;))
  e &lt;- split(a,a$hrs)
  lapply(e, function(r){
    n &lt;- nrow(r) 
    numdelayed &lt;- sum(r$isdelayed)
    rhcollect(as.vector(unlist(c(r[1, c(&quot;hrs&quot;)]))), c(n, numdelayed))
  })
})
reduce &lt;- expression(
  pre = {
    sums &lt;- c(0,0)
  },
  reduce = {
    sums &lt;- sums + apply(do.call(&quot;rbind&quot;, reduce.values), 2, sum)
  },
  post = { 
    rhcollect(reduce.key, sums) 
  }
)
mapred &lt;- list()
mapred$rhipe_map_buff_size &lt;- 5
z &lt;- rhwatch(
  map      = map,
  reduce   = reduce,
  combiner = TRUE,
  input    = rhfmt(&quot;/tmp/airline/output/blocks/&quot;, type = &quot;sequence&quot;),
  output   = rhfmt(&quot;/tmp/airline/output/delaybyhours&quot;, type = &quot;sequence&quot;),
  mapred   = mapred,
  readback = FALSE
)
</code></pre>

</div>


<div class='tab-pane' id='subsetting-data-under-construction'>
<h3>Subsetting Data (Under Construction)</h3>

<p>Recall that the airline data take the form</p>

<pre><code>BUGBUG
</code></pre>

<p>The carrier name is column 9. The carrier code for Southwest Airlines is WN, and the code for Delta Airways is DL. Only those rows with column 9 equal to WN or DL will be saved.</p>

<pre><code class="r">map &lt;- expression({
  tkn &lt;- strsplit(unlist(map.values), &quot;,&quot;)
  text &lt;- do.call(&quot;rbind&quot;, tkn)
  text &lt;- text[text[, 9] %in% c(&quot;WN&quot;, &quot;DL&quot;), , drop = FALSE]
  if (nrow(text) &gt; 0)
    apply(text, 1, function(r) rhcollect(r[9], r))
})
</code></pre>

<p>Note that <code>rhcollect()</code> requires both a key and value.
However, since the key is not used, <code>NULL</code> is passed to the key argument and <code>mapred.textoutputformat.usekey</code> is set to <code>FALSE</code> so that the key is not written to disk.
By default RHIPE includes strings in quotes, but since we do not wish to do so here, we set <code>rhipe_string_quote</code> to <code>&#39;&#39;</code> and <code>mapred.field.separator</code> to <code>&quot;,&quot;</code> as the original data is comma separated.
A partitioner is used to send all the flight information belonging to Southwest Airlines to one file and that belonging to Delta Airways to another.</p>

<pre><code class="r">z &lt;- rhwatch(
  map = map,
  reduce = rhoptions()$templates$identity,
  input = &quot;tmp/airline/data/part-m-00005&quot;,
  output = &quot;tmp/airline/southdelta&quot;,
  , inout = c(&quot;text&quot;, &quot;text&quot;),
  orderby = &quot;char&quot;,
  part = list(lims = 1, type = &quot;string&quot;),
  mapred = list(
    mapred.reduce.tasks = 2,
    rhipe_string_quote = &#39;&#39;,
    mapred.field.separator = &quot;,&quot;,
    mapred.textoutputformat.usekey = FALSE)
  )
rhex(z)
</code></pre>

<p>The first output file contains</p>

<pre><code>BUGBUG
</code></pre>

<p>and the sceond output file contains</p>

<pre><code>BUGBUG
</code></pre>

</div>


<div class='tab-pane' id='transforming-data-under-construction'>
<h3>Transforming Data (Under Construction)</h3>

<p>In this example, we convert each airport code to their name equivalent. Airport codes can be found at the <a href="http://stat-computing.org/dataexpo/2009/the-data.html">JSM website</a>. When working with massive data, repeatedly used operations need to be as fast as possible. Thus we will save the airport code to airport name as a hash table using the <code>new.env()</code> function. Airport codes (origin and destination) are in columns 17 and 18. The setup expression loads this data set and creates a function that does the mapping.</p>

<pre><code class="r">airport &lt;- read.table(&quot;~/tmp/airports.csv&quot;, sep=&quot;,&quot;, header = TRUE, stringsAsFactors = FALSE)
aton &lt;- new.env()
for(i in 1:nrow(airport)){
  aton[[ airport[i, &quot;iata&quot;] ]] &lt;- list(ap = airport[i, &quot;airport&quot;], latlong = airport[i, c(&quot;lat&quot;, &quot;long&quot;)])
}
rhsave(aton, file = &quot;/tmp/airports.Rdata&quot;)

setup &lt;- expression({
  load(&quot;airports.Rdata&quot;)
  co &lt;- function(N){
    sapply(text[, N], function(r){
      o &lt;- aton[[ r[1] ]]$ap
      if(is.null(o)) NA else sprintf(&#39;&quot;%s&quot;&#39;, o)
    })
  }
})
</code></pre>

<p>The map function will use the <code>aton</code> dictionary to get the complete names.
Note that removing the <code>sprintf()</code> function statement makes it much faster.</p>

<pre><code class="r">map &lt;- expression({
  tkn &lt;- strsplit(unlist(map.values), &quot;,&quot;)
  text &lt;- do.call(&quot;rbind&quot;, tkn)
  text[, 17] &lt;- co(17)
  text[, 18] &lt;- co(18)
  apply(text, 1, function(r) { rhcollect(NULL, r) })
})

z &lt;- rhwatch(map = map,ifolder = &quot;/airline/data/2005.csv&quot;,
  ofolder = &quot;/airline/transform&quot;, 
  , inout = c(&quot;text&quot;,&quot;text&quot;),
  shared = c(&quot;/airport/airports.Rdata&quot;),
  setup = setup,
  mapred = list(
    mapred.reduce.tasks = 0,
    rhipe_string_quote = &#39;&#39;,
    mapred.field.separator = &quot;,&quot;,
    mapred.textoutputformat.usekey = FALSE))
rhex(z)
</code></pre>

<p>The output is shown as follows.</p>

<pre><code>BUGBUG
</code></pre>

</div>


<div class='tab-pane' id='the-combiner'>
<h3>The Combiner</h3>

<h4>Combiner as an Optimization</h4>

<p>Between the map phase and reduce phase of a MapReduce job, Hadoop sends all the intermediate values for a given key to the reducer. The intermediate values for a given key are located on several compute nodes and need to be shuffled (sent across the network) to the node assigned the processing of that intermediate key. This involves a lot of network transfer. </p>

<p>Some operations do not need access to all of the data (intermediate values), i.e they can compute on subsets and order does not matter, i.e associative and commutative operations. For example, the minimum, or the sum, of some numbers. In these cases, a combiner can be useful.</p>

<p>The idea of the combiner is that the reduce is first run locally on mapper outputs before they are sent for the final reduce. When the combiner is enabled, the reduction occurs just after the map phase on a subset of intermediate values for a given intermediate keys. The output of this is then sent to the reducer. This greatly reduces network transfer and accelerates the job speed, especially if the output from a map contains a lot of data. </p>

<h4>Enabling Combiner in RHIPE</h4>

<p>Combiner can be enabled in RHIPE by specifying <code>combiner = TRUE</code> when calling function <code>rhwatch()</code>.</p>

<p>To be able to use a combiner, your <strong>reduce expression needs to pass the same data type as it receives</strong>, i.e the two arguments in the function <code>rhcollect()</code> in your map expression need to be of the same type as those in the function <code>rhcollect()</code> in your reduce expression. For example, if you pass a string as the key and a numeric vector as the value in the map expression, you need to pass a string as key and a numeric vector as the value in the reduce expression as well.</p>

<p>We will demonstrate the usage of combiners through the following examples.</p>

</div>


<div class='tab-pane' id='airline-data-overall-distance-by-carrier'>
<h3>Airline Data: Overall Distance by Carrier</h3>

<p>We will make use of the Airline data introduced in the previous sections, and more specifically, we will use the database of flight information created in the HDFS under <code>/tmp/airline/output/blocks</code>.</p>

<p>Recall that in this database, the key is a time interval, and the value is a data frame of information of flights during this time interval. 
Each row of the data frame is a flight, and the flight information are in the columns, including carrier name, distance of flight, and so on.</p>

<p>Suppose we would like to compute the all time distance for each carrier, which is the summation of distance over all flights of each carrier.</p>

<h4>Without Combiner</h4>

<p>First, let&#39;s compute the sum without combiner. Here&#39;s the RHIPE code for this computation.</p>

<pre><code class="r"># RHIPE code to compute summation of distances for each carrier
map &lt;- expression({
  lapply(map.values, function(r) {
    # r is a data frame with each line being a flight
    # for each flight, collect 
    #   the carrier as the key 
    #   the distance as the value
    Map(rhcollect, r$carrier, r$dist) 
  })
})
reduce &lt;- expression(
  pre = {
    dist = 0
  }, 
  reduce = {
    dist = dist + sum(unlist(reduce.values), na.rm = TRUE)
  },
  post = {
    rhcollect(reduce.key, dist)
  }
)
z &lt;- rhwatch(
  map    = map,
  reduce = reduce,
  input  = &quot;/tmp/airline/output/blocks&quot;,
  output = &quot;/tmp/combiner/distance.by.carrier/no.combiner&quot;
)
</code></pre>

<p>Notice that in the map expression, we used the base R function <code>Map()</code> to collect the carrier name as the key and distance of each flight as the value, this is not to be confused with the map in the MapReduce framework.</p>

<p>In the reduce stage, the distances of all the flights from the same carrier have been transmitted to the same reducer. 
So in the reduce expression, we sum up all values of the distance for each carrier as <code>dist</code>: 
we first initialize the sum to be 0 in <code>pre</code>; 
then we keep adding the sum of <code>reduce.values</code>, which is a list of individual flight distances, to <code>dist</code> in <code>reduce</code>; 
at last, when all flights of the carrier have been aggregated, we collect the <code>reduce.key</code>, which is the carrier name, as the key and the aggregated distance as the value in <code>post</code>. 
The <code>na.rm</code> argument in the function <code>sum()</code> is turned on because for some of the flights, the distance field is missing.</p>

<p>We can read the output into the R global environment and transform into a data frame through the following code:</p>

<pre><code class="r"># read the output into the R global environment
rst = rhread(&quot;/tmp/combiner/distance.by.carrier/no.combiner&quot;)
# form a data frame for carriers and their distances
df = data.frame(  
  carrier  = sapply(rst, &quot;[[&quot;, 1),
  distance = sapply(rst, &quot;[[&quot;, 2)
)
# re-arrange the data frame by distance
df = df[order(df$distance, decreasing = TRUE), ]  
# re-assign the row names
row.names(df) = NULL
</code></pre>

<p>And the resulting data frame <code>df</code> will look like this, with American Airlines (AA), Delta Airline (DL), and United Airline (UA) being the top 3:</p>

<pre><code>   carrier   distance
1       AA 5547694472
2       DL 4951504638
3       UA 4915176282
4       US 3652941933
5       CO 3025408204
6       NW 2948310164
7       TW 1710404680
8       WN 1628292542
9       HP 1075453193
10      AS  602641612
11      EA  557435834
12      PI  331802193
13  PA (1)  213910356
14  ML (1)   47795815
15      PS   30274790
</code></pre>

<p>For the purpose of comparison, we will keep track of the running time of this job and the size of data shuffled between map phase and reduce phase. Both of the these can be located on the Hadoop JobTracker web interface, and for this job, the running time is 1mins, 54sec and size of data shuffled is 1,151,750,924 bytes. Keep in mind that these numbers depend on the configuration of the cluster on which the jobs are running, so you might get different values.</p>

<h4>With Combiner</h4>

<p>Since summation is one of the associative and commutative operations, a combiner can be used here. Let&#39;s enable the combiner. </p>

<p>Notice that in the above code, both map expression and reduce expression are collecting a string as the key and a numeric as the value, so the condition that <strong>reduce expression passing the same data type as it receives</strong> holds.</p>

<p>All we need to do is to set the argument <code>combiner</code> to be <code>TRUE</code> when calling function <code>rhwatch</code>.</p>

<pre><code class="r"># enabling combiner
z2 &lt;- rhwatch(
  map      = map,
  reduce   = reduce,
  input    = &quot;/tmp/airline/output/blocks&quot;,
  output   = &quot;/tmp/combiner/distance.by.carrier/yes.combiner&quot;,
  combiner = TRUE
)
</code></pre>

<p>We can read the output into the R global environment and transform the output into a data frame in the same way:</p>

<pre><code class="r">rst2 = rhread(&quot;/tmp/combiner/distance.by.carrier/yes.combiner&quot;)
df2 = data.frame(  
  carrier  = sapply(rst2, &quot;[[&quot;, 1),
  distance = sapply(rst2, &quot;[[&quot;, 2)
)
df2 = df2[order(df2$distance, decreasing = TRUE), ]  
row.names(df2) = NULL
</code></pre>

<p>Just to verify that using the combiner does not alter the result, we can compare <code>df2</code> with <code>df</code> from the previous job without combiner:</p>

<pre><code class="r">identical(df, df2)
</code></pre>

<pre><code>[1] TRUE
</code></pre>

<p>We can once again refer to the Hadoop JobTracker web interface, and now the running time of the job with combiner is 1mins, 42sec, and the size of data shuffled is 10,656 bytes. </p>

<p>After enabling the combiner, the size of data shuffled has seen significantly reduced, from around 1 GB to 10 KB; and the running timing has also decreased by about 10 seconds. The decrease in running time is not drastic simply because the size of data in this example (less than 4 GB) is relatively small compared with the capacity of the cluster, more specifically, the network bandwidth between servers is 1 GB/sec and it wouldn&#39;t take much time even if the whole dataset were to be shuffled. </p>

<p>The combiner really isn&#39;t useful with this tiny data set, but it can be very powerful. For example, on a project we are working on looking at packets from Internet connections, we have one connection with over 60 million packets, where each packet corresponds to a row in a data frame. One of our tasks is to create a data set that contains the first 1500 packets from each connection. The connection data are stored by 10,000 packet chunks by connection. To get the first 1500 packets, the first 1500 packets from each chunk are extracted and sent to the reducer, which combines these and sorts again to retrieve the first 1500 packets. For the large connection, there would be over 6,000 maps passing the same key with 1500 observations each. Without using a combiner, the reducer would receive over 9 million records to sort for this connection, and the job takes a very long time to complete. Using a combiner breaks this down and speeds things up dramatically.</p>

</div>


<div class='tab-pane' id='airline-data-average-distance-per-flight'>
<h3>Airline Data: Average Distance per Flight</h3>

<p>We have demonstrated the usage of the combiner to compute the summation of some numbers in the above example, and all we had to do is to simply turn it on. However, care needs to be taken to make sure that using a combiner does produce the same result as it would without a combiner.</p>

<p>For example, if we were to compute the average distance per flight instead of the sum, we need to do a bit more than just turning on combiner, assuming we replace the function <code>sum()</code> with <code>mean()</code> of course. This is due to fact that the average of subset averages, which are computed by combiners, is not necessarily the same as the overall average. </p>

<p>However, this does not prevent us from taking advantage of the combiner, we just need a quick detour: instead of computing the average directly in RHIPE, we could compute the summation and number of values; and then we could read in the output and compute the average in the R global environment.</p>

<p>The computation of summation of distances and number of flights can be done through the following RHIPE code: </p>

<pre><code class="r"># RHIPE code to compute average distance per flight for each carrier
map &lt;- expression({
  lapply(map.values, function(r) {
    # for each flight, collect 
    #   the carrier as the key 
    #   the distance and flight count as the value
    dist.count = Map(c, r$dist, as.numeric(!is.na(r$dist)))
    Map(rhcollect, r$carrier, dist.count) 
  })
})
reduce &lt;- expression(
  pre = {
    dist = 0
    count = 0
  }, 
  reduce = {
    dist = dist + sum(sapply(reduce.values, &quot;[[&quot;, 1), na.rm = TRUE)
    count = count + sum(sapply(reduce.values, &quot;[[&quot;, 2), na.rm = TRUE)
  },
  post = {
    rhcollect(reduce.key, c(dist, count))
  }
)
z &lt;- rhwatch(
  map      = map,
  reduce   = reduce,
  input    = &quot;/tmp/airline/output/blocks&quot;,
  output   = &quot;/tmp/combiner/average.distance.by.carrier&quot;,
  combiner = TRUE
)
</code></pre>

<p>In the map expression, we have altered the values being collected from the previous example: 
instead of collecting the distance for each flight alone, we collect both the distance and the number of flights being aggregated, which is set to 0 if the distance is missing and thus this flight does not contribute to the sum or average distance, and 1 otherwise. 
This is to make sure that the map expression outputs same data type as the reduce expression. 
We first create a list object <code>dist.count</code>, each element of which is a numeric vector of length two, consisting the distance of a flight and the number 1; 
then we collect the carrier of each flight as the key, and each element in <code>dist.count</code> as the value. </p>

<p>In the reduce expression, we sum up distances as <code>dist</code> and numbers of flights as <code>count</code>, and collect the carrier name as the key, and the vector of aggregated distance and number of flights as the value.</p>

<p>We can read the output into the R global environment and transform into a data frame through the following code:</p>

<pre><code class="r"># read the output into the R global environment
rst = rhread(&quot;/tmp/combiner/average.distance.by.carrier&quot;)
# form a data frame
df = data.frame(  
  carrier  = sapply(rst, &quot;[[&quot;, 1),
  do.call(&quot;rbind&quot;, lapply(rst, &quot;[[&quot;, 2))
)
# re-name the columns of the data frame
names(df) = c(&quot;carrier&quot;, &quot;distance&quot;, &quot;flights&quot;)
</code></pre>

<p>Now that we have computed the aggregated distance and number of flights for each carrier, the average distance per flight can be easily computed as the ratio of the two:</p>

<pre><code class="r"># compute average distance per flight
transform(df, average = distance / flights)
</code></pre>

<pre><code>   carrier   distance flights  average
1       AA 5547694472 6602267 840.2711
2       AS  602641612  965798 623.9831
3       CO 3025408204 4012874 753.9255
4       DL 4951504638 7829867 632.3868
5       EA  557435834  917232 607.7370
6       HP 1075453193 1806551 595.3074
7       NW 2948310164 4405188 669.2813
8       PI  331802193  873957 379.6551
9       PS   30274790   83617 362.0650
10      TW 1710404680 2417453 707.5234
11      UA 4915176282 5931255 828.6908
12      US 3652941933 7284227 501.4866
13      WN 1628292542 4207081 387.0362
14  ML (1)   47795815   70622 676.7837
15  PA (1)  213910356  315075 678.9188
</code></pre>

</div>


<div class='tab-pane' id='function-rhcounter'>
<h3>Function rhcounter()</h3>

<p>The function <code>rhcounter()</code> is a very useful function that can help us with collecting information during the mapreduce job. There is sometimes 
that when we are collecting key/value pairs, we would also like to count the frequnce of specific key/value pairs that we are interested in. 
<code>rhcounter(group, name, value)</code> increments the distributed counter name that belongs to family <code>group</code> by <code>value</code>. Ideally <code>group</code> and <code>name</code> 
should be strings, any R object can be sent and it will be converted to its string representation. The next thing that we should be concerned 
with is how to access this counter we defined in a mapreduce job. At the jobtracker webpage, there will be a counter table in job summary page
of each mapreduce job. There is two types of counters can be found in the table, one is default counters. Mapreduce job itself has some default 
job counters, filesystem counters and so forth. Those counters is defined by Hadoop. Another type of counters is user defined. The first column
of the table is the <code>group</code>, second column is <code>name</code>, and the rest of columns are the frequence counts in map step, reduce step, and total
respectively.</p>

</div>


<div class='tab-pane' id='airline-data-download-status'>
<h3>Airline Data: Download Status</h3>

<p>When we download the airline data, we can add two counters to assist us in chekcing the downloading status. We just add two more lines in the map
expression we used to have for downloading code:</p>

<pre><code class="r">map &lt;- expression({
  msys &lt;- function(on){
    system(sprintf(&quot;wget  %s --directory-prefix ./tmp 2&gt; ./errors&quot;, on))
    if(length(grep(&quot;(failed)|(unable)&quot;, readLines(&quot;./errors&quot;))) &gt; 0){
      stop(paste(readLines(&quot;./errors&quot;), collapse=&quot;\n&quot;))
    }
  }
  lapply(map.values, function(r){
    x = 1986 + r
    on &lt;- sprintf(&quot;http://stat-computing.org/dataexpo/2009/%s.csv.bz2&quot;, x)
    fn &lt;- sprintf(&quot;./tmp/%s.csv.bz2&quot;,x)
    msys(on)
    system(sprintf(&#39;bunzip2 %s&#39;, fn))
    rhcounter(&quot;FILES&quot;, x, 1)
    rhcounter(&quot;FILES&quot;, &quot;_ALL_&quot;, 1)
  })
})
</code></pre>

<p>For each downloading job, we create a counter, belongs to &quot;FILES&quot; group, with name same as the corresponding year value. And also, we create a overall
cumulative counter which counts the total number of downloading files, which also belongs to &quot;FILES&quot; group, with the name &quot;<em>ALL</em>&quot;. So we are going to 
have 10 individual counters with count 1 for each, and one total counter with value equal to 10.</p>

</div>


<div class='tab-pane' id='airline-data-delay-rate'>
<h3>Airline Data: Delay Rate</h3>

<p>In the Airline data, we have given an example of calculating the delay rate on every day in the dataset. Within this example, we read in blocks
of data and output sequence file with keys are date of a day, values are total number of flights and number of delayed fights. You may have noticed
the first key/value pair has <code>NA</code>s as the key, and two zero as the value. </p>

<pre><code class="r">b &lt;- rhread(&quot;/tmp/airline/output/delaybyday&quot;)
head(b, 3)
</code></pre>

<pre><code>[[1]]
[[1]][[1]]
[1] NA NA NA

[[1]][[2]]
[1] 0 0


[[2]]
[[2]][[1]]
[1] &quot;1987&quot; &quot;10&quot;   &quot;1&quot;   

[[2]][[2]]
[1] 14759  9067


[[3]]
[[3]][[1]]
[1] &quot;1987&quot; &quot;10&quot;   &quot;2&quot;   

[[3]][[2]]
[1] 14740  8978
</code></pre>

<p>The reason for appearance of <code>NA</code> key/value pairs is that we used <code>split()</code> function. If there is no flight information for a day, <code>split()</code> will still create
an element which is an empty data frame, with the name as <code>NA</code>, in the result list. Previously we remove the <code>NA</code> key from the final data frame <code>results</code> read
from HDFS to local, but here we are planning to remove the key/value pairs with <code>NA</code> before they are saved to HDFS, and collect the rest of key/value pairs as 
usual. At the same time, we also would like to keep recording the number of key/value pairs with <code>NA</code>. This can be done as following:</p>

<pre><code class="r">map &lt;- expression({
  a &lt;- do.call(&quot;rbind&quot;, map.values)
  a$delay.sec &lt;- as.vector(a[, &#39;arrive&#39;]) - as.vector(a[, &#39;sarrive&#39;])
  a &lt;- a[!is.na(a$delay.sec), ]
  a$isdelayed &lt;- sapply(a$delay.sec, function(r) if(r &gt; 0) TRUE else FALSE)
  e &lt;- split(a, list(a$year, a$month, a$day))
  lapply(e,function(r){
    n &lt;- nrow(r)
    numdelayed &lt;- sum(r$isdelayed)
    if (n == 0) {
      rhcounter(&quot;SPLITS&quot;, &quot;EMPTY&quot;, 1)
    }
    else {
      rhcollect(c(unique(r$year), unique(r$month), unique(r$day)), c(n, numdelayed))
    }
  })
})
reduce &lt;- expression(
  pre = {
    sums &lt;- c(0, 0)
  },
  reduce = {
    sums &lt;- sums + apply(do.call(&quot;rbind&quot;, reduce.values), 2, sum)
  },
  post = { 
    rhcollect(reduce.key, sums) 
  }
)
mapred &lt;- list()
mapred$rhipe_map_buff_size &lt;- 5
z &lt;- rhwatch(
  map      = map,
  reduce   = reduce,
  combiner = TRUE,
  input    = rhfmt(&quot;/tmp/airline/output/blocks&quot;, type = &quot;sequence&quot;),
  output   = rhfmt(&quot;/tmp/airline/output/delaybyday&quot;, type = &quot;sequence&quot;),
  mapred   = mapred,
  readback = FALSE
)
</code></pre>

<p>For <code>reduce</code> expression and <code>rhwatch()</code>, we do not change anything to keep it the same as the previous. On jobtracker webpage, we can find that besides 
all system counters, there is an counter named <code>EMPTY</code> belongs to group <code>SPLITS</code> which has 707 in map step. So now we know there are 707 days are not 
included in our final output.</p>

<table><thead>
<tr>
<th></th>
<th>Counter</th>
<th align="right">Map</th>
<th align="right">Reduce</th>
<th align="right">Total</th>
</tr>
</thead><tbody>
<tr>
<td>SPLITS</td>
<td>EMPTY</td>
<td align="right">707</td>
<td align="right">0</td>
<td align="right">707</td>
</tr>
<tr>
<td>Job Counters</td>
<td>SLOTS_MILLIS_MAPS</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1,812,399</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td align="right">...</td>
<td align="right">...</td>
<td align="right">...</td>
</tr>
</tbody></table>

<p>Of course we can have multiple counters within same group we defined. For every flight, we defined delay second to be the difference between arriving time
and schedule arriving time. There will be three different situations after taking this difference, positive and zero delay second means delayed flight, 
neigative delay second means arriving in advance, and &#39;NA&#39; means one of arriving time is missing and so delay second will be <code>NA</code> consequently. Now we 
define a flight is delayed only if the delay second is greater or equal to 15 mins. At the same time, we do want to keep the record about how many flights
are arriving in advance, how many are arriving right on time, and how many are delayed less than 15 mins. <code>rhcounter()</code> will be a good helper for this task.</p>

<pre><code class="r">map &lt;- expression({
  a &lt;- do.call(&quot;rbind&quot;, map.values)
  a$delay.sec &lt;- as.vector(a[, &#39;arrive&#39;]) - as.vector(a[, &#39;sarrive&#39;])
  rhcounter(&quot;DELAY&quot;, &quot;NA&quot;, sum(is.na(a$delay.sec)))
  a &lt;- a[!is.na(a$delay.sec), ]
  a$isdelayed &lt;- sapply(a$delay.sec, function(r) if(r &gt; 900) TRUE else FALSE)
  rhcounter(&quot;DELAY&quot;, &quot;ADVANCE&quot;, sum(sapply(a$delay.sec, function(r) if(r &lt; 0) TRUE else FALSE)))
  rhcounter(&quot;DELAY&quot;, &quot;ONTIME&quot;, sum(sapply(a$delay.sec, function(r) if(r == 0) TRUE else FALSE)))
  rhcounter(&quot;DELAY&quot;, &quot;LESS&quot;,  sum(sapply(a$delay.sec, function(r) if(r &lt; 900 &amp; r &gt; 0) TRUE else FALSE)))
  e &lt;- split(a, list(a$year, a$month, a$day))
  lapply(e,function(r){
    n &lt;- nrow(r)
    numdelayed &lt;- sum(r$isdelayed)
    if (n == 0) {
      rhcounter(&quot;SPLITS&quot;, &quot;EMPTY&quot;, 1)
    }
    else {
      rhcollect(c(unique(r$year), unique(r$month), unique(r$day)), c(n, numdelayed))
    }
  })
})
</code></pre>

<table><thead>
<tr>
<th></th>
<th>Counter</th>
<th align="right">Map</th>
<th align="right">Reduce</th>
<th align="right">Total</th>
</tr>
</thead><tbody>
<tr>
<td>DELAY</td>
<td>ADVANCE</td>
<td align="right">21,040,562</td>
<td align="right">0</td>
<td align="right">21,040,562</td>
</tr>
<tr>
<td></td>
<td>ONTIME</td>
<td align="right">2,535,977</td>
<td align="right">0</td>
<td align="right">2,535,977</td>
</tr>
<tr>
<td></td>
<td>NA</td>
<td align="right">15,407</td>
<td align="right">0</td>
<td align="right">15,407</td>
</tr>
<tr>
<td></td>
<td>LESS</td>
<td align="right">15,326,924</td>
<td align="right">0</td>
<td align="right">15,326,924</td>
</tr>
<tr>
<td>SPLITS</td>
<td>EMPTY</td>
<td align="right">707</td>
<td align="right">0</td>
<td align="right">707</td>
</tr>
<tr>
<td>Job Counters</td>
<td>SLOTS_MILLIS_MAPS</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1,812,399</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td align="right">...</td>
<td align="right">...</td>
<td align="right">...</td>
</tr>
</tbody></table>

</div>


<div class='tab-pane' id='to-be-added'>
<h3>To be added...</h3>

</div>


<div class='tab-pane' id='the-problem'>
<h3>The Problem</h3>

<p>Word count example is a problem of counting the number of occurrences of each word in a large collection of documents.
Now let us start to look at how to do this by using RHIPE package in R. </p>

</div>


<div class='tab-pane' id='preparation'>
<h3>Preparation</h3>

<p>First we have to get the text files that we want to study on. Please download your favorite Shakespeare plays from 
<a href="http://shakespeare.mit.edu/">http://shakespeare.mit.edu/</a> and save it to a text file or many files. We grab one of the Poetries, A Lover&#39;s Complaint, 
and save into two text files to the working directory. </p>

<pre><code class="r">list.files(&quot;./&quot;)
</code></pre>

<pre><code>[1] &quot;ALoversComplaint01.txt&quot; &quot;ALoversComplaint02.txt&quot;
</code></pre>

<p>In these two text files, we remove all space lines, as well as all arbitary spaces before 
any line. Here is how they look like:</p>

<pre><code class="r">system(&quot;head ALoversComplaint01.txt&quot;)
</code></pre>

<pre><code>A Lover&#39;s Complaint
FROM off a hill whose concave womb reworded
A plaintful story from a sistering vale,
My spirits to attend this double voice accorded,
And down I laid to list the sad-tuned tale;
Ere long espied a fickle maid full pale,
Tearing of papers, breaking rings a-twain,
Storming her world with sorrow&#39;s wind and rain.
Upon her head a platted hive of straw,
Which fortified her visage from the sun,
</code></pre>

<p>After the texts are ready, we are going to write them to the HDFS. Originally, there is an empty directory on HDFS named 
<code>/tmp/wordcount/input</code>. <code>rhput()</code> function is the one will be used to write files onto HDFS. The first argument is the path to the 
local file to be copied to the HDFS, and the second argument is path to the file on the HDFS. After that, We can use <code>rhls()</code> 
to access the content under a specific path on HDFS. Information of files such as permission, owner, group, will be displayed.
Becides the path of directory on HDFS to be the first argument of <code>rhls()</code>, argument <code>recurse</code> can be set to be TRUE, which
will list all files and directories in sub-directories, argument <code>nice</code> can be set to be one of <code>g</code>,<code>m</code>,<code>b</code> or <code>h</code> (gigabytes, 
megabytes, bytes, human readable)</p>

<pre><code class="r">rhput(&quot;~/ALoversComplaint01.txt&quot;, &quot;/tmp/wordcount/input/ALoversComplaint01.txt&quot;)
rhls(&quot;/tmp/wordcount/input&quot;)
</code></pre>

<pre><code>  permission owner      group     size          modtime
1 -rw-r--r-- tongx supergroup 7.708 kb 2014-06-08 10:40
2 -rw-r--r-- tongx supergroup 6.271 kb 2014-06-08 10:40
                                         file
1 /tmp/wordcount/input/ALoversComplaint01.txt
2 /tmp/wordcount/input/ALoversComplaint02.txt
</code></pre>

<p>Text file is an acceptable file format in Mapreduce. If the type of input file in a mapreduce job is &quot;text&quot;, RHIPE will 
actually generate a key/value pair for every row. The key is the row index, and the value is the content of corresponding 
line which saved as a string. </p>

</div>


<div class='tab-pane' id='entirety'>
<h3>Entirety</h3>

<p>The entire code of RHIPE for word count is:</p>

<pre><code class="r">map1 &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    line = gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, map.values[[r]])
    line = strsplit(line, split=&quot; +&quot;)[[1]]
    lapply(line, function(word) {
      rhcollect(word, 1)
    })
  })
})
reduce1 &lt;- expression(
  pre = {
    count = 0
  },
  reduce = {
    count = count + sum(unlist(reduce.values))
  },
  post = {
    rhcollect(reduce.key, count)
  }
)
mr1 &lt;- rhwatch(
  map      = map1,
  reduce   = reduce1,
  input    = rhfmt(&quot;/tmp/wordcount/input/ALoversComplaint01.txt&quot;,type=&quot;text&quot;),
  output   = rhfmt(&quot;/tmp/wordcount/output/word.count&quot;, type=&quot;sequence&quot;),
  mapred   = list( mapred.reduce.tasks=5 ),
  readback = FALSE
)
</code></pre>

<p>A valid mapreduce job in RHIPE is consist of a <code>map</code> expression, an optional <code>reduce</code> expression, and a execution funtion 
<code>rhwatch()</code>. Right now you do not have to worry too much about the details of map and reduce expressions, we will discrib 
more details in later sessions. Here you can just run this code in R. In R console you will see that job running information
is keeping popping out, which will be helpful for you to have some idea about the status of running job. </p>

<pre><code>Waiting 5 seconds
[Mon Jun  9 09:12:09 2014] Name:2014-06-09 09:11:59 Job: job_201405301308_0844  State: RUNNING Duration: 10.318
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_0844
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      1        1       0       0        1      0               0               0
reduce   0        5       0       5        0      0               0               0
Waiting 5 seconds
[Mon Jun  9 09:12:14 2014] Name:2014-06-09 09:11:59 Job: job_201405301308_0844  State: RUNNING Duration: 15.364
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_0844
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      1        1       0       0        1      0               0               0
reduce   0        5       0       5        0      0               0               0`
</code></pre>

<p>After the job is successfully executed, you can access the output on HDFS by calling <code>rhread()</code> function. </p>

<pre><code class="r">rhls(&quot;/tmp/wordcount/output&quot;)
</code></pre>

<pre><code>  permission owner      group size          modtime                             file
1 drwxr-xr-x tongx supergroup    0 2014-06-09 10:23 /tmp/wordcount/output/word.count
</code></pre>

<pre><code class="r">rst1 &lt;- rhread(&quot;/tmp/wordcount/output/word.count&quot;)
head(rst1, 3)
</code></pre>

<pre><code>[[1]]
[[1]][[1]]
[1] &quot;I&quot;

[[1]][[2]]
[1] 10


[[2]]
[[2]][[1]]
[1] &quot;He&quot;

[[2]][[2]]
[1] 1


[[3]]
[[3]][[1]]
[1] &quot;In&quot;

[[3]][[2]]
[1] 3
</code></pre>

<p>As you can see, the class of the result is a list. Each element of the list is a list with two elements. The first 
element is a word, the second element is a integer. It is obvious to claim that this is a pair of unique word and
its corresponding occurence. The length of <code>rst1</code> is the total number of unique word in the text file.</p>

<pre><code class="r">length(rst1)
</code></pre>

<pre><code>[1] 745
</code></pre>

<p>Of course, the class of output object, which is a list, may not be convenient for further analysis in R. It is easy 
to convert list to be a data.frame in R.</p>

<pre><code class="r">data &lt;- data.frame(key=unlist(lapply(rst1, &quot;[[&quot;, 1)), value=unlist(lapply(rst1, &quot;[[&quot;,2)))
data &lt;- data[with(data, order(value, decreasing=TRUE)),]
head(data)
</code></pre>

<pre><code>    key value
303  in    38
456 and    36
7    of    31
16  his    30
323 the    30
603  to    27
</code></pre>

<p>The final output is converted to be a data.frame, and the words also are ordered by its occurences decreasingly. In the
next map session, we will give more details about the map function in the mapreduce job we just had.</p>

</div>


<div class='tab-pane' id='map'>
<h3>Map</h3>

<p>Map is an R expression that is evaluated by RHIPE during the map stage. For each task, RHIPE will call this expression 
multiple times. The input and output of map function are both key/value pairs. A key/value pair (KVP) is an abstract data 
type that includes a group of key identifiers and a set of associated values. In other words, the map function processes a 
key/value pair to generate a set of intermediate key/value pairs. So in our previous map function, we process the key/value 
pairs we got from the text file into new key/value pairs which every word is the key, and the corresponding value would 
be integer 1. The key/value pairs read in from input file will be saved as map.keys and map.values respectively. Map.keys 
and map.values are two lists which are consist of all keys and all values that will be excuted in one task at one monment 
respectively. In this example, which the input file of a mapreduce job is a text file, all keys (indices) in map.keys will 
not have any meaning but will be unique, and all the corresponding values in map.values are each row of text file saved as 
a string.</p>

<pre><code class="r">map1 &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    line = gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, map.values[[r]])
    line = strsplit(line, split=&quot; +&quot;)[[1]]
    lapply(line, function(word) {
      rhcollect(word, 1)
    })
  })
})
</code></pre>

<p>So in map expression, we iterate over all key/value pairs. The length of map.keys and map.values are the same as the total
number of key/value pairs, which here is the number of row in text file. <code>map.keys[[r]]</code> and <code>map.values[[r]]</code> is the r&#39;th
key/value pair. For each <code>map.values[[r]]</code>, we remove all those special character in each row from the string by using <code>gsub()</code>
function. And then we split the <code>line</code> by spaces using <code>strsplit()</code> function, collect a new key/value pair, which key is a 
single word and value is 1, by using <code>rhcollect()</code> function in RHIPE. The first argument of <code>rhcollect()</code> is the key, and the
second argument is the value. Suppose we have 100 rows, and each row has 20 words, by using our map function, we will be
collecting 2,000 new key/value pairs, or we call them intermediate key/value pairs.</p>

<p>As we mentioned previously, the reduce expression is optional, and this can be helpful for us if you are interested map.keys
and map.values from text file, or the intermediate key/value pairs after the map expression. Let us first look at the map.keys
and map.values. In map expression, we can just collect one key/value pair. Key is a meaningless integer like 1, value is a list
with two elements named &#39;keys&#39; and &#39;values&#39;, which are asigned with <code>map.keys</code> list and <code>map.values</code> list respectively.</p>

<pre><code class="r">map2 &lt;- expression({
  rhcollect(1, list(keys=map.keys, values=map.values))
})
mr2 &lt;- rhwatch(
  map      = map2,
  input    = rhfmt(&quot;/tmp/wordcount/input/ALoversComplaint01.txt&quot;, type = &quot;text&quot;),
  output   = rhfmt(&quot;/tmp/wordcount/output/map&quot;, type = &quot;sequence&quot;),
  mapred   = list( mapred.reduce.tasks = 10 ),
  readback = FALSE
)
rst2 &lt;- rhread(&quot;/tmp/wordcount/output/map&quot;)
</code></pre>

<p>The <code>rst2</code> is a list with one element because there is only one key/value pair been collected in map step. <code>rst2[[1]][[1]]</code> is the 
key, and <code>rst2[[1]][[2]]</code> is the value.</p>

<pre><code class="r">str(rst2[[1]][[2]])
</code></pre>

<pre><code>List of 2
 $ keys  :List of 185
  ..$ : num 0
  ..$ : num 20
  ..$ : num 64
  ..$ : num 105
...
 $ values:List of 185
  ..$ : chr &quot;A Lover&#39;s Complaint&quot;
  ..$ : chr &quot;FROM off a hill whose concave womb reworded&quot;
  ..$ : chr &quot;A plaintful story from a sistering vale,&quot;
  ..$ : chr &quot;My spirits to attend this double voice accorded,&quot;
...
</code></pre>

<p>Another way to demonstrate the <code>map.keys</code> and <code>map.values</code> is to collect every element in <code>map.keys</code> and <code>map.values</code> as a key/value 
pair, and then output them.</p>

<pre><code class="r">map3 &lt;- expression({
  lapply(seq_along(map.keys), function(r){
    rhcollect(map.keys[[r]], map.values[[r]])
  })
})
mr3 &lt;- rhwatch(
  map      = map3,
  input    = rhfmt(&quot;/tmp/wordcount/input/ALoversComplaint01.txt&quot;, type = &quot;text&quot;),
  output   = rhfmt(&quot;/tmp/wordcount/output/identity.map&quot;, type = &quot;sequence&quot;),
  mapred   = list( mapred.reduce.tasks = 10 ),
  readback = FALSE
)
rst3 &lt;- rhread(&quot;/tmp/wordcount/output/identity.map&quot;)
head(rst3, 3)
</code></pre>

<pre><code>[[1]]
[[1]][[1]]
[1] 2190

[[1]][[2]]
[1] &quot;And often kiss&#39;d, and often &#39;gan to tear:&quot;


[[2]]
[[2]][[1]]
[1] 154

[[2]][[2]]
[1] &quot;And down I laid to list the sad-tuned tale;&quot;


[[3]]
[[3]][[1]]
[1] 4169

[[3]][[2]]
[1] &quot;If best were as it was, or best without.&quot;
</code></pre>

<p>The result is a list with length equals to 185, which is the number of rows in total. Each element is also a 
list with length two. The first element is key, and the second element is value. The map step above can be called
as identity map function, since the input and output key/value pairs of this map function are the same.</p>

</div>


<div class='tab-pane' id='reduce'>
<h3>Reduce</h3>

<p>The next thing we would like to dig into is a reduce function in our mapreduce job. In RHIPE, reduce is an R expression that is evaluated 
by RHIPE during the reduce stage, or it is a vector of expressions with names pre, reduce, and post. All key/value pairs that 
share same key will be grouped together and processed to be applied reduce funtion. In reduce-pre session, we initialize the occurrence 
<code>count</code> for each unique word to be 0. <code>reduce.key</code> is the shared key, and reduce.values is a list that includes all values corresponding to 
that unique reduce.key. In reduce-reduce session, we cumulative all reduce.values. Finally in post session, we collect the final key/value 
pair for each unique word.</p>

<pre><code class="r">reduce1 &lt;- expression(
  pre = {
    count = 0
  },
  reduce = {
    count = count + sum(unlist(reduce.values))
  },
  post = {
    rhcollect(reduce.key, count)
  }
)
</code></pre>

<p>It is acceptable to only have reduce session in reduce function. Similar in map step, we can have a reduce expression that only have 
reduce-reduce session to collect <code>reduce.key</code> and corresponding <code>reduce.values</code>, which can help us to demonstrate the <code>reduce.key</code> and
<code>reduce.values</code>.</p>

<pre><code class="r">reduce2 &lt;- expression(
  reduce = {
    rhcollect(reduce.key,reduce.values) 
  }
)
mr4 &lt;- rhwatch(
  map      = map1,
  reduce   = reduce2,
  input    = rhfmt(&quot;/tmp/wordcount/input/ALoversComplaint01.txt&quot;, type = &quot;text&quot;),
  output   = rhfmt(&quot;/tmp/wordcount/output/reduce&quot;, type = &quot;sequence&quot;),
  mapred   = list( mapred.reduce.tasks = 5 ),
  readback = FALSE
)
rst4 &lt;- rhread(&quot;/tmp/wordcount/output/reduce&quot;)
str(rst4[[1]])
</code></pre>

<pre><code>List of 2
 $ : chr &quot;I&quot;
 $ :List of 10
  ..$ : num 1
  ..$ : num 1
  ..$ : num 1
  ..$ : num 1
  ..$ : num 1
  ..$ : num 1
  ..$ : num 1
  ..$ : num 1
  ..$ : num 1
  ..$ : num 1
</code></pre>

<p>The result <code>rst4</code> is a list with 745 elements, each is a key/value pair for one unique word. When intermediate key/value pairs are passed
into reduce step, all key/value pairs that share same key are grouped together, then all corresponding values are also grouped to be a list, 
which is the <code>reduce.values</code>, as in the example, the <code>reduce.values</code> for key &#39;I&#39; is a list with length 10, and each one is integer 1.</p>

<p>It is also possible to have an identity reduce function that write all intermediate key/value pairs to disk without doing anything else.</p>

<pre><code class="r">reduce3 &lt;- expression(
  reduce = {
    lapply(reduce.values, function(r) rhcollect(reduce.key,r)) 
  }
)
mr5 &lt;- rhwatch(
  map      = map1,
  reduce   = reduce3,
  input    = rhfmt(&quot;/tmp/wordcount/input/ALoversComplaint01.txt&quot;, type = &quot;text&quot;),
  output   = rhfmt(&quot;/tmp/wordcount/output/identity.reduce&quot;, type = &quot;sequence&quot;),
  mapred   = list( mapred.reduce.tasks = 5 ),
  readback = FALSE
)
rst5 &lt;- rhread(&quot;/tmp/wordcount/output/identity.reduce&quot;)
</code></pre>

<p>The result is still a list with length 1425. This is the total number of words in the text file. Each element
is a list of key/value pair.</p>

<pre><code class="r">length(rst5)
</code></pre>

<pre><code>[1] 1425
</code></pre>

<pre><code class="r">head(rst5, 3)
</code></pre>

<pre><code>[[1]]
[[1]][[1]]
[1] &quot;I&quot;

[[1]][[2]]
[1] 1


[[2]]
[[2]][[1]]
[1] &quot;I&quot;

[[2]][[2]]
[1] 1


[[3]]
[[3]][[1]]
[1] &quot;I&quot;

[[3]][[2]]
[1] 1
</code></pre>

</div>


<div class='tab-pane' id='execution-function'>
<h3>Execution Function</h3>

<p>After the map and reduce expression, we are heading to the execution function of a mapreduce job in RHIPE.
<code>rhwatch()</code> is a call that packages the MapReduce job which is sent to Hadoop. 
In <code>rhwatch()</code> function, we specify what the map and reduce stage of the mapreduce job is, which are the expressions
we defined. We asign the map and reduce expression to <code>map</code> and <code>reduce</code> argument in <code>rhwatch()</code> respectively. Input and 
output argument in <code>rhwatch()</code> function is used to specify the path on HDFS of input file and output file respectively, and 
there are three types of file we can consider, text, sequence, and map file. Mapred argument is a list that can be used to 
customize the Hadoop and RHIPE options. Here we specify the <code>mapred.reduce.tasks</code> to be 5 or 10, so the number of reduce tasks will 
be set to be 5 or 10. This number also is related to the number of output files, since each reduce task will generate one piece
of output file for the final output. </p>

<pre><code class="r">mr1 &lt;- rhwatch(
  map      = map1,
  reduce   = reduce1,
  input    = rhfmt(&quot;/tmp/wordcount/input/ALoversComplaint01.txt&quot;,type=&quot;text&quot;),
  output   = rhfmt(&quot;/tmp/wordcount/output/word.count&quot;, type=&quot;sequence&quot;),
  mapred   = list( mapred.reduce.tasks=5 ),
  readback = FALSE
)
</code></pre>

<p>In <code>mr1</code> job, we specify <code>mapred.reduce.tasks</code> to be 5, so there will be five files in output
&quot;/tmp/wordcount/output/word.count&quot;, named from &quot;part-r-00000&quot; to &quot;part-r-00004&quot;. Besides these five files, there will be
another two files named &quot;_SUCCESS&quot; and &quot;_logs&quot; which records the metadata and log information.</p>

<pre><code class="r">rhls(&quot;/tmp/wordcount/output/word.count&quot;)
</code></pre>

<pre><code>  permission owner      group     size          modtime
1 -rw-r--r-- tongx supergroup        0 2014-06-09 10:23
2 drwxr-xr-x tongx supergroup        0 2014-06-09 10:23
3 -rw-r--r-- tongx supergroup 4.966 kb 2014-06-09 10:23
4 -rw-r--r-- tongx supergroup  5.01 kb 2014-06-09 10:23
5 -rw-r--r-- tongx supergroup 4.878 kb 2014-06-09 10:23
6 -rw-r--r-- tongx supergroup 5.177 kb 2014-06-09 10:23
7 -rw-r--r-- tongx supergroup 4.928 kb 2014-06-09 10:23
                                           file
1     /tmp/wordcount/output/word.count/_SUCCESS
2        /tmp/wordcount/output/word.count/_logs
3 /tmp/wordcount/output/word.count/part-r-00000
4 /tmp/wordcount/output/word.count/part-r-00001
5 /tmp/wordcount/output/word.count/part-r-00002
6 /tmp/wordcount/output/word.count/part-r-00003
7 /tmp/wordcount/output/word.count/part-r-00004
</code></pre>

<p>If we go to the output directory of output from <code>mr2</code>, which we specify the <code>mapred.reduce.tasks</code> to be 10, we will find without
surprise that there are ten files plus &quot;_SUCCESS and &quot;_logs&quot;</p>

<pre><code class="r">rhls(&quot;/tmp/wordcount/output/map&quot;)
</code></pre>

<pre><code>   permission owner      group        size          modtime
1  -rw-r--r-- tongx supergroup           0 2014-06-09 11:42
2  drwxr-xr-x tongx supergroup           0 2014-06-09 11:42
3  -rw-r--r-- tongx supergroup    11.66 kb 2014-06-09 11:42
4  -rw-r--r-- tongx supergroup    94 bytes 2014-06-09 11:42
5  -rw-r--r-- tongx supergroup    94 bytes 2014-06-09 11:42
6  -rw-r--r-- tongx supergroup    94 bytes 2014-06-09 11:42
7  -rw-r--r-- tongx supergroup    94 bytes 2014-06-09 11:42
8  -rw-r--r-- tongx supergroup    94 bytes 2014-06-09 11:42
9  -rw-r--r-- tongx supergroup    94 bytes 2014-06-09 11:42
10 -rw-r--r-- tongx supergroup    94 bytes 2014-06-09 11:42
11 -rw-r--r-- tongx supergroup    94 bytes 2014-06-09 11:42
12 -rw-r--r-- tongx supergroup    94 bytes 2014-06-09 11:42
                                     file
1      /tmp/wordcount/output/map/_SUCCESS
2         /tmp/wordcount/output/map/_logs
3  /tmp/wordcount/output/map/part-r-00000
4  /tmp/wordcount/output/map/part-r-00001
5  /tmp/wordcount/output/map/part-r-00002
6  /tmp/wordcount/output/map/part-r-00003
7  /tmp/wordcount/output/map/part-r-00004
8  /tmp/wordcount/output/map/part-r-00005
9  /tmp/wordcount/output/map/part-r-00006
10 /tmp/wordcount/output/map/part-r-00007
11 /tmp/wordcount/output/map/part-r-00008
12 /tmp/wordcount/output/map/part-r-00009
</code></pre>

<p>Here you may find out that there are 9 output files are with size of 94 bytes, and only one is 11.66 Kb. The reason for this is 
because in <code>mr2</code>, we only collect one key/value pair, and one key/value pair cannot be split to multiple files. But we do specify
that we want 10 output files. So this only key/value pair is saved into one file, and the rest of files will be empty file with 
fixed size, 94 bytes. If the number of reduce tasks is specified larger than the number of key/value pairs, some of files will
be empty. <code>readback</code> argument in <code>rhwatch()</code> is a logical argument that controls if the results of mapreduce job will be read 
back. </p>

</div>


<div class='tab-pane' id='combiner'>
<h3>Combiner</h3>

<p>After map function is finished, there may be significant repetition in the intermediate keys produced by each map task.
For our example here, it is highly possible that each map task will produce hundreds or thousands of records of the form
(the, 1). All of these counts will be sent over the network to a single reduce task and then added together by the reduce 
function to produce one number. A better way to speed up this mapreduce job is try to eliminate the objects that need to 
be transferred. So we can specify an optional combiner function that does partial merging of intermediate key/value pairs
before it is sent over the network.</p>

<p>The combiner function is executed on each machine that performs a map task. Typically the same code is used to implement 
both the combiner and the reduce functions.</p>

<pre><code class="r">mr6 &lt;- rhwatch(
  map      = map1,
  reduce   = reduce1,
  input    = rhfmt(&quot;/tmp/wordcount/input/ALoversComplaint01.txt&quot;,type=&quot;text&quot;),
  output   = rhfmt(&quot;/tmp/wordcount/output/combiner&quot;, type=&quot;sequence&quot;),
  mapred   = list( mapred.reduce.tasks=5 ),
  combiner = TRUE,
  readback = FALSE,
)
rst6 &lt;- rhread(&quot;/tmp/wordcount/output/combiner&quot;)
</code></pre>

<p>Technically, the number of input key/value pairs to reduce function is smaller when we active the combiner function. One way
to check this is that we can go to the jobtracker webpage, one of counter named &quot;Reduce input records&quot; tells us how many
input key/value pairs to the reduce function. For previous example, the &quot;Reduce input records&quot; counter is 1,425. When we 
consider the combiner function, the &quot;Reduce input records&quot; counter is 745. So combiner function does help us to eliminate
the number of key/value pairs to be transported from map to reduce.</p>

</div>


<div class='tab-pane' id='multiple-input-files'>
<h3>Multiple input files</h3>

<p>It is quite common that we have more than one input files. RHIPE allows us to have a vector of path string to be input. For
this situation, you can specify the input as a input directory which includes all input files. If a particular file cannot be
understood by the input format (e.g. a text file given to <code>type=sequence</code>), RHIPE will throw an error. </p>

<pre><code class="r">mr7 &lt;- rhwatch(
  map      = map1,
  reduce   = reduce1,
  input    = rhfmt(&quot;/tmp/wordcount/input&quot;, type=&quot;text&quot;),
  output   = rhfmt(&quot;/tmp/wordcount/output/total&quot;, type=&quot;sequence&quot;),
  mapred   = list( mapred.reduce.tasks=5 ),
  readback = FALSE,
  combiner = TRUE,
)
rst7 &lt;- rhread(&quot;/tmp/wordcount/output/total&quot;)
</code></pre>

<pre><code class="r">length(rst7)
</code></pre>

<pre><code>[1] 1171
</code></pre>

<pre><code class="r">head(rst7, 3)
</code></pre>

<pre><code>[[1]]
[[1]][[1]]
[1] &quot;I&quot;

[[1]][[2]]
[1] 20


[[2]]
[[2]][[1]]
[1] &quot;He&quot;

[[2]][[2]]
[1] 2


[[3]]
[[3]][[1]]
[1] &quot;In&quot;

[[3]][[2]]
[1] 8
</code></pre>

<p>Still, we can easily transform the output list to be a data.frame for further analysis.</p>

<pre><code class="r">data &lt;- data.frame(key = unlist(lapply(rst7, &quot;[[&quot;, 1)), value = unlist(lapply(rst7, &quot;[[&quot;,2)))
data &lt;- data[with(data, order(value, decreasing=TRUE)),]
head(data)
</code></pre>

<pre><code>    key value
721 and    63
8    of    58
508 the    57
483  in    55
951  to    51
17  his    40
</code></pre>

</div>

   
   <ul class="pager">
      <li><a href="#" id="previous">&larr; Previous</a></li> 
      <li><a href="#" id="next">Next &rarr;</a></li> 
   </ul>
</div>


</div>
</div>

<hr>

<div class="footer">
   <p>&copy; , 2014</p>
</div>
</div> <!-- /container -->

<script src="assets/jquery/jquery.js"></script>
<script type='text/javascript' src='assets/custom/custom.js'></script>
<script src="assets/bootstrap/js/bootstrap.js"></script>
<script src="assets/custom/jquery.ba-hashchange.min.js"></script>
<script src="assets/custom/nav.js"></script>

</body>
</html>