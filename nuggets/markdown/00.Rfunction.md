# RHIPE Nuggets #

## RHIPE function reference ##

### rhwrite ###

Write R data to the HDFS

#### Usage ####


```r
rhwrite(object, file, numfiles = 1, chunk = 1, passByte = 1024 * 1024 * 20, 
  kvpairs = TRUE, verbose = TRUE)
```


#### Arguments ####

**object**  
  An object whose elements are written

**file**  
  Where to write(it is overwritten)

**numfiles**  
  Number of files to write to

**chunk**  
  An integer specificed to chunk data frames into rows or lists into sublists

**passByte**  
  Buffer size for writing (see details)

**kvpairs**  
  If `TRUE`, `object` should be a list of key-value pairs - otherwise, it should be a data frame or 
matrix (in which case `NULL` keys will be written with arbitrary chunking of the data)

**verbose**  
  Logical - print messages about what is being done

#### Description ####

Takes a list of objects, found in `object` and writes them to the folder pointed to by `file` which 
will be located on the HDFS.

#### Details ####

This code, will chunk a data frame(or matrix) or list into sub objects, defined by chunks and then 
written to the HDFS across numfiles files. Thus if chunks is 10, and numfiles is 20, then a data 
frame is divided into sub data frames of rows 10 each and written across 20 files. In order to 
improve the R-Java switch, this is buffered, the buffer size defined by passByte(bytes).

### rhread ###

Read Key/Value Pairs From The HDFS

#### Usage ####


```r
rhread(files, type = c("sequence"), max = -1L, skip = rhoptions()$file.types.remove.regex, 
  mc = lapply, textual = FALSE, verbose = TRUE, ...)
```


#### Arguments ####

**files**  
Path to file or directory containing map, sequence, or text file to be read on the HDFS. This can
also be the output from `rhwatch()` provided with `readback = FALSE` or `rhmr`.

**type**  
Type of file on HDFS.  Must be `sequence`, `map`, or `text`.

**max**  
Maximum number of key/value pairs to read for map and sequence files.  Maximum number of lines to 
read for text files.

**mc**  
Set to lapply by default. User can change this to `mclapply` for parallel lapply.

**textual**  
If the keys and values are hadoop Text objects.

**skip**  
Files to skip while reading the hdfs Various installs of Hadoop add additional log info to HDFS 
output from MapReduce.  Attempting to read these files is not what we want to do in rhread.  To get
around this we specify pieces of filenames to grep and remove from the read.  skip is a vector 
argument just to have sensible defaults for a number of different systems.  You can learn which if 
any files need to be skipped by using `rhls` on target directory.

**verbose**  
Logical - print messages about what is being done.

**...**  
Arguments passed to the function.

#### Value ####

For map and sequence files, a list of key, pairs of up to length MAX.  For text files, a matrix of
lines, each row a line from the text files.

#### Description ####

Reads all or a limited number of key/value pairs from HDFS files.

#### Details ####

Reads the key,value pairs from the files pointed to by `files`. The source `files` can end in a wildcard
`/path/input/p*` will read all the key,value pairs contained in files starting with `p` in the folder 
`/path/input/`.  
The parameter `type` specifies the format of `files`. This can be one of `text`, `map` or `sequence`
which imply a Text file, MapFile or a SequenceFile respectively.  
The parameter `max` specifies the maximum number of entries to read, by default all the key,value 
pairs will be read. Specifying `max` for text files, limits the number of lines read.  
`mc` is by default `lapply`. The user can change this to `mclapply` for faster throughput.  
Data written by `rhwrite` can be read using `rhread`.

#### Author #####

Saptarshi Guha

#### See also ####

`rhget`, `rhput`, `rhdel`, `rhwrite`, `rhsave`


### rhsave ###

Save .RData to HDFS

#### Usage ####


```r
rhsave(..., file, envir = parent.frame())
```


#### Arguments ####

**...**  
Additional parameters for `rhsave`

**file**  
Absolute path to file on HDFS. Creates the file or overwrites

**envir**  
Environment to search for objects to be saved it.

#### Description ####

Puts the result of a `save` call unto the HDFS. Useful if you have variables in the current 
environment you want to work with in a MapReduce as a shared object.

#### Author ####

Saptarshi Guha

#### See also ####

`rhsave.image`, `rhload`

### rhload ###

Load an RData from the HDFS.

#### Usage ####


```r
rhload(file, envir = parent.frame())
```


#### Arguments ####

**file**  
Path to the .RData file on the HDFS.

**envir**  
Environment in which to load the .RData file.

#### Value ####

Data from HDFS

#### Description ####

Calls the function load after fetching an RData file from the HDFS.

#### author ####

Saptarshi Guha

#### See also ####

`rhsave`, `rhsaveimage`

### rhput ###

Put a file unto the HDFS.

#### Usage ####


```r
rhput(src, dest, deletedest = TRUE)
```


#### Arguments ####

**src**  
Path to the local file to be copied to the HDFS.

**dest**  
Path to the file on the HDFS. `rhput` creates the file at dest.

**deletedest**  
If `TRUE` this function attempts to delete the destination of the HDFS before trying to copy to that
location on the HDFS.

#### Description ####

Copies the local file called `src` (not a folder) to the destination `dest` on the HDFS. Uses
`path.expand` to expand the `src` parameter.  
Local filesystem copy remains after the operation is complete.

#### Author ####

Saptarshi Guha

#### See also ####
`rhget`, `rhdel`, `rhread`, `rhwrite`, `rhsave`


### rhget ###

Copying from the HDFS. Moves files from the HDFS to a local directory.

#### Usage ####


```r
rhget(src, dest)
```


#### Arguments ####

**src**  
Absolute path to file or directory on HDFS to get.

**dest**  
Path to file or directory on local filesystem to create as the new copy.

#### Description ####

Copies the files (or folder) at `src`, located on the HDFS to the destination `dest` located on the
local filesystem. If a file or folder of the same name as `dest` exists on the local filesystem, it
will be deleted. The `dest` can contain `~` which will be expanded. The original copy of the file or
folder is left on the HDFS after this operation.

#### Author ####

Saptarshi Guha

#### See also ####

`rhput`, `rhdel`, `rhread`, `rhwrite`, `rhsave`


### rhls ###

List Files On HDFS

#### Usage ####


```r
rhls(folder = NULL, recurse = FALSE, nice = "h")
```


#### Arguments ####

**folder**  
Path of directory on HDFS or output from `rhwatch(read = FALSE)`

**recurse**  
If `TRUE` list all files and directories in sub-directories.

**nice**  
One of 'g','m','b' or 'h' (gigabytes, megabytes, bytes, human readable)

####  Value ####

Vector of file and directory names.

#### Description ####
List all files and directories contained in a directory on the HDFS.

#### Details ####

Returns a data.frame of filesystem information for the files located at `path`. If `recurse` is 
`TRUE`, `rhls` will recursively travel the directory tree rooted at `path`. The returned object is
a data.frame consisting of the columns: permission, owner, group, size (which is numeric), 
modification time, and the file name. `path` may optionally end in `*` which is the wildcard and 
will match any character(s).

#### Author  ####

Saptarshi Guha

#### See also ####
`rhput`, `rhdel`, `rhread`, `rhwrite`, `rhsave`, `rhget`

### rhfmt ###

A function that returns a function to specify input/output formats

#### Usage ####

```r
rhfmt(type, ...)
```

#### Arguments ####

**type**  
  The name of the function handler

**...**  
  Addtional arguments passed to the function


#### Description ####

Returns a function to spec out the input output formats

#### Details ####

the function returned must take 3 arguments  `lines`, `direction`(input or output), `caller`(call 
signature) see `rhoptions()$ioformats` for examples on how to write your own.
