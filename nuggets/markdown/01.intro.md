# RHIPE: Nuggets #

## Introduction ##

### Outline and Reference ###

This tutorial covers examples of how to use R package `Rhipe`.


#### Outline

- First, we 
- Next, we 
- Then we 
- We also provide R source files for all of the examples throughout the documentation.

#### Reference


References:
   - [datadr.org](http://datadr.org): Divide and Recombine (D&R) with `Rhipe
   - [RHIPE](http://github.com/saptarshiguha/RHIPE): the engine that makes D&R work for large datasets
   - [datadr](http://github.com/hafen/datadr): R package providing the D&R framework
   - [trelliscope](http://github.com/hafen/trelliscope): the visualization companion to `datadr`
   - [Large complex data: divide and recombine (D&R) with RHIPE. *Stat*, 1(1), 53-67](http://onlinelibrary.wiley.com/doi/10.1002/sta4.7/full)


### Background ###
<!--
#### Hadoop 
Hadoop is an open source programming framework for distributed computing with massive data sets 
using a cluster of networked computers. It has changed the way many web companies work, bringing 
cluster computing to people with little knowledge of the intricacies of concurrent/distributed 
programming. Part of the reason for its success is that it has a fixed programming paradigm. It 
somewhat restricts what the user can parallelize but once an algorithm has been written the 
‘MapReduce way’, concurrency and distribution over a cluster comes for free.

It consists of two components: the Hadoop Distributed Filesystem and Hadoop MapReduce. They are 
based on the Google Filesystem and Google MapReduce respectively. Companies using these include 
Amazon, Ebay, New York Times, Facebook to name a few. 

#### Hadoop Distributed Filesystem

The Hadoop Distributed Filesystem (HDFS) sits on top of the file system of a computer (called the 
local filesystem). It pools the hard drive space of a cluster or heterogenous computers (e.g. 
different hardware and operating systems) and provides a unified view to the user. For example, with
 a cluster of 10 computers each with 1TB hard drive space available to Hadoop, the HDFS provides a 
user 10 TB of hard drive space. A single file can be bigger than maximum size on the local filesystem 
e.g. 2TB files can be saved on the HDFS. The HDFS is catered to large files and high throughput reads.
 Appends to files are not allowed. Files written to the HDFS are chunked into blocks, each block is 
replicated and saved on different cluster computers. This provides a measure of safety in case of 
transient or permanent computer failures. When a file is written to the HDFS, the client contacts the 
Namenode, a computer that serves as the gateway to the HDFS. It also performs a lot of administrative 
tasks, such as saving the mapping between a file and the location of its block across the cluster and
 so on. The Namenode tells the client which Datanodes (the computers that make up the HDFS) to store 
the data onto. It also tells the client which Datanodes to read the data from when a read request is 
performed. 
#### Hadoop MapReduce

Concurrent programming is difficult to get right. As Herb Sutter put it:

    ... humans are quickly overwhelmed by concurrency and find it much more difficult
        to reason about concurrent than sequential code.

A statistician attempting concurrent programming needs to be aware of race conditions, deadlocks and 
tools to prevent this: locks, semaphores, and mutually exclusive regions etc. An approach suggested by 
Sutter et al (Software and the concurrency revolution, H. Sutter and J. Larus, ACM Queue, Volume 3,
Number 7 2005) is to provide programming models not functions that force the programmer to approach 
her algorithms differently. Once the programmer constructs the algorithm using this model, concurrency 
comes for free. The MapReduce programming model is one example. Correctly coded Condor DAGS are another 
example.

MapReduce (MapReduce: Simplified Data Processing on Large Clusters, Jeffrey Dean and Sanjay Ghemawat,
*Communications of the ACM*, 2008) consists of several embarrassingly parallel splits which are 
evaluated in parallel. This is called the Map. There is a synchronization guard where intermediate 
data created at the end of the Map is exchanged between nodes and another round of parallel computing
 starts, called the Reduce phase. In effect large scale simulation trials in which the programmer 
launches several thousands of independent computations is an example of a Map. Retrieving and collating 
the results (usually done in the R console) is an example of a manual reduce.

In detail, the input to a MapReduce computation is a set of N key,value pairs. The N pairs are 
partitioned into S arbitrary splits. Each split is a unit of computation and is assigned to one 
computing unit on the cluster. Thus the processing of the S splits occurs in parallel. Each split is
processed by a user given function M, that takes a sequence of input key,value pairs and outputs 
(one or many) intermediate key,value pairs. The Hadoop framework will partition the intermediate 
values by the intermediate key. That is intermediate values sharing the same intermediate key are 
grouped together. Once the map is complete, the if there are M distinct intermediate keys, a user 
given function R, will be given an intermediate key and all intermediate values associated with the 
same key. Each processing core is assigned a subset of intermediate keys to reduce and the reduction 
of the M intermediate keys occurs in parallel. The function R, takes an intermediate key, a stream of 
associated intermediate values and returns a final key,value pair or pairs.

The R programmer has used MapReduce ideas. For example, the tapply command splits a vector by a list
 of factors. This the map equivalent: each row of the vector is the value and the keys are the distinct
 levels of the list of factors. The reduce is the user given function applied to the partitions of the 
vector. The xyplot function in lattice takes a formula e.g. $F\sim Y|A*B$, subsets the the data frame by 
the cartesian product of the levels of A and B (the map) and displays each subset (the reduce). Hadoop 
MapReduce generalizes this to a distributed level.
-->
#### RHIPE

RHIPE is the R and Hadoop Integrated Programming Environment. It provides a way to execute Hadoop MapReduce
 jobs completely from within R and with R data structures.


### Getting Started ###

This tutorial assumes RHIPE has been installed on the server.
Please refer to [Install RHIPE](http://www.datadr.org/install.html) for installation guide.

We can load and initialize the package:


```r
library(Rhipe)
rhinit()
```


and we are ready to go.

