## Reading Structured Data  ##

### Structured Data ###
Data that resides in a fixed field within a record or file is called structured data. This includes data contained in 
relational databases and spreadsheets. Structured data has the advantage of being easily entered, stored, queried and 
analyzed. At one time, because of the high cost and performance limitations of storage, memory and processing, relational 
databases and spreadsheets using structured data were the only way to effectively manage data. Tremendous amount of 
raw data now a day are stored as structured data in the form of text files. In this section, we are going to demostrate
how to read, store, and manipulate structure data on HDFS by using RHIPE.

The Airline data set consists of flight arrival and departure details for all commercial flights from 1987 to 2008. 
The approximately 120MM records (CSV format), occupy 120GB space.

### Copying the Data to the HDFS ###
The Airline data can be found [at this site](http://stat-computing.org/dataexpo/2009/the-data.html) . In this example, 
we download the data sets for the individual years and save them on the HDFS with the following code (with limited error checks)

```r
map <- expression({
  msys <- function(on){
    system(sprintf("wget  %s --directory-prefix ./tmp 2> ./errors", on))
    if(length(grep("(failed)|(unable)", readLines("./errors"))) > 0){
      stop(paste(readLines("./errors"), collapse="\n"))
    }
  }
  lapply(map.values, function(r){
    x = 1986 + r
    on <- sprintf("http://stat-computing.org/dataexpo/2009/%s.csv.bz2", x)
    fn <- sprintf("./tmp/%s.csv.bz2", x)
    msys(on)
    system(sprintf('bunzip2 %s', fn))
  })
})
z <- rhwatch(
  map       = map,
  input     = rep(length(1987:1996),2),
  output    = "/tmp/airline/data",
  mapred    = list( mapred.reduce.tasks = 0 ),
  copyFiles = TRUE,
  readback  = FALSE
)
```

In the map expression, we first define `msys()` function which grabs a download link as function input argument. Within `msys()` function, 
we call the system command `wget` to download the materials from the input `on` to a forlder named tmp, and also change the standard 
error output to a folder named errors under current directory. If there is any failed or unable in error message, there will be an error
message showing in R by calling `stop` function.

Here we only plan to download 10 years data to demonstrate our example, so the numbers 1 to N (which is equal to `length(1987:1996)`) are 
assigned as both keys and values. The variables map.values and map.keys will be both lists of numbers 1 to 10 respectively. 
The entries need not be in the order 1 to 10. For more information about `input=c(integer,integer)`, please refer to simulation section.

Then we iterate over each `map.values`. For a given `map.values`, we define the download link to be a string, and assign it to object `on`.
Assign anther string which is the name of zip file we downloaded. Then we apply our `msys()` function to download link `on`, and call system
command `bunzip2`.

In short, the expression downloads the CSV file, unzips its, and stores in the folder tmp located in the current directory. No copying is performed. 
The current directory is a temporary directory on the local filesystem of the compute node, not on the HDFS. Upon successful completion of the
split, the files stored in tmp (of the current directory) will be copied to the output folder specified by `output` in the call to `rhwatch()`. Files 
are copied only if `copyFiles` is set to TRUE.


```r
rhls()
```

```
   permission owner      group        size          modtime                           file
1  -rw-r--r-- tongx supergroup           0 2014-06-11 10:44     /tmp/airline/data/_SUCCESS
2  drwxrwxrwt tongx supergroup           0 2014-06-11 10:43        /tmp/airline/data/_logs
3  drwxr-xr-x tongx supergroup           0 2014-06-11 17:51     /tmp/airline/data/_outputs
4  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00000
5  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00001
6  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00002
7  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00003
8  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00004
9  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00005
10 -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00006
11 -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00007
12 -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00008
13 -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00009
```
All 'part-m-...' files are empty since we did not have real output content from the mapreduce job. Downloaded files are actually coyied into a 
subdirectory named `_outputs`


```r
rhls("/tmp/airline/data/_outputs")
```

```
   permission owner      group     size          modtime                                file
1  -rw-r--r-- tongx supergroup 121.3 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1987.csv
2  -rw-r--r-- tongx supergroup 477.8 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1988.csv
3  -rw-r--r-- tongx supergroup   464 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1989.csv
4  -rw-r--r-- tongx supergroup 485.6 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1990.csv
5  -rw-r--r-- tongx supergroup 468.5 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1991.csv
6  -rw-r--r-- tongx supergroup 469.5 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1992.csv
7  -rw-r--r-- tongx supergroup   468 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1993.csv
8  -rw-r--r-- tongx supergroup 478.3 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1994.csv
9  -rw-r--r-- tongx supergroup 506.2 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1995.csv
10 -rw-r--r-- tongx supergroup 509.2 mb 2014-06-11 17:51 /tmp/airline/data/_outputs/1996.csv
```
### Converting to R Objects ###

The data needs to be converted to R objects. Since we will be doing repeated analyses on the data, it is better to spend time converting them 
to R objects making subsequent computations faster, rather than tokenizing strings and converting to R objects for every analysis.

A sample of the text file

```
1987,10,14,3,741,730,912,849,PS,1451,NA,91,79,NA,23,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA
1987,10,15,4,729,730,903,849,PS,1451,NA,94,79,NA,14,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA
1987,10,17,6,741,730,918,849,PS,1451,NA,97,79,NA,29,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA
...
```

The meaning of the columns can be found [here](http://stat-computing.org/dataexpo/2009/the-data.html) . Rather than store the entire 120MM rows 
as one big data frame, it is efficient to store it as rectangular blocks of R rows and M columns. We will not store all the above columns only the
following:
- Dates: day of week, date, month and year (1,2,3, and 4)
- Arrival and departure times: actual and scheduled (5,6,7 and 8)
- Flight time: actual and scheduled (12 and 13)
- Origin and Destination: airport code, latitude and longitude (17 and 18)
- Distance (19)
- Carrier Name (9)
Since latitude and longitude are not present in the data sets, we will compute them later as required. Carrier names are located in a different R 
data set which will be used to do perform carrier code to carrier name translation.

Before we start any mapreduce job for converting, there is one thing we have to do. As we already seen previously, the real text files are located 
in /tmp/airline/data/_outputs/ directory. The underscore at the beginning of a directory/file on HDFS makes the system to treat the directory/file 
as invisible. That's why when we read from a directory that is an output from a mapreduce job, those file '_SUCCESS' and '_logs'are skipped and only 
files 'part-m-...' are read in. So in order to read in those csv files, we have to change the name of directory to be without underscore.


```r
rhmv("/tmp/airline/data/_outputs", "/tmp/airline/data/outputs")
rhls("/tmp/airline/data")
```

```
   permission owner      group        size          modtime                           file
1  -rw-r--r-- tongx supergroup           0 2014-06-11 10:44     /tmp/airline/data/_SUCCESS
2  drwxrwxrwt tongx supergroup           0 2014-06-11 10:43        /tmp/airline/data/_logs
3  drwxr-xr-x tongx supergroup           0 2014-06-11 22:05      /tmp/airline/data/outputs
4  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00000
5  -rw-r--r-- tongx supergroup    94 bytes 2014-06-11 10:44 /tmp/airline/data/part-m-00001
...
```
Now we are ready to go. First, we would like to store the data set as blocks of 5000 \(\times\) 12 rows 
and columns. These will be the values. The class of the values will be data.frame in R, and every value 
must be mapped to a key.

```r
map <- expression({
  convertHHMM <- function(s){
    t(sapply(s, function(r){
      l = nchar(r)
      if(l == 4) c(substr(r, 1, 2), substr(r, 3, 4))
      else if(l == 3) c(substr(r, 1, 1), substr(r, 2, 3))
      else c('0', '0')
    })
  )}
  y <- do.call("rbind", lapply(map.values, function(r) {
    if(substr(r, 1, 4) != 'Year') strsplit(r, ",")[[1]]
  }))
  mu <- rep(1,nrow(y))
  yr <- y[, 1]
  mn <- y[, 2]
  dy <- y[, 3]
  hr <- convertHHMM(y[,5])
  depart <- ISOdatetime(
    year  = yr,
    month = mn,
    day   = dy,
    hour  = hr[,1],
    min   = hr[,2],
    sec   = mu
  )
  hr <- convertHHMM(y[,6])
  sdepart <- ISOdatetime(
    year  = yr,
    month = mn,
    day   = dy,
    hour  = hr[,1],
    min   = hr[,2],
    sec   = mu
  )
  hr <- convertHHMM(y[,7])
  arrive <- ISOdatetime(
    year  = yr,
    month = mn,
    day   = dy,
    hour  = hr[,1],
    min   = hr[,2],
    sec   = mu
  )
  hr <- convertHHMM(y[,8])
  sarrive <- ISOdatetime(
    year  = yr,
    month = mn,
    day   = dy,
    hour  = hr[,1],
    min   = hr[,2],
    sec   = mu
  )
  d <- data.frame(
    depart           = depart,
    sdepart          = sdepart,
    arrive           = arrive,
    sarrive          = sarrive,
    carrier          = y[, 9],
    origin           = y[, 17],
    dest             = y[, 18],
    dist             = as.numeric(y[, 19]),
    year             = as.numeric(yr),
    month            = as.numeric(mn),
    day              = as.numeric(dy),
    cancelled        = as.numeric(y[, 22]),
    stringsAsFactors = FALSE
  )
  d <- d[order(d$sdepart),]
  rhcollect(d[c(1,nrow(d)), "sdepart"], d)
})
reduce <- expression(
  reduce = {
    lapply(reduce.values, function(i) rhcollect(reduce.key, i))
  }
)
z <- rhwatch(
  map      = map,
  reduce   = reduce,
  input    = rhfmt("/tmp/airline/data/outputs", type = "text"),
  output   = rhfmt("/tmp/airline/output/blocks", type = "sequence"),
  mapred   = list( rhipe_map_buff_size = 5000 ),
  orderby  = "numeric",
  readback = FALSE
)
```

```
...
[Thu Jun 12 10:43:12 2014] Name:2014-06-12 10:13:54 Job: job_201405301308_0878  State: RUNNING Duration: 1757.38
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_0878
             pct numtasks pending running complete killed failed_attempts killed_attempts
map    1.0000000       37       0       0       37      0               0               3
reduce 0.9915602        1       0       1        0      0               0               0
Waiting 5 seconds
[Thu Jun 12 10:43:17 2014] Name:2014-06-12 10:13:54 Job: job_201405301308_0878  State: RUNNING Duration: 1762.414
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_0878
             pct numtasks pending running complete killed failed_attempts killed_attempts
map    1.0000000       37       0       0       37      0               0               3
reduce 0.9936993        1       0       1        0      0               0               0
Waiting 5 seconds
...
```
There are 37 splits for the mapreduce job. A split can consist of many map.values that need to be processed. For text files as input, a split is 
128MB or whatever your Hadoop block size is. In map expression, we first define a R function `convertHHMM()`, which will be used to separate an
input of four digit time record (hhmm) to a vector with hour and minute separately. 
Then we iterate over the `map.values`, which are the lines in text files, and tokenizing them. The first line in each downloaded file is about the
variable names. The first line starts with column year which must be ignored. The lines of text are aggregated using rbind and time related columns 
converted to datetime objects. The data frame is sorted by scheduled departure and saved to disk indexed by the range of scheduled departures in the 
data frame. The size of the value (data frame) is important. RHIPE will can write any sized object but cannot read key/value pairs that are more than
256MB. A data frame of 5000 rows and 12 columns fits very well into 256MB.

Here maybe we can add a small session talking more about Sorting and Shuffling????...Just a thought

### Example set up for counter ###

It is very likely in the future analysis, we want to study the flights information for a specific day. So for this scenario we want to create new
key/value pairs by using RHIPE. The input files are the blocks of data we created previously, and the ouput will be 'sequence' file with key is the
date, and corresponding value is a data frame of data for that particular day. For example, we would like to know what is the delay rate on everyday.


```r
map <- expression({
  a <- do.call("rbind", map.values)
  a$delay.sec <- as.vector(a[,'arrive']) - as.vector(a[,'sarrive'])
  a <- a[!is.na(a$delay.sec),]
  a$isdelayed <- sapply(a$delay.sec, function(r) if(r > 0) TRUE else FALSE)
  e <- split(a, list(a$year, a$month, a$day))
  lapply(e, function(r){
    n <- nrow(r)
    numdelayed <- sum(r$isdelayed)
    rhcollect(as.vector(unlist(c(r[1, c("year", "month", "day")]))), c(n, numdelayed))
  })
})
reduce <- expression(
  pre = {
    sums <- c(0, 0)
  },
  reduce = {
    sums <- sums + apply(do.call("rbind", reduce.values), 2, sum)
  },
  post = { 
    rhcollect(reduce.key, sums) 
  }
)
mapred <- list()
mapred$rhipe_map_buff_size <- 5
z <- rhwatch(
  map      = map,
  reduce   = reduce,
  combiner = TRUE,
  input    = rhfmt("/tmp/airline/output/blocks", type = "sequence"),
  output   = rhfmt("/tmp/airline/output/delaybyday", type = "sequence"),
  mapred   = mapred,
  readback = FALSE
)
```

In map expression, we still iterate over the `map.values`, which are number `rhipe_map_buff_size` of the data frame with 5000 rows and 12 columns
flights information. we define the second of delay as `delay.sec`. Of course, we have to remove all `NA` in `delay.sec` since there are some 
records of flight have `NA` as missing data of arriving time. Then create a flag variable `isdelayed` to identify if the flight is delayed. 
Object `e` is a list which come from the calling of `split()` function. What we get for `e` is a data frame for each day as elements of the 
list. At last, we collect the key which is the date, and value which is a vector with total number of flights and number of delayed flights 
for each element of `e`.

In reduce expression, we initialize the `sums` in `pre` of reduce, which will be the final total number of flights and number of delay for a given day.
And in `reduce` of reduce, we just cumulate all two numbers for same key. Finally, in `post` of reduce, collect the final key/value pairs. 
`reduce.key` here is one particular date of the day, and `reduce.values` is a list with all `c(numberofflight, numberofdelay)` as elements.

We read the output by using `rhread()`, and then grab all the keys assigned to `y1`, grab all the values assigned to `y2`. Based on keys and
values, we create a data frame named `results` with 6 columns. The delay rate is the number of delay divided by the number of total flights 
on that day. Finally, the data frame is sorted by the day.


```r
b <- rhread("/tmp/airline/output/delaybyday")
y1 <- do.call("rbind", lapply(b, "[[", 1))
y1 <- y1[-1, ]
y2 <- do.call("rbind", lapply(b, "[[", 2))
y2 <- y2[-1, ]
results <- data.frame(
  year    = y1[, 1],
  month   = y1[, 2],
  day     = y1[, 3],
  nflight = y2[, 1],
  ndelay  = y2[, 2]
)
results$rate <- results$ndelay/results$nflight
results <- results[order(results$year, results$month, results$day), ]
head(results)
```

```
   year month day nflight ndelay      rate
1  1987    10   1   14759   9067 0.6143370
10 1987    10  10   13417   7043 0.5249311
11 1987    10  11   14016   7790 0.5557934
12 1987    10  12   14792   8376 0.5662520
13 1987    10  13   14859   8623 0.5803217
14 1987    10  14   14799   8806 0.5950402
```
We can do very similar thing that calculating the delay rate, but for each hour, instead of doing that for each day. We only need to change
the key to be the hour variable in data.

```r
map <- expression({
  a <- do.call("rbind",map.values)
  a$delay.sec <- as.vector(a[, 'arrive'])-as.vector(a[, 'sarrive'])
  a <- a[!is.na(a$delay.sec),]
  a$isdelayed <- sapply(a$delay.sec, function(r) if(r > 0) TRUE else FALSE)
  a$hrs <- as.numeric(format(a[, 'sdepart'], "%H"))
  e <- split(a,a$hrs)
  lapply(e, function(r){
    n <- nrow(r) 
    numdelayed <- sum(r$isdelayed)
    rhcollect(as.vector(unlist(c(r[1, c("hrs")]))), c(n, numdelayed))
  })
})
reduce <- expression(
  pre = {
    sums <- c(0,0)
  },
  reduce = {
    sums <- sums + apply(do.call("rbind", reduce.values), 2, sum)
  },
  post = { 
    rhcollect(reduce.key, sums) 
  }
)
mapred <- list()
mapred$rhipe_map_buff_size <- 5
z <- rhwatch(
  map      = map,
  reduce   = reduce,
  combiner = TRUE,
  input    = rhfmt("/tmp/airline/output/blocks/", type = "sequence"),
  output   = rhfmt("/tmp/airline/output/delaybyhours", type = "sequence"),
  mapred   = mapred,
  readback = FALSE
)
```


