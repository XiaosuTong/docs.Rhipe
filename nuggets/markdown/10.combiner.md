## Using a Combiner ##

### The Combiner ###

#### Combiner as an Optimization

Between the map phase and reduce phase of a MapReduce job, Hadoop sends all the intermediate values for a given key to the reducer. The intermediate values for a given key are located on several compute nodes and need to be shuffled (sent across the network) to the node assigned the processing of that intermediate key. This involves a lot of network transfer. 

Some operations do not need access to all of the data (intermediate values), i.e they can compute on subsets and order does not matter, i.e associative and commutative operations. For example, the minimum, or the sum, of some numbers. In these cases, a combiner can be useful.

The idea of the combiner is that the reduce is first run locally on mapper outputs before they are sent for the final reduce. When the combiner is enabled, the reduction occurs just after the map phase on a subset of intermediate values for a given intermediate keys. The output of this is then sent to the reducer. This greatly reduces network transfer and accelerates the job speed, especially if the output from a map contains a lot of data. 


#### Enabling Combiner in RHIPE

Combiner can be enabled in RHIPE by specifying `combiner = TRUE` when calling function `rhwatch()`.

To be able to use a combiner, your **reduce expression needs to pass the same data type as it receives**, i.e the two arguments in the function `rhcollect()` in your map expression need to be of the same type as those in the function `rhcollect()` in your reduce expression. For example, if you pass a string as the key and a numeric vector as the value in the map expression, you need to pass a string as key and a numeric vector as the value in the reduce expression as well.

We will demonstrate the usage of combiners through the following examples.


### Airline Data: Distance by Carrier ###

We will make use of the Airline data introduced in the previous sections, and more specifically, we will use the database of R objects created in the HDFS under `/tmp/airline/output/blocks`.

Suppose we would like to compute the all time distance for each carrier, which is the summation of distance over all flights of each carrier.

#### Without Combiner

First, let's compute the sum without combiner. Here's the RHIPE code for this computation.


```r
map <- expression({
  lapply(map.values, function(r) {
    # r is a data frame with each line being a flight
    # for each flight, collect the carrier and the distance
    Map(rhcollect, r$carrier, r$dist) 
  })
})
reduce <- expression(
  pre = {
    dist = 0
  }, 
  reduce = {
    dist = dist + sum(unlist(reduce.values), na.rm = TRUE)
  },
  post = {
    rhcollect(reduce.key, dist)
  }
)
z <- rhwatch(
  map    = map,
  reduce = reduce,
  input  = "/tmp/airline/output/blocks",
  output = "/tmp/combiner/distance.by.carrier/no.combiner"
)
```


Notice that in the map expression, we used the base R function `Map()` to collect the carrier (as the key) and distance (as the value) for each flight, this is not to be confused with the map in the MapReduce framework.

In the reduce expression, we sum up all values of the distance for each carrier. And `na.rm` argument in the function `sum()` is turned on because for some of the flights, the distance field is missing.

We can read the output of the job and transform into a data frame through the following code:


```r
# read the output
rst = rhread("/tmp/combiner/distance.by.carrier/no.combiner")
# form a data frame for carriers and their distances
df = data.frame(  
  carrier  = sapply(rst, "[[", 1),
  distance = sapply(rst, "[[", 2)
)
# re-arrange the data frame by distance
df = df[order(df$distance, decreasing = TRUE), ]  
# re-assign the row names
row.names(df) = NULL
```


And the resulting data frame `df` will look like this, with American Airlines (AA), Delta Airline (DL), and United Airline (UA) being the top 3:

```
   carrier   distance
1       AA 5547694472
2       DL 4951504638
3       UA 4915176282
4       US 3652941933
5       CO 3025408204
6       NW 2948310164
7       TW 1710404680
8       WN 1628292542
9       HP 1075453193
10      AS  602641612
11      EA  557435834
12      PI  331802193
13  PA (1)  213910356
14  ML (1)   47795815
15      PS   30274790
```

For the purpose of comparison, we will keep track of the running time of this job and the size of data shuffled between map phase and reduce phase. Both of the these can be located on the Hadoop JobTracker web interface, and for this job, the running time is 1mins, 54sec and size of data shuffled is 1,151,750,924 bytes. Keep in mind that these numbers depend on the configuration of the cluster on which the jobs are running, so you might get different values.

#### With Combiner

Now let's enable the combiner. 

Notice that in both map expression and reduce expression of the above code, we are collecting a string as the key and a numeric as the value, so the condition that reduce expression passing the same data type as it receives holds.

All we need to do is to set the argument `combiner` to be `TRUE` when calling function `rhwatch`.


```r
# enabling combiner
z2 <- rhwatch(
  map      = map,
  reduce   = reduce,
  input    = "/tmp/airline/output/blocks",
  output   = "/tmp/combiner/distance.by.carrier/yes.combiner",
  combiner = TRUE
)
```


We can read the output of the job and transform the output into a data frame in the same way:


```r
rst2 = rhread("/tmp/combiner/distance.by.carrier/yes.combiner")
df2 = data.frame(  
  carrier  = sapply(rst2, "[[", 1),
  distance = sapply(rst2, "[[", 2)
)
df2 = df2[order(df2$distance, decreasing = TRUE), ]  
row.names(df2) = NULL
```


Just to verify that using the combiner does not alter the result, we can compare `df2` with `df` from the previous job without combiner:

```r
identical(df, df2)
```

```
[1] TRUE
```

We can once again refer to the Hadoop JobTracker web interface, and now the running time of the job with combiner is 1mins, 42sec, and the size of data shuffled is 10,656 bytes. 

After enabling the combiner, the size of data shuffled has seen significantly reduced, from around 1 GB to 10 KB; and the running timing has also decreased by about 10 seconds. The decrease in running time is not drastic simply because the size of data in this example (less than 4 GB) is relatively small compared with the capacity of the cluster, more specifically, the network bandwidth between servers is 1 GB/sec and it wouldn't take much time even if the whole dataset were to be shuffled. 

The combiner really isn't useful with this tiny data set, but it can be very powerful. For example, on a project we are working on looking at packets from Internet connections, we have one connection with over 60 million packets, where each packet corresponds to a row in a data frame. One of our tasks is to create a data set that contains the first 1500 packets from each connection. The connection data are stored by 10,000 packet chunks by connection. To get the first 1500 packets, the first 1500 packets from each chunk are extracted and sent to the reducer, which combines these and sorts again to retrieve the first 1500 packets. For the large connection, there would be over 6,000 maps passing the same key with 1500 observations each. Without using a combiner, the reducer would receive over 9 million records to sort for this connection, and the job takes a very long time to complete. Using a combiner breaks this down and speeds things up dramatically.

Some operations do not need access to all of the data (intermediate values), i.e they can compute on subsets and order does not matter, i.e associative and commutative operations. For example, the minimum, or the sum, of some numbers. In these cases, a combiner can be useful.


#### Be Careful When Using Combiners

We have demonstrated the usage of the combiner to compute the summation of some numbers in the above example, and all we had to do is to simply turn it on. However, care needs to be taken to make sure that using a combiner does produce the same result as it would without a combiner.

For example, if we were to compute the average distance per flight instead of the sum, we need to do a bit more than just turning on combiner, assuming we replace the function `sum()` with `mean()` of course. This is due to fact that the average of subset averages, which are computed by combiners, is not necessarily the same as the overall average. 

However, this does not prevent us from taking advantage of the combiner, we just need a quick detour: instead of computing the average directly in RHIPE, we could compute the summation and number of values; and then we could read in the output and compute the average in the R global environment.

