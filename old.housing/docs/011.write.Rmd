### Write housing.txt to the HDFS ###

To get started, we need to make `housing.txt` available as a text file within
the HDFS file system. This puts it in a place where it can be read into R, form
subsets, and then write them to the HDFS. This is similar to what we do
using R in the standard serial way. If we have a text file to read into R, we
move put it in a place where we can read it into R, for example, in the working
directory of the R session.

The first step, as for the standard R case, is to copy `housing.txt` to a
directory on the R-session server where your R session is running.
Suppose the path name of this directory is

```{r eval=FALSE, tidy=FALSE}
[1] "/home/myusername/myRdirectory/"
```

The next step is to get `housing.txt` onto the
HDFS as a text file, so we can read it into R on the cluster.
There are Hadoop commands that could be used directly to copy the file, but
our promise to you is that you never need to use Hadoop commands. There is a
a RHIPE function, `rhput()` that will do it for you.

```{r eval=FALSE, tidy=FALSE}
rhput("/home/myusername/mydirectory/housing.txt", "/yourloginname/housing/housing.txt")
```

The first argument specifies the full linux path of the text file on the R
session server to be copied. The second argument is the path name of a
directory on the HDFS.

<!--
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Recall that this directory is on the initiating R server. Then download the data file to your 
local working directory with the following command:

```{r eval=FALSE, tidy=FALSE}
system("wget https://raw.githubusercontent.com/xiaosutong/docs-RHIPE/gh-pages/housing.txt")
```

If it downloaded properly, then "housing.txt" will show up in the output of this command, which lists files
in your local working directory:

```{r eval=FALSE, tidy=FALSE}
list.files(".")
```

This tutorial assumes that you've already installed `RHIPE` using the instructions provided.
Every time we use `RHIPE`, we have to call the `RHIPE` library in R and initialize it.  Your values
for `zips` and `runner` might be different than these, depending on the details of your installation.

```{r eval=FALSE, tidy=FALSE}
library(Rhipe)
rhinit()
rhoptions(zips = "/ln/share/RhipeLib.tar.gz")
rhoptions(runner = "sh ./RhipeLib/library/Rhipe/bin/RhipeMapReduce.sh")
```

Now we want to copy the raw text file to the HDFS.  The function that writes files to HDFS is `rhput()`.  
Replace `tongx` with an appropriate HDFS directory, such as your user name.

```{r eval=FALSE, tidy=FALSE}
rhput("./housing.txt", "/ln/tongx/housing/housing.txt")
```
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-->

The `rhput` function takes two arguments.
The first argument is the path to the local file to be copied, and the second argument is the HDFS path where
the file will be written. `rhput` creates the file at destination, overwriting the destination if
it already exists.
We can also copy files onto HDFS via Hadoop's command line interface, but
`RHIPE` allows us to achieve this task from within R.

We can confirm that the housing data text file has been written to HDFS with the `rhexists` function.
Make sure you specify the same directory as you used in the last step.

```{r eval=FALSE, tidy=FALSE}
rhexists("/ln/tongx/housing/housing.txt")
```
```
[1] TRUE
```

If we want to see more details about a file or directory on HDFS, we can use `rhls()`.
```{r eval=FALSE, tidy=FALSE}
rhls("/ln/tongx/housing")
```
```
  permission owner      group     size          modtime                            file
1 -rw-rw-rw- tongx supergroup 7.683 mb 2014-09-17 11:11   /ln/tongx/housing/housing.txt
```
`rhls()` is very similar to the bash command `ls`.  It will list all content under a given address. 
We can see that the "housing.txt" file with size 11.82Mb is located under "/ln/tongx/housing/" 
on HDFS.

With our data on the HDFS, we are ready to start a D\&R analysis.
