### Read and Divide by State ###

In the first study of the housing data we will divide the data by state so
there will 49 subsets. Each subset will be a `data.frame` object with six
variables. The state variable is not there as a variable, of course, because
there is only one value per subset; it appears on the object as an attribute
with the name `state`.

The first step is to read each line of the file 'house.txt' into R. By
convention there are alreay key-values pairs for the text file.
Each line is a key-value pair. The line number is the key. The value is the
data for each line which are 7 observations of the 7 variables of the data,
which consist of the data for one month and one county. However, when a line is
read into R, each observation becomes type character.

Each line is read as part of a Map R code written by the user.
The output of the function is a
key-value pair for each line. The key is the observed state variable for the
line, and the value is the 6 observations of the other variables for the line.
The Map outputs are the inputs for Reduce R code written by the user.
Reduce assembles the line outputs into groups by state, and creates the
subset `dataframe' object for each state, and then writes every object to a
directory in the HDFS specfied by the user.


#### The Map R Code ####

Here is the R code.

```{r eval =FALSE, tidy=FALSE}
map1 <- expression({
  lapply(seq_along(map.keys), function(r) {
    line <- strsplit(map.values[[r]], ",")[[1]]
    outputkey <- line[3]
    outputvalue <- data.frame(
	  FIPS = line[1], 
	  county = line[2],
	  date = as.numberic(line[4]), 
	  units =  as.numeric(line[5]), 
	  list = as.numeric(line[6]), 
	  selling = as.numeric(line[7])  
	  stringsAsFactors = FALSE
	)
    rhcollect(key, value)
  })
})
```
This code is really a `for loop`, but is done by `lapply()` because it is in
general faster. But we could have used `for r in 1:length(map.keys)`.
The loop proceeds throught the input keys, which are the line numbers, as
specified by the first argument of `lapply`. The R `list` object `map.keys`
contains them; it is created by the RHIPE R function that manages the running
of the user Map and Reduce R code. We will show this function later. The
second argument of the above `lapply` defines the Map function with the
argument `r`, an index for the Map keys that goes from 1 to `length(map.keys)`. 

`map.values` is also a `list` object created by the `RHIPE` function;
`map.values[[r]]` is the value for key `map.keys[[r]]`.
The result of `line` in the `lapply` function is the input values, a
string.  The function `strsplit()` takes those values into the individual
observations of each text line, and creates a `list` of length one whose
element is a vector of length 7 that becomes `line`. `key` is the state
observation for the file line. `value` is a `dataframe` with one row and 6
columns of the values of variables for the text line that omits the state.

The RHIPE function `rhcollect()` forms a key-value for each line.

#### The Reduce R Code ####

```{r eval=FALSE, tidy=FALSE}
reduce1 <- expression(
  pre = {
    reduceoutputvalue <- data.frame()
  },
  reduce = {
    reduceoutputvalue <- rbind(reduceoutputvalue, do.call(rbind, reduce.values))
  },
  post = {
    attr(reduceoutputvalue), "state") <- reduce.key
    rhcollect(reduce.key, reduceoutputvalue)
  }
)
```

`reduce.values` is a `list` object. Each element is the Reduce input values for
one Reduce input key.

Reduce groups the Map output key-value pairs are grouped by output key,
the state name.  The current group's key is available in the object `reduce.key`, and all values associated with that key
are elements of the list object `reduce.values`.  The reduce expression has three parts: `pre`, which is 
executed once first; `reduce`, which is executed repeatedly until all intermediate values associated with
the current key have been processed; and `post`, which is executed once at the end.  

In the reduce expression above, our goal is to combine all observations associated with one particular
state into a single data frame.  In `pre`, we initialize an empty data frame, `oneState`.  In `reduce`, 
we use `rbind`
to combine all observations associated with one particular state.  In `post`, we add an attribute to the
data frame containing the state name and emit the final key-value
pair.  The key is the state name, and the value is the data frame with all observations belonging to
that state.  These final key-value pairs are written to HDFS, and will persist for subsequent analyses.

#### Execution Function ####

```{r eval=FALSE, tidy=FALSE}
mr1 <- rhwatch(
  map      = map1,
  reduce   = reduce1,
  input    = rhfmt("/ln/tongx/housing/housing.txt", type = "text"),
  output   = rhfmt("/ln/tongx/housing/byState", type = "sequence"),
  mapred   = list(
    mapred.reduce.tasks = 10
  ),
  readback = FALSE
)
```

The `rhwatch` function packages and executes our `RHIPE` MapReduce job.  In addition to the `map` and
`reduce` expressions created above, we specify the HDFS locations of the input and output for this
MapReduce job.  The input is the location where we stored our raw text file using `rhput` in the
previous section.  The output is any location on HDFS we choose.  Be careful, as any existing data
in the output location will be overwritten.

The `mapred` argument contains optional configuration parameters for the MapReduce job.  In this
case, we've specified 10 reduce tasks using `mapred.reduce.tasks`.  This means that of the 49 groups
of key-value pairs corresponding to the 49 states in our data set, 10 at a time will be processed
in parallel.  Specifying 10 reduce tasks also means that the output written to HDFS will be broken
into 10 files (we'll come back to this point in the next section).  Finally, `readback = FALSE` 
tells `RHIPE` not to read the final output from HDFS into global environment of our interactive R 
session.  We'll do that with a separate command. 

```
Saving 1 parameter to /tmp/rhipe-temp-params-bbb96e029776c9953476a54c74d9eaf7 (use rhclean to delete all temp files)
[Thu Sep 18 22:25:52 2014] Name:2014-09-18 22:25:52 Job: job_201405301308_4709  State: PREP Duration: 0.203
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4709
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0        1       1       0        0      0               0               0
reduce   0       10      10       0        0      0               0               0
Waiting 5 seconds
[Thu Sep 18 22:25:57 2014] Name:2014-09-18 22:25:52 Job: job_201405301308_4709  State: RUNNING Duration: 5.24
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4709
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0        1       0       1        0      0               0               0
reduce   0       10      10       0        0      0               0               0
Waiting 5 seconds
``` 

After our job has completed successfully, the output will be in the location we specified on the HDFS.
Since this data set is quite small, we can read the whole thing from HDFS into our interactive R
environment using `rhread`.  All we have to specify is the HDFS location we wish to read from.

```{r eval=FALSE, tidy=FALSE}
stateSubsets <- rhread("/ln/tongx/housing/byState")
```
```
Read 49 objects(13.52 MB) in 1.41 seconds
```

`RHIPE` conveniently packages the key-value pairs in our HDFS output location as a nested list, which we've
assigned to the variable `stateSubsets`.  Since there were 49 key-value pairs in the output of our reduce
stage, there are 49 elements in `stateSubsets`.  Each element is itself a list with two elements: a key
and a value.  In this case, the keys are character strings containing the state names, and the 
values are data frames, just as they should be based on our reduce code.

We can take a look at the data frame contained in the first key-value pair.  The key-value pairs
are in no particular order.    

```{r eval=FALSE, tidy=FALSE}
head(stateSubsets[[1]][[2]])
```

We can also look at the structure of `stateSubsets` and confirm that it's what we expected.

```{r eval=FALSE, tidy=FALSE}
str(stateSubsets)
```

```
 $ :List of 2
  ..$ : chr "WV"
  ..$ :'data.frame':  3630 obs. of  6 variables:
  .. ..$ FIPS   : chr [1:3630] "54103" "54103" "54033" "54051" ...
  .. ..$ county : chr [1:3630] "Wetzel" "Wetzel" "Harrison" "Marshall" ...
  .. ..$ date   : chr [1:3630] "65" "66" "66" "1" ...
  .. ..$ units  : num [1:3630] NA NA NA NA NA NA NA NA NA NA ...
  .. ..$ list   : num [1:3630] 57.9 52.7 91.4 67 63.8 ...
  .. ..$ selling: num [1:3630] NA NA NA NA NA NA NA NA NA NA ...
  .. ..- attr(*, "state")= chr "WV"
 $ :List of 2
  ..$ : chr "KY"
  ..$ :'data.frame':  7458 obs. of  6 variables:
  .. ..$ FIPS   : chr [1:7458] "21173" "21173" "21155" "21155" ...
  .. ..$ county : chr [1:7458] "Montgomery" "Montgomery" "Marion" "Marion" ...
  .. ..$ date   : chr [1:7458] "2" "1" "66" "65" ...
  .. ..$ units  : num [1:7458] NA NA NA NA NA NA NA NA NA NA ...
  .. ..$ list   : num [1:7458] 76 80.2 60.6 71.5 72.5 ...
  .. ..$ selling: num [1:7458] NA NA NA NA NA NA NA NA NA NA ...
  .. ..- attr(*, "state")= chr "KY"
 $ :List of 2
  ..$ : chr "NV"
  ..$ :'data.frame':	1056 obs. of  6 variables:
  .. ..$ FIPS   : chr [1:1056] "32029" "32029" "32029" "32029" ...
  .. ..$ county : chr [1:1056] "Storey" "Storey" "Storey" "Storey" ...
  .. ..$ date   : chr [1:1056] "3" "4" "5" "6" ...
  .. ..$ units  : num [1:1056] NA NA NA NA NA NA 11 NA 11 12 ...
  .. ..$ list   : num [1:1056] 155 149 143 147 149 ...
  .. ..$ selling: num [1:1056] NA NA NA NA NA NA NA NA NA NA ...
  .. ..- attr(*, "state")= chr "NV"
......
```

We've now successfully divided our data into subsets and stored each subset as an R object.  
They will persist on HDFS and be used for many analytic methods, each applied using `RHIPE`. 


