### Read and Divide by Date ###

Same as before, we can create the subsets of dataset divided by date variable. But here one of
potential problem that we may face is that what if some of the subsets are very large? This is
a very general problem if we are dealing with a large and complex dataset. In fact, `RHIPE` is 
able to write any size of one subset to HDFS, but only can read one subset less than 256 Mb 
from HDFS. In other words, we are able to create any size of subset in a mapreduce job, but we
will get issue when we are trying to read and apply analysis method on oversize subset.

In this example, we are assuming that each subset by date is oversize. One way to overcome this
issue is that we are going to create multiple subsets for each month. 

#### Map ####

```{r eval=FALSE, tidy=FALSE}
map5 <- expression({
  lapply(seq_along(map.keys), function(r) {
    line = strsplit(map.values[[r]], ",")[[1]]
    key <- line[[4]]
    value <- as.data.frame(rbind(line[-4]), stringsAsFactors = FALSE)
    rhcollect(key, value)
  })
})
```

There is not too much difference in the map expression comparing with the map expression in previous 
two division sessions. We used the fourth column in each row of text file as the key which is the index
of month, and rest of columns saved as a data frame was collected as corresponding value.

#### Reduce ####

```{r eval=FALSE, tidy=FALSE}
reduce5 <- expression(
  pre = {
  },
  reduce = {
    date500 <- do.call(rbind, reduce.values)
    names(date500) <- c(
      "FIPS", "county", "state", 
      "units", "list", "selling"
    )
    date500$list <- as.numeric(date500$list)
    date500$selling <- as.numeric(date500$selling)
    rhcollect(reduce.key, date500)
  },
  post = {
  }
)
```

Here, the reduce expression is not similar as before. We left `pre` and `post` as empty. Recall that `pre` 
and `post` in reduce expression will be executed only once for each unique key. `reduce` part, on the other 
hand, will be executed repeatedly until all intermediate values associated with the current key have been 
processed. And we actually can control how many intermediate values to be executed in `reduce` every time. 
Suppose we want to execute 500 intermediate values for each key at one time, then the `reduce.values` is a 
list only includes 500 intermediate values for corresponding `reduce.key`. In other words, we use the 
property that intermediate values for a given key can be executed by part in `reduce` to create 
multiple subsets for a given key. Previously we only created one subset for a given key, like a state or 
county.

What we have done above in `reduce` is for a given key, for every 500 (later one we will illustrate where 
we specified this 500) intermediate value corresponding to this key, we created a data.frame named
`date500` which is the row-bind of all these 500 rows(recall that each value is a data frame with single 
row from map output) for this key. Then we collected `date500` as value with the key. Then is the next
500 rows for the same key, repeatedly until all rows for this key has been processed. If, for example, for 
one key we have 1324 values(rows), then finally we will get three key-value pairs with same key by using 
this reduce expression.

#### Execution Function ####

```{r eval=FALSE, tidy=FALSE}
mr5 <- rhwatch(
  map      = map5,
  reduce   = reduce5,
  input    = rhfmt("/ln/tongx/housing/housing.txt", type = "text"),
  output   = rhfmt("/ln/tongx/housing/bydate", type = "sequence"),
  mapred   = list(
    mapred.reduce.tasks = 10,
    rhipe_reduce_buff_size = 500
  ),
  readback = FALSE
)
```

In `rhwatch` this time, we add one more argument in `mapred` list. `rhipe_reduce_buff_size` is the 
argument used to control how many intermediate values for one key we want to process at one time. It is
where we specified that process 500 rows for one key at one time, as we mentioned in reduce expression.

Let us examine if we got what we want. Recall that we have 2,883 counties in total, and each county has
66 monthly observations. If we only create one subset for each month, there should be 66 subsets, each of
which is a data frame with 2,883 rows.

```{r eval=FALSE, tidy=FALSE}
rst <- rhread("/ln/tongx/housing/bydate")
```
```
Read 396 objects(8.85 MB) in 2.76 seconds
```

But we got 396 subsets in `rst`, which is 66 times 6. And if you examine the `rst` more carefully, it is
not hard to find that for each month, we created 6 subsets.

```{r eval=FALSE, tidy=FALSE}
rows <- unlist(lapply(rst, function(r){dim(r[[2]])[1]}))
rows
```
```
  [1] 500 500 500 500 500 383 500 500 500 500 500 383 500 500 500 500 500 383 500 500 500 500
 [23] 500 383 500 500 500 500 500 383 500 500 500 500 500 383 500 500 500 500 500 383 500 500
......
```
What we got above is the number of rows in each subset. We can see that we always have five `500` and one 
`383` as one replicate, which together will be the total number of rows for one month. This means we ran 
our `reduce` in reduce expression six times for each key.
