## Housing Data ##

### Information about Data ###

We introduce `RHIPE` using
housing data that consist of 7 monthly variables on housing sales from Oct
2008 to Mar 2014 for 2883 counties in the 49 U.S. states, excluding Hawaii and
Alaska, but including the District of Columbia which for simplicity here we
will refer to as a ``state''.  The data were harvested from Quandl's Zillow
Housing Data. The data contain 224,369 sales of housing units.

The variables are
- **FIPS**: FIPS county code, an unique identifier for each U.S. county
- **county**: county name
- **state**: state abbreviation
- **date**: time measured in months, from 1 to 66, the number of months
- **units**: number of units sold
- **list**: monthly median list price (dollars per square foot)
- **selling**: monthly median selling (dollars per square foot)

Many observations of the last three variables are missing: units 68%, list 7%,
and selling 68%.
 
The raw data are in a text file whose size is about 8 MB.
It is available as "housing.txt" in our Tesseradata Github repository of the
`RHIPE` documentation [here](https://raw.githubusercontent.com/xiaosutong/docs-RHIPE/gh-pages/housing.txt).   

So the raw data are a table with 190,278 rows (units) and 7 columns (variables).
In the text file, the fields in each row are separated by a comma, and there
are not headers
Here are the first few lines of the file:
```
01001,Autauga,AL,1,27,96.616541353383,99.1324
01001,Autauga,AL,2,28,96.856993190152,95.8209
01001,Autauga,AL,3,16,98.055555555556,96.3528
01001,Autauga,AL,4,23,97.747480735033,95.2189
01001,Autauga,AL,5,22,97.747480735033,92.7127
```

A housing unit is a house, an apartment, a mobile home, a group of rooms, or a
single room that is occupied or intended to occupied  as
separate living quarter. Separate living quarters are those in which the
occupants live and eat separately from any other persons in the 
building and which have direct access from the outside of the building or
through a common hall.

### Write Text to HDFS ###

First of all, you can double-check the name of your local working directory with the `getwd` command
in R.  Mine is called "/home/median/u41/tongx" but yours will be different.


```r
getwd()
```
```
[1] "/home/median/u41/tongx"
```

Recall that this directory is on the initiating R server. Then download the data file to your 
local working directory with the following command:


```r
system("wget https://raw.githubusercontent.com/xiaosutong/docs-RHIPE/gh-pages/housing.txt")
```

If it downloaded properly, then "housing.txt" will show up in the output of this command, which lists files
in your local working directory:


```r
list.files(".")
```

This tutorial assumes that you've already installed `RHIPE` using the instructions provided.
Every time we use `RHIPE`, we have to call the `RHIPE` library in R and initialize it.  Your values
for `zips` and `runner` might be different than these, depending on the details of your installation.


```r
library(Rhipe)
rhinit()
rhoptions(zips = "/ln/share/RhipeLib.tar.gz")
rhoptions(runner = "sh ./RhipeLib/library/Rhipe/bin/RhipeMapReduce.sh")
```

Now we want to copy the raw text file to the HDFS.  The function that writes files to HDFS is `rhput()`.  
Replace `tongx` with an appropriate HDFS directory, such as your user name.


```r
rhput("./housing.txt", "/ln/tongx/housing/housing.txt")
```

The `rhput` function takes two arguments.
The first argument is the path to the local file to be copied, and the second argument is the HDFS path where
the file will be written. `rhput` creates the file at destination, overwriting the destination if
it already exists.  We can also copy files onto HDFS via Hadoop's command line interface, but
`RHIPE` allows us to achieve this task from within R.

We can confirm that the housing data text file has been written to HDFS with the `rhexists` function.
Make sure you specify the same directory as you used in the last step.


```r
rhexists("/ln/tongx/housing/housing.txt")
```
```
[1] TRUE
```

If we want to see more details about a file or directory on HDFS, we can use `rhls()`.

```r
rhls("/ln/tongx/housing")
```
```
  permission owner      group     size          modtime                            file
1 -rw-rw-rw- tongx supergroup 7.683 mb 2014-09-17 11:11   /ln/tongx/housing/housing.txt
```
`rhls()` is very similar to the bash command `ls`.  It will list all content under a given address. 
We can see that the `housing.txt` file with size 11.82Mb is located under `/user/tongx/housing/` 
on HDFS.

With our data on the HDFS, we are ready to start a D\&R analysis.

### Read and Divide by State ###

The first step in a D\&R analysis is to choose a division method and create subsets.  We'll
divide the housing data by state. `RHIPE` allows us to read the raw text file on HDFS and 
create a copy of the data broken into subsets using a MapReduce job.  The original data file
will remain on HDFS. The data within each subset will be stored as R objects, rather than raw
text.  This will allow us to apply subsequent data analytic and visual methods using familiar
R commands.

In the MapReduce framework, we keep track of subsets as key-value pairs.  The key is a identifier for
the subset.  It's usually a small data object, like a single number or character string.  There
can be multiple subsets that share a common key.  The value is where we store the data.  For 
example, it may be an R data frame.

The map is a function applied to every key-value pair.  Its output is one or more key-value
pairs, where the key may or may not have changed from the input key.  The output key-value 
pairs of the map are sometimes called intermediate key-value pairs, because they will be the
input to the reduce function.  The intermediate key-value pairs are grouped by key, and then
the reduce function is applied to each group.  The output of the reduce function is again
one or more key-value pairs.  `RHIPE` allows us to write the map and reduce functions in
R code, using R objects for the keys and values.

A valid MapReduce job in Rhipe consists of a map expression, an optional reduce expression, 
and an execution function `rhwatch()`.  If no reduce expression is given, the output of the
map is the final output.

#### Map ####


```r
map1 <- expression({
  lapply(seq_along(map.keys), function(r) {
    line = strsplit(map.values[[r]], ",")[[1]]
    key <- line[3]
    value <- as.data.frame(rbind(line[-3]), stringsAsFactors = FALSE)
    names(value) <- c(
      "FIPS", "county", "date", 
      "units", "list", "selling"
    )
    value$list <- as.numeric(value$list)
    value$selling <- as.numeric(value$selling)
    value$units <- as.numeric(value$units)
    rhcollect(key, value)
  })
})
```

The input keys are line numbers in the housing data text file.  They are the elements of the list
object `map.keys`.  The input values are the lines of text, which are the elements of the list 
object `map.values`. This is the default when the input is a raw text file.  For each input key-value 
pair, `rhcollect` emits an intermediate key-value pair, where the key is the state name (the third 
field in the comma-separated line) and the value is all other fields in the line, stored as a data 
frame with a single row.

#### Reduce ####


```r
reduce1 <- expression(
  pre = {
    oneState <- data.frame()
  },
  reduce = {
    oneState <- rbind(oneState, do.call(rbind, reduce.values))
  },
  post = {
    attr(oneState, "state") <- reduce.key
    rhcollect(reduce.key, oneState)
  }
)
```

Between the map and reduce stages, the intermediate key-value pairs are grouped by key (the state name).
The current group's key is available in the object `reduce.key`, and all values associated with that key
are elements of the list object `reduce.values`.  The reduce expression has three parts: `pre`, which is 
executed once first; `reduce`, which is executed repeatedly until all intermediate values associated with
the current key have been processed; and `post`, which is executed once at the end.  

In the reduce expression above, our goal is to combine all observations associated with one particular
state into a single data frame.  In `pre`, we initialize an empty data frame, `oneState`.  In `reduce`, 
we use `rbind`
to combine all observations associated with one particular state.  In `post`, we add an attribute to the
data frame containing the state name and emit the final key-value
pair.  The key is the state name, and the value is the data frame with all observations belonging to
that state.  These final key-value pairs are written to HDFS, and will persist for subsequent analyses.

#### Execution Function ####


```r
mr1 <- rhwatch(
  map      = map1,
  reduce   = reduce1,
  input    = rhfmt("/ln/tongx/housing/housing.txt", type = "text"),
  output   = rhfmt("/ln/tongx/housing/byState", type = "sequence"),
  mapred   = list(
    mapred.reduce.tasks = 10
  ),
  readback = FALSE
)
```

The `rhwatch` function packages and executes our `RHIPE` MapReduce job.  In addition to the `map` and
`reduce` expressions created above, we specify the HDFS locations of the input and output for this
MapReduce job.  The input is the location where we stored our raw text file using `rhput` in the
previous section.  The output is any location on HDFS we choose.  Be careful, as any existing data
in the output location will be overwritten.

The `mapred` argument contains optional configuration parameters for the MapReduce job.  In this
case, we've specified 10 reduce tasks using `mapred.reduce.tasks`.  This means that of the 49 groups
of key-value pairs corresponding to the 49 states in our data set, 10 at a time will be processed
in parallel.  Specifying 10 reduce tasks also means that the output written to HDFS will be broken
into 10 files (we'll come back to this point in the next section).  Finally, `readback = FALSE` 
tells `RHIPE` not to read the final output from HDFS into global environment of our interactive R 
session.  We'll do that with a separate command. 

```
Saving 1 parameter to /tmp/rhipe-temp-params-bbb96e029776c9953476a54c74d9eaf7 (use rhclean to delete all temp files)
[Thu Sep 18 22:25:52 2014] Name:2014-09-18 22:25:52 Job: job_201405301308_4709  State: PREP Duration: 0.203
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4709
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0        1       1       0        0      0               0               0
reduce   0       10      10       0        0      0               0               0
Waiting 5 seconds
[Thu Sep 18 22:25:57 2014] Name:2014-09-18 22:25:52 Job: job_201405301308_4709  State: RUNNING Duration: 5.24
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4709
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0        1       0       1        0      0               0               0
reduce   0       10      10       0        0      0               0               0
Waiting 5 seconds
``` 

After our job has completed successfully, the output will be in the location we specified on the HDFS.
Since this data set is quite small, we can read the whole thing from HDFS into our interactive R
environment using `rhread`.  All we have to specify is the HDFS location we wish to read from.


```r
stateSubsets <- rhread("/ln/tongx/housing/byState")
```
```
Read 49 objects(13.52 MB) in 1.41 seconds
```

`RHIPE` conveniently packages the key-value pairs in our HDFS output location as a nested list, which we've
assigned to the variable `stateSubsets`.  Since there were 49 key-value pairs in the output of our reduce
stage, there are 49 elements in `stateSubsets`.  Each element is itself a list with two elements: a key
and a value.  In this case, the keys are character strings containing the state names, and the 
values are data frames, just as they should be based on our reduce code.

We can take a look at the data frame contained in the first key-value pair.  The key-value pairs
are in no particular order.    


```r
head(stateSubsets[[1]][[2]])
```

We can also look at the structure of `stateSubsets` and confirm that it's what we expected.


```r
str(stateSubsets)
```

```
List of 49
 $ :List of 2
  ..$ : chr "WV"
  ..$ :'data.frame':  3836 obs. of  6 variables:
  .. ..$ FIPS   : chr [1:3836] "54001" "54109" "54109" "54109" ...
  .. ..$ county : chr [1:3836] "Barbour" "Wyoming" "Wyoming" "Wyoming" ...
  .. ..$ date   : chr [1:3836] "1" "66" "65" "64" ...
  .. ..$ units  : num [1:3836] NA NA NA NA NA NA NA NA NA NA ...
  .. ..$ list   : num [1:3836] 71.7 50 50 44.4 44.4 ...
  .. ..$ selling: num [1:3836] NA NA NA NA NA NA NA NA NA NA ...
  .. ..- attr(*, "state")= chr "WV"
 $ :List of 2
  ..$ : chr "KY"
  ..$ :'data.frame':	8059 obs. of  6 variables:
  .. ..$ FIPS   : chr [1:8059] "21033" "21033" "21239" "21239" ...
  .. ..$ county : chr [1:8059] "Caldwell" "Caldwell" "Woodford" "Woodford" ...
  .. ..$ date   : chr [1:8059] "83" "84" "69" "68" ...
  .. ..$ units  : num [1:8059] NA NA NA NA NA NA NA NA NA NA ...
  .. ..$ list   : num [1:8059] NA 75 97.2 96.3 97.1 ...
  .. ..$ selling: num [1:8059] 45.4 NA NA NA NA ...
  .. ..- attr(*, "state")= chr "KY"
 $ :List of 2
  ..$ : chr "NV"
  ..$ :'data.frame':	1284 obs. of  6 variables:
  .. ..$ FIPS   : chr [1:1284] "32019" "32019" "32019" "32019" ...
  .. ..$ county : chr [1:1284] "Lyon" "Lyon" "Lyon" "Lyon" ...
  .. ..$ date   : chr [1:1284] "24" "25" "26" "27" ...
  .. ..$ units  : num [1:1284] NA NA 84 91 135 NA NA 134 134 148 ...
  .. ..$ list   : num [1:1284] 90.3 NA 89.9 88.2 NA ...
  .. ..$ selling: num [1:1284] NA 74.2 NA 72.8 71 ...
  .. ..- attr(*, "state")= chr "NV"
 $ :List of 2
  ..$ : chr "OK"
  ..$ :'data.frame':	6468 obs. of  6 variables:
  .. ..$ FIPS   : chr [1:6468] "40151" "40151" "40151" "40151" ...
  .. ..$ county : chr [1:6468] "Woods" "Woods" "Woods" "Woods" ...
  .. ..$ date   : chr [1:6468] "57" "55" "54" "53" ...
  .. ..$ units  : num [1:6468] NA 13 NA NA NA NA NA NA NA NA ...
  .. ..$ list   : num [1:6468] 51.7 53.7 NA 53.5 NA ...
  .. ..$ selling: num [1:6468] NA NA NA NA NA NA NA NA NA NA ...
  .. ..- attr(*, "state")= chr "OK"
......
```

We've now successfully divided our data into subsets and stored each subset as an R object.  
They will persist on HDFS and be used for many analytic methods, each applied using `RHIPE`. 

### Compute State  Means ###

Now we'll apply an analytic method to the subsets of our housing data.  Our analytic
method will be to take the mean.  We have monthly list prices and sale prices per square foot, 
so our desired output is two numbers for each state: a mean of list prices and a mean 
of sale prices.  The recombination step will be to put all the state means in a single data
frame, which can be further analyzed in an interactive R session.

We apply our chosen analytic method with a second `RHIPE` MapReduce job:

#### Map ####


```r
map2 <- expression({
  lapply(seq_along(map.keys), function(r) {
    value <- data.frame(
      state = attr(map.values[[r]], "state"),
      listMean = mean(map.values[[r]]$list, na.rm = TRUE),
      saleMean = mean(map.values[[r]]$selling, na.rm = TRUE),
      stringsAsFactors = FALSE
    )
    rhcollect(1, value)
  })
})
```

The input to our map is the the final output we created in the previous section.  The keys 
were state names, stored as character objects, and the values were data frames with prices,
dates, and county information.  In general, 
keys need not be unique, but in this case we know that there are 49 state names, where no state
is duplicated.  

To each state's data frame, we apply the same action: take the mean of the list price and 
the mean of the sale price.  These are stored in a single row data frame to be recombined in 
the reduce step below.  The row has three columns: state name plus the two calculated means.
As before, `rhcollect` emits intermediate key-value pairs.  One key-value pair is emitted per
state.  The value is the single row data frame we just created.  We use the same key for all 49
states so that they will all appear together in the reduce step.  The key itself is meaningless,
so we'll use 1 as a placeholder.

#### Reduce ####


```r
reduce2 <- expression(
  pre = {
    allMeans <- data.frame()
  },
  reduce = {
    allMeans <- rbind(allMeans, do.call(rbind, reduce.values))
  },
  post = {
    rhcollect(reduce.key, allMeans)
  }
)
```

We divided our data into subsets and applied an analytic method to each subset.  The reduce step
above is our recombination.  We recombine the state means into a data frame with 49 rows and 3
columns.  Later, we can read that data frame from HDFS into our interactive R session for further
analysis.

The input key is the placeholder value 1, which is left unchanged as the output key.  The input
values are the single row data frames for each state, and the output values is the 49 row data 
frame with all states represented.  In the `pre` step we initialize an empty data frame.  In
`reduce` we use `rbind` to append the single row data frames to the empty data frame we created, 
and in `post` we use `rhcollect` to emit the final output so that it will be written to HDFS.

#### Execution Function ####


```r
stateMeans <- rhwatch(
  map      = map2,
  reduce   = reduce2,
  input    = rhfmt("/ln/tongx/housing/byState", type = "sequence"),
  output   = rhfmt("/ln/tongx/housing/meanByState", type = "sequence"),
  mapred   = list( 
    mapred.reduce.tasks = 5
  ),
  readback = TRUE
)
```

In `rhwatch()` this time, we've changed several arguments. First, in the `rhfmt` of `input` argument,
`type` is specified to be "sequence", since the input file to this mapreduce job is the output
from our division. This indicates to `RHIPE` that the input is not a raw text file, but rather a 
file already organized as key-value pairs.  Also we request 5 reduce tasks for this job using
the `mapred.reduce.tasks` option.  Finally, we assign
`readback` to be `TRUE`. By doing this, the final results not only will be saved on HDFS, but also
will be read back from HDFS (without using `rhread()`) and assigned to an object in our interactive R
session.  We've named that object `stateMeans`.

Just as before, once we submit the job, we see job status information.

```
[Thu Sep 18 23:48:19 2014] Name:2014-09-18 23:48:19 Job: job_201405301308_4715  State: PREP Duration: 0.175
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4715
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0       10      10       0        0      0               0               0
reduce   0        5       5       0        0      0               0               0
Waiting 5 seconds
[Thu Sep 18 23:48:24 2014] Name:2014-09-18 23:48:19 Job: job_201405301308_4715  State: RUNNING Duration: 5.206
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4715
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0       10       0      10        0      0               0               0
reduce   0        5       5       0        0      0               0               0
Waiting 5 seconds
.......
Read 1 objects(2.42 KB) in 0.06 seconds
```


```r
str(stateMeans)
```
```
List of 1
 $ :List of 2
  ..$ : num 1
  ..$ :'data.frame':  49 obs. of  3 variables:
  .. ..$ state      : Factor w/ 49 levels "IA","KS","AZ",..: 1 2 3 4 5 6 7 8 9 10 ...
  .. ..$ listmean   : num [1:49] 74.5 66 110.6 369.1 151 ...
  .. ..$ sellingmean: num [1:49] 95.1 NaN 101.3 422.5 143.4 ...
```

As we can see, the result is a list of length 1.  This make sense, since we only output a single
key-value pair.  This single element is itself a list of length 2: one element for the key, and 
another for the value.  The value is what we're interested in, namely, the data frame of 49 state means.
It has three columns which are state abbreviation, mean of median list price per square feet, and mean of 
median sale price per square feet.
 

```r
stateMeans <- stateMeans[[1]][[2]]
head(stateMeans)
```
```
   state listMean    saleMean
4     DC 369.0546    422.4744
45    MA 259.1316    210.8714
44    CA 192.6343    185.8716
10    RI 188.9953    174.9670
46    NJ 180.0136    171.4495
22    CT 162.4686    151.7553
```

Let's take a look at how the output was stored on HDFS using `rhls`.


```r
rhls("/ln/tongx/housing/meanByState")
```
```
  permission owner      group        size          modtime                                         file
1 -rw-r--r-- tongx supergroup           0 2014-09-18 23:56     /ln/tongx/housing/meanByState/_SUCCESS
2 drwxrwxrwx tongx supergroup           0 2014-09-18 23:56        /ln/tongx/housing/meanByState/_logs
3 -rw-r--r-- tongx supergroup    1.363 kb 2014-09-18 23:56 /ln/tongx/housing/meanByState/part-r-00000
4 -rw-r--r-- tongx supergroup    94 bytes 2014-09-18 23:56 /ln/tongx/housing/meanByState/part-r-00001
5 -rw-r--r-- tongx supergroup    94 bytes 2014-09-18 23:56 /ln/tongx/housing/meanByState/part-r-00002
6 -rw-r--r-- tongx supergroup    94 bytes 2014-09-18 23:56 /ln/tongx/housing/meanByState/part-r-00003
7 -rw-r--r-- tongx supergroup    94 bytes 2014-09-18 23:56 /ln/tongx/housing/meanByState/part-r-00004
```

There are five files in "/ln/tongx/housing/meansByState", named from "part-r-00000" to 
"part-r-00004". There are five of these because we selected 5 reduce tasks with `mapred.reduce.tasks`.
Besides these five files, there are another two files named "_SUCCESS" and 
"_logs" which record the metadata and log information. 

Notice that four of the five files have the same size, 94 bytes, which is quite small. 
This is because those four files are 
empty. Since we only had one intermediate key-value pair (the output of the map), four of the 
reduce tasks we requested did nothing.

We've now successfully completed our first D\&R analysis with `RHIPE`.  We created a division by state, applied an
analytic method to each subset when we took the mean, and recombined the subset outputs into an R
data frame for further interactive analysis.  We did the division in one Rhipe MapReduce job, and 
the analytic method and recombination in a second job.


### Read and Divide by County ###

As we showed previously, we divided the whole data set to subsets by state, and then calculated the mean list
and selling price for each state. According to different purpose of the data analysis, we sometime would
like to divide the data set by different variables. So we are going to show that what if we divide our housing
data by county from the text file as well.

The input keys are line numbers in the housing data text file.  They are the elements of the list
object `map.keys`.  The input values are the lines of text, which are the elements of the list object `map.values`.
This is the default when the input is a raw text file.  For each input key-value pair, `rhcollect` emits an
intermediate key-value pair, where the key is the state name (the third field in the comma-separated
line) and the value is all other fields in the line, stored as a data frame with a single row.

#### Map ####


```r
map3 <- expression({
  lapply(seq_along(map.keys), function(r) {
    line = strsplit(map.values[[r]], ",")[[1]]
    key <- line[1:3]
    value <- as.data.frame(rbind(line[c(-1, -2, -3)]), stringsAsFactors = FALSE)
    names(value) <- c("date", "units", "list", "selling")
    value$list <- as.numeric(value$list)
    value$selling <- as.numeric(value$selling)
    rhcollect(key, value)
  })
})
```

Note that we want to use the FIPS code as a unique identifier for the county, since counties in
different states can share a common name.  This time we've used a character vector of length 3
as the key.  It contains the unique FIPS code, the county name, and the state name.

#### Reduce ####


```r
reduce3 <- expression(
  pre = {
    oneCounty <- data.frame()
  },
  reduce = {
    oneCounty <- rbind(oneCounty, do.call(rbind, reduce.values))
  },
  post = {
    attr(oneCounty, "FIPS") <- reduce.key[1]
    attr(oneCounty, "county") <- reduce.key[2]
    attr(oneCounty, "state") <- reduce.key[3]
    rhcollect(reduce.key, oneCounty)
  }
)
```

By removing the FIPS, county, and state columns from the data frame and storing them as
attributes, we've eliminated redundant information in each data frame. Working with massive 
data sets, we want our data to take up the least possible space on disk in order to save 
read/write time.

#### Execution Function ####


```r
mr3 <- rhwatch(
  map      = map3,
  reduce   = reduce3,
  input    = rhfmt("/ln/tongx/housing/housing.txt", type = "text"),
  output   = rhfmt("/ln/tongx/housing/byCounty", type = "sequence"),
  mapred   = list(
    mapred.reduce.tasks = 10
  ),
  readback = FALSE
)
```

After the job completes successfully, we'll read the results from HDFS into our interactive R 
session as we did before.  This time, let's use the `max` argument to `rhread`, which specifies 
how many key-value pairs to read. The default value is -1, which means read in all key-value pairs.


```r
countySubsets <- rhread("/ln/tongx/housing/byCounty", max = 10)
```
```
Read 10 objects(31.39 KB) in 0.04 seconds
```
Suppose we want to see all 10 keys that we read.  Recall that key-value pairs are stored as a nested
list.  So what we want is the first element of each element in the list.  We can use `lapply` to get
them:


```r
keys <- unlist(lapply(countySubsets, "[[", 1))
keys
```
```
 [1] "01013" "01031" "01059" "01077" "01095" "01103" "01121" "04001" "05019" "05037"
```

Finally, let's check that we have the FIPS code, state name, and county name information saved as 
attributes of the data frame.


```r
attributes(countySubsets[[1]][[2]])
```
```
$names
[1] "date"             "units"            "list"             "selling"

$row.names
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
[33] 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 
[65] 65 66

$state
[1] "AL"

$county
[1] "Butler"

$class
[1] "data.frame"
```

### Compute County Means ###

We calculate the mean list price and mean sale price by county exactly the same way we 
calculated them by state. 

#### Map ####


```r
map4 <- expression({
  lapply(seq_along(map.keys), function(r) {
    value <- data.frame(
      listMean = mean(map.values[[r]]$list, na.rm = TRUE),
      saleMean = mean(map.values[[r]]$selling, na.rm = TRUE),
      stringsAsFactors = FALSE
    )
    value$state <- attr(map.values[[r]], "state")
    value$county <- attr(map.values[[r]], "county")
    value$FIPS <- attr(map.values[[r]], "FIPS")
    rhcollect(1, value)
  })
})
```

In the map expression, we create a single row data frame for each county. The data frame has five
columns: FIPS, listMean, saleMean, state, and county name. FIPS code, state name, and county name 
can be found in the attributes of each element of `map.values`. In order to combine all means by
county into one data frame, we will assign 1 to be the key for all intermediate key-value pairs.

#### Reduce ####


```r
reduce4 <- expression(
  pre = {
    countyMeans <- data.frame()
  },
  reduce = {
    countyMeans <- rbind(countyMeans, do.call(rbind, reduce.values))
  },
  post = {
    rhcollect(reduce.key, countyMeans)
  }
)
```

We can use the same reduce expression which we used to find the means by state. The final output
consists of one key-value pair, where the key is 1, and value is the data frame with all county
means.

#### Execution Function ####


```r
meansByCounty <- rhwatch(
  map      = map4,
  reduce   = reduce4,
  input    = rhfmt("/ln/tongx/housing/byCounty", type = "sequence"),
  output   = rhfmt("/ln/tongx/housing/meansByCounty", type = "sequence"),
  mapred   = list( 
    mapred.reduce.tasks = 1
  ),
  readback = TRUE
)
```

This time we specify only one reduce task by setting `mapred.reduce.tasks` to be 1.
Since we know there is only one
key-value pair in the output, we only need one output file. Eliminating unnecessary output files 
can speed up this job and future jobs which read its output.


```r
str(meansByCounty)
```
```
List of 1
 $ :List of 2
  ..$ : num 1
  ..$ :'data.frame' :    2883 obs. of  5 variables:
  .. ..$ fips       : chr [1:2883] "01005" "01023" "01041" "01069" ...
  .. ..$ listmean   : num [1:2883] 85.5 54.3 59.3 87.6 74.4 ...
  .. ..$ sellingmean: num [1:2883] NaN NaN NaN NaN NaN ...
  .. ..$ state      : chr [1:2883] "AL" "AL" "AL" "AL" ...
  .. ..$ county     : chr [1:2883] "Barbour" "Choctaw" "Crenshaw" "Houston" ...
```

```r
head(meansByCounty[[1]][[2]])
```
```
   FIPS listMean  saleMean state       county
1 01005 85.46983       NaN    AL      Barbour
2 01023 54.27378       NaN    AL      Choctaw
3 01041 59.29840       NaN    AL     Crenshaw
4 01069 87.64237       NaN    AL      Houston
5 01087 74.35722       NaN    AL        Macon
6 01113 70.92100       NaN    AL      Russell
```

### Read and Divide by Date ###

Same as before, we can create the subsets of dataset divided by date variable. But here one of
potential problem that we may face is that what if some of the subsets are very large? This is
a very general problem if we are dealing with a large and complex dataset. In fact, `RHIPE` is 
able to write any size of one subset to HDFS, but only can read one subset less than 256 Mb 
from HDFS. In other words, we are able to create any size of subset in a mapreduce job, but we
will get issue when we are trying to read and apply analysis method on oversize subset.

In this example, we are assuming that each subset by date is oversize. One way to overcome this
issue is that we are going to create multiple subsets for each month. 

#### Map ####


```r
map5 <- expression({
  lapply(seq_along(map.keys), function(r) {
    line = strsplit(map.values[[r]], ",")[[1]]
    key <- line[[4]]
    value <- as.data.frame(rbind(line[-4]), stringsAsFactors = FALSE)
    rhcollect(key, value)
  })
})
```

There is not too much difference in the map expression comparing with the map expression in previous 
two division sessions. We used the fourth column in each row of text file as the key which is the index
of month, and rest of columns saved as a data frame was collected as corresponding value.

#### Reduce ####


```r
reduce5 <- expression(
  pre = {
  },
  reduce = {
    date500 <- do.call(rbind, reduce.values)
    names(date500) <- c(
      "fips", "county", "state", 
      "units", "list", "selling"
    )
    date500$list <- as.numeric(date500$list)
    date500$selling <- as.numeric(date500$selling)
    rhcollect(reduce.key, date500)
  },
  post = {
  }
)
```

Here, the reduce expression is not similar as before. We left `pre` and `post` as empty. Recall that `pre` 
and `post` in reduce expression will be executed only once for each unique key. `reduce` part, on the other 
hand, will be executed repeatedly until all intermediate values associated with the current key have been 
processed. And we actually can control how many intermediate values to be executed in `reduce` every time. 
Suppose we want to execute 500 intermediate values for each key at one time, then the `reduce.values` is a 
list only includes 500 intermediate values for corresponding `reduce.key`. In other words, we use the 
property that intermediate values for a given key can be executed by part in `reduce` to create 
multiple subsets for a given key. Previously we only created one subset for a given key, like a state or 
county.

What we have done above in `reduce` is for a given key, for every 500 (later one we will illustrate where 
we specified this 500) intermediate value corresponding to this key, we created a data.frame named
`date500` which is the row-bind of all these 500 rows(recall that each value is a data frame with single 
row from map output) for this key. Then we collected `date500` as value with the key. Then is the next
500 rows for the same key, repeatedly until all rows for this key has been processed. If, for example, for 
one key we have 1324 values(rows), then finally we will get three key-value pairs with same key by using 
this reduce expression.

#### Execution Function ####


```r
mr5 <- rhwatch(
  map      = map5,
  reduce   = reduce5,
  input    = rhfmt("/ln/tongx/housing/housing.txt", type = "text"),
  output   = rhfmt("/ln/tongx/housing/bydate", type = "sequence"),
  mapred   = list(
    mapred.reduce.tasks = 10,
    rhipe_reduce_buff_size = 500
  ),
  readback = FALSE
)
```

In `rhwatch` this time, we add one more argument in `mapred` list. `rhipe_reduce_buff_size` is the 
argument used to control how many intermediate values for one key we want to process at one time. It is
where we specified that process 500 rows for one key at one time, as we mentioned in reduce expression.

Let us examine if we got what we want. Recall that we have 2,883 counties in total, and each county has
66 monthly observations. If we only create one subset for each month, there should be 66 subsets, each of
which is a data frame with 2,883 rows.


```r
rst <- rhread("/ln/tongx/housing/bydate")
```
```
Read 396 objects(8.85 MB) in 2.76 seconds
```

But we got 396 subsets in `rst`, which is 66 times 6. And if you examine the `rst` more carefully, it is
not hard to find that for each month, we created 6 subsets.


```r
rows <- unlist(lapply(rst, function(r){dim(r[[2]])[1]}))
rows
```
```
  [1] 500 500 500 500 500 383 500 500 500 500 500 383 500 500 500 500 500 383 500 500 500 500
 [23] 500 383 500 500 500 500 500 383 500 500 500 500 500 383 500 500 500 500 500 383 500 500
......
```
What we got above is the number of rows in each subset. We can see that we always have five `500` and one 
`383` as one replicate, which together will be the total number of rows for one month. This means we ran 
our `reduce` in reduce expression six times for each key.

### Compute Total Sold Units ###

We then would like to calculate the total sold units for every month based on the month division.
One thing we should keep in mind about the month division is that we have 6 subsets for each month.
And this will be the main reason that we want to consider one of the optimization method in `RHIPE`.

#### Combiner as an Optimization ####

Between the map phase and reduce phase of a MapReduce job, Hadoop sends all the intermediate values
for a given key to the reducer. The intermediate values for a given key are located on several compute 
nodes and need to be shuffled (sent across the network) to the node assigned the processing of that
intermediate key. This involves a lot of network transfer.

Some operations do not need access to all of the data (intermediate values), i.e they can compute on 
subsets and order does not matter, i.e associative and commutative operations. For example, the minimum, 
or the sum, of some numbers. In these cases, a combiner can be useful.

The idea of the combiner is that the reduce is first run locally on mapper outputs before they are sent 
for the final reduce. When the combiner is enabled, the reduction occurs just after the map phase on a 
subset of intermediate values for a given intermediate keys. The output of this is then sent to the 
reducer. This greatly reduces network transfer and accelerates the job speed, especially if the output 
from a map contains a lot of data.

#### Enabling Combiner in RHIPE ####

Combiner can be enabled in `RHIPE` by specifying `combiner = TRUE` when calling function `rhwatch()`.

To be able to use a combiner, our reduce expression needs to pass the same data type as it receives,
i.e the two arguments in the function rhcollect() in your map expression need to be of the same type as 
those in the function rhcollect() in your reduce expression. For example, if you pass a string as the key
and a numeric vector as the value in the map expression, you need to pass a string as key and a numeric 
vector as the value in the reduce expression as well.

We will demonstrate the usage of combiners through the following examples. First, let us look at how to 
achieve this task without using a combiner.

#### Without Combiner ####


```r
map6 <- expression({
  lapply(seq_along(map.keys), function(r) {
    key <- map.keys[[r]]
    value <- sum(as.numeric(map.values[[r]]$units), na.rm = TRUE)
    rhcollect(key, value)
  })
})
```

In the map expression, we calculated the sum of the sold units for each subset. Since month index is the
input key, and we have 6 subsets for each key, there will be 6 key-value pairs for each month. The value
of each key-value pair is the summation of number of sold units in corresponding subset. So totally there
will be 396 intermediate key-value pairs as output of map expression.


```r
reduce6 <- expression(
  pre = {
    count <- 0
  },
  reduce = {
    count <- count + sum(unlist(reduce.values), na.rm = TRUE)
  },
  post = {
    rhcollect(reduce.key, count)
  }
)
```

Then in the reduce expression, we grouped all 6 summation for each unique key(month index), and calculated
the overall summation for each month


```r
mr6 <- rhwatch(
  map       = map6,
  reduce    = reduce6,
  input     = rhfmt("/ln/tongx/housing/bydate", type = "sequence"),
  output    = rhfmt("/ln/tongx/housing/soldbydate", type = "sequence"),
  mapred    = list(
    mapred.reduce.tasks = 10
  ),
  jobname   = "total sold unit for each month",
  mon.sec   = 10,
  combiner  = FALSE,
  readback  = FALSE
)
```

The default for combiner is `FALSE`, which is what we have used before. We also specified two more
arguments in `rhwatch` which are `mon.sec` and `jobname`. Both of them are easy to understand. `mon.sec`
is a numeric integer which specifies how often in terms of second the job status will be reported in R after
the job is submitted. As we seen in "Division by State" session, after job is submitted, we would see job
status every 5 seconds which is the default value of `mon.sec`. `jobname` is a string that we can used to 
name our job. The default job name is the date and time when job is submitted.

#### With Combiner ####

Now let us see how to do the same task with combiner.


```r
mr7 <- rhwatch(
  map       = map6,
  reduce    = reduce6,
  input     = rhfmt("/ln/tongx/housing/bydate", type = "sequence"),
  output    = rhfmt("/ln/tongx/housing/soldbydate.combiner", type = "sequence"),
  mapred    = list(
    mapred.reduce.tasks = 10
  ),
  jobname   = "total sold unit for each month with combiner",
  mon.sec   = 10,
  combiner  = TRUE,
  readback  = FALSE
)
```

The map and reduce expression for combiner situation will be exactly same as the situation without combiner.
The only difference is that we changed the `combiner` argument in `rhwatch` to be `TRUE` to active the 
combiner. So what happened when combiner was active was nothing but each mapper(server or node that ran map
expression) run the reduce expression before it generated intermediate key-value pairs.

For this example,
one of the mapper may generated two out of six summation for one month. Without combiner, these two 
summation will be transferred with other four summation which calculated on another mapper to one reducer
(server or node that ran reduce expression). With combiner, however, these two summation will be first 
summed up together then only one key-value pairs will be transferred to the reducer instead of two. This
will be a huge difference with respect to transferring time when we have very many or very large size of 
values related to one key.

Finally, we can double check if combiner active or not will effect the final result.


```r
rst.comb <- rhread("/ln/tongx/housing/soldbydate.combiner")
rst <- rhread("/ln/tongx/housing/soldbydate")
identical(rst, rst.comb)
```
```
[1] TRUE
```

We can see that the final results from these two jobs are identical. We also can convert the results
to be a data frame with following code:


```r
df <- data.frame(  
  date  = sapply(rst, "[[", 1),
  total = sapply(rst, "[[", 2)
)
df <- df[order(df$date, decreasing = FALSE), ]  
df.comb <- data.frame(  
  date  = sapply(rst.comb, "[[", 1),
  total = sapply(rst.comb, "[[", 2)
)
df.comb <- df.comb[order(df.comb$date, decreasing = FALSE), ]
head(df.comb)
head(df)
```
```
   date  total
45    1 272879
50    2 201648
59    3 229829
51    4 180674
60    5 187276
1     6 219149
```

### Division by State from County Division ###

In the previous two divisions, we started from raw text data, where input key-value pairs were each row
of the text file. In some cases it might be more efficient to go from one subset division method to
another.  Either way, it's a good thing to know how to do.  In this section we'll recreate the 
division by state from the division by county.

Let's start by removing our existing state subsets from the HDFS so there aren't too many copies
floating around.  We can delete files from the HDFS using the `rhdel` function.  It only requires
one argument - the directory to be deleted - and it recursively deletes any subdirectories or files.

We can use `rhls` to see that the division by state is on the HDFS, right where we put it:


```r
rhls("/ln/tongx/housing")
```
```
Philip, you should put the output of rhls right here
```

We can delete it with the `rhdel` function:


```r
rhdel("/ln/tongx/housing/byState")
```
```
Philip, you should put the output of rhdel right here
```

And we can use `rhls` again to see that we were successful:


```r
rhls("/ln/tongx/housing")
```
```
Philip, you should put the new output of rhls right here
```

As long as we're talking about HDFS file management, let's try out a few more functions.  Suppose
we want to make a copy of the original text file on the HDFS.  We can use the
`rhcp` function.  It takes two arguments.  The first is the source, or the directory on HDFS we 
want to copy.  The second is the target, or location for the new copy.


```r
rhcp("/ln/tongx/housing/housing.txt", "/ln/tongx/housing/tmp/housing.txt")
rhls("/ln/tongx/housing/tmp"
```
```
Philip, you should put the output of rhls here
```

We can use `rhmv` to move the text file to a different folder.  This
function also takes two arguments, just like `rhcp`.


```r
rhmv("/ln/tongx/housing/tmp/housing.txt", "/ln/tongx/housing/tmp2/housing.txt")
rhls("/ln/tongx/housing/"
```
```
Philip, you should put the output of rhls here
```

That's enough file management.  Now let's recreate the division by state from the division by 
county, which is still on the HDFS.

#### Map ####


```r
map9 <- expression({
  lapply(seq_along(map.keys), function(r) {
    key <- attr(map.values[[r]], "state")
    value <- map.values[[r]]
    value$FIPS <- attr(map.keys[[r]], "FIPS")
    value$county <- attr(map.keys[[r]], "county")
    rhcollect(key, value)
  })
})
```

The most important part of this map expression is the key we assigned.  Our input key was the length 3
character vector containing FIPS code, county name, and state name.  Our output key is the state
name only.  This ensures that the data frames of all the counties belonging to a single state go to
a single reduce task, so that we can combine them into a single data frame for that state.  Meanwhile,
we also want to make the FIPS code and county name columns of our data frame again.

#### Reduce ####


```r
reduce9 <- expression(
  pre = {
    combine <- data.frame()
  },
  reduce = {
    combine <- rbind(combine, do.call(rbind, reduce.values))
  },
  post = {
    rhcollect(reduce.key, combine)
  }
)
```

We can use the same reduce expression that we used in the first division by state.  But keep in mind
that this time, the intermediate key-value pairs are different.  Before there was one key-value pair
for each line of the text file, but now there is one for each county.

#### Execution Function ####
 

```r
mr9 <- rhwatch(
  map      = map9,
  reduce   = reduce9,
  input    = rhfmt("/ln/tongx/housing/byCounty", type = "sequence"),
  output   = rhfmt("/ln/tongx/housing/byState", type = "sequence"),
  mapred   = list( 
    mapred.reduce.tasks = 10
  ),
  readback = FALSE,
  noeval = TRUE
)
```

Here we did something new in the `rhwatch` function.  We set the `noeval` argument to indicate
that we don't want to run this job, we just want to package it to be run later.

You won't see any output, because no commands have been sent to Hadoop.  Instead, the packaged
job has been stored in the `mr5` object.  When we're ready to run the job, we call `rhex()`.


```r
byState <- rhex(mr5, async = FALSE)
```

The first argument to `rhex` is the packaged job we just created.  The second argument `async` 
specifies whether the job should be run asynchronously.  If we set `async = FALSE` then we'll see
continuously updated job status information, and we won't be able to issue any further R commands
until the job completes.  If we set it to `TRUE`, which is the default, the job will run in the 
background while we continue to interact with our R session.  It's similar to the `\&` command in
a Linux shell.

After the job successfully completes, we'll have state subsets in the specified output folder
on the HDFS, just as before, but created in a different way.
