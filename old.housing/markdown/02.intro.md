# RHIPE Tutorial #

## Introduction ##

### Massive Data ###

Massive data sets have become commonplace today. Powerful hardware is readily available with a terabyte
of hard drive storage costing less than $150 and computers with many cores a norm. Today, the moderately 
adventurous scientist can connect two computers to form a distributed computing platform. Languages 
and software tools have made concurrent and distributed computing accessibly to the statistician.

It is important to stress that a massive data set is not just a single massive entity that needs to be
stored across multiple hard drives but rather the size of the data created during the steps of an analysis.
A 'small' 14 GB data set can easily become 190 GB as new data structures are created, or where multiple 
subsets/transformations are each saved as different data sets. Large data sets can come as they are 
or grow big because of the nature of the analysis. No analyst wants her research to be restricted 
because the computing infrastructure cannot keep up with the size or complexity.

### Hadoop ###

Hadoop is an open source programming framework for distributed computing with massive data sets using
a cluster of networked computers. It has changed the way many web companies work, bringing cluster 
computing to people with little knowledge of the intricacies of concurrent/distributed programming.
Part of the reason for its success is that it has a fixed programming paradigm. It somewhat restricts 
what the user can parallelize but once an algorithm has been written the 'Map-reduce way', concurrency
and distribution over a cluster comes for free.

It consists of two components: the Hadoop Distributed File System and Hadoop Map-reduce. They are based
on the Google File System and Google Map-reduce respectively. Companies using these include Amazon, 
Ebay, New York Times, Facebook to name a few. 

### Hadoop Distributed Filesystem ###

The Hadoop Distributed File System (HDFS) sits on top of the file system of a computer (called the 
local file system). It pools the hard drive space of a cluster or heterogeneous computers (e.g. different
hardware and operating systems) and provides a unified view to the user. For example, with a cluster
of 10 computers each with 1TB hard drive space available to Hadoop, the HDFS provides a user 10 TB 
of hard drive space. A single file can be bigger than maximum size on the local file system e.g. 2TB 
files can be saved on the HDFS. The HDFS is catered to large files and high throughput reads. Appends
to files are not allowed. Files written to the HDFS are chunked into blocks, each block is replicated
and saved on different cluster computers. This provides a measure of safety in case of transient or 
permanent computer failures. When a file is written to the HDFS, the client contacts the Namenode, a
computer that serves as the gateway to the HDFS. It also performs a lot of administrative tasks, such
as saving the mapping between a file and the location of its block across the cluster and so on. The 
Namenode tells the client which Datanodes (the computers that make up the HDFS) to store the data onto.
It also tells the client which Datanodes to read the data from when a read request is performed.

### Hadoop MapReduce ###

Concurrent programming is difficult to get right. As Herb Sutter put it:
> ...humans are quickly overwhelmed by concurrency and find it much more difficult to reason about 
concurrent than sequential code.

A statistician attempting concurrent programming needs to be aware of race conditions, deadlocks and
tools to prevent this: locks, semaphores, and mutually exclusive regions etc. An approach suggested 
by Sutter et al is to provide programming
models not functions that force the programmer to approach her algorithms differently. Once the 
programmer constructs the algorithm using this model, concurrency comes for free. The Map-reduce 
programming model is one example. Correctly coded Condor DAGS are another example.

Map-reduce consists of several 
embarrassingly parallel subsets which are evaluated in parallel. This is called the Map. There is a 
synchronization guard where intermediate data created at the end of the Map is exchanged between nodes
and another round of parallel computing starts, called the Reduce phase. In effect large scale 
simulation trials in which the programmer launches several thousands of independent computations is
an example of a Map. Retrieving and collating the results (usually done in the R console) is an 
example of a manual reduce.

In detail, the input to a Map-reduce computation is a set of *N* key,value pairs. The *N* pairs are 
partitioned into *S* arbitrary subsets. Each subset is a unit of computation and is assigned to one 
computing unit on the cluster. Thus the processing of the *S* subsets occurs in parallel. Each subset 
is processed by a user given function *M*, that takes a sequence of input key,value pairs and outputs
(one or many) intermediate key,value pairs. The Hadoop framework will partition the intermediate 
values by the intermediate key. That is intermediate values sharing the same intermediate key are 
grouped together. Once the map is complete, the if there are *M* distinct intermediate keys, a user 
given function *R*, will be given an intermediate key and all intermediate values associated with the 
same key. Each processing core is assigned a subset of intermediate keys to reduce and the reduction
of the *M* intermediate keys occurs in parallel. The function *R*, takes an intermediate key, a stream 
of associated intermediate values and returns a final key,value pair or pairs.

The R programmer has used Map-reduce ideas. For example, the `tapply` command splits a vector by a 
list of factors. This the map equivalent: each row of the vector is the value and the keys are the 
distinct levels of the list of factors. The reduce is the user given function applied to the partitions
of the vector. The `xyplot` function in `lattice` takes a formula e.g. F\sim Y|A*B, subsets the the 
data frame by the Cartesian product of the levels of A and B (the map) and displays each subset (the 
reduce). Hadoop Map-reduce generalizes this to a distributed level.


### RHIPE ###

The R and Hadoop Integrated Programming Environment is R package to compute across massive data sets,
create subsets, apply routines to subsets, produce displays on subsets across a cluster of computers
using the Hadoop DFS and Hadoop Map-reduce framework. This is accomplished from within the R 
environment, using standard R programming idioms. For efficiency reasons, the programming style is 
slightly different from that outlined in the previous section.

The native language of Hadoop is Java. Java is not suitable for rapid development such as is needed 
for a data analysis environment. [Hadoop Streaming](http://hadoop.apache.org/docs/r1.2.1/streaming.html)
bridges this gap. Users can write Map-reduce programs in other languages e.g. Python, Ruby, Perl which
is then deployed over the cluster. Hadoop Streaming then transfers the input data from Hadoop to the
user program.

Data analysis from R does not involve the user writing code to be deployed from the command line. The
analyst has massive data sitting in the background, she needs to create data, partition the data, 
compute summaries or displays. This need to be evaluated from the R environment and the results 
returned to R. Ideally not having to resort to the command line.

RHIPE is just that.

* RHIPE consist of several functions to interact with the HDFS e.g. save data sets, read data created 
by RHIPE Map-reduce, delete files.
* Compose and launch Map-reduce jobs from R using the command `rhwatch` and `rhex`. Monitor the status
using `rhstatus` which returns an R object. Stop jobs using `rhkill`.
+ Compute *side effect* files. The output of parallel computations may include the creation of PDF 
files, R data sets, CVS files etc. These will be copied by RHIPE to a central location on the HDFS 
removing the need for the user to copy them from the compute nodes or setting up a network file system.
+ Data sets that are created by RHIPE can be read using other languages such as Java, Perl, Python 
and C. The serialization format used by RHIPE (converting R objects to binary data) uses Googles
[Protocol Buffers](https://code.google.com/p/protobuf/) which is very fast and creates compact 
representations for R objects. Ideal for massive data sets.
+ Data sets created using RHIPE are *key-value* pairs. A key is mapped to a value. A Map-reduce 
computations iterates over the key,value pairs in parallel. If the output of a RHIPE job creates 
unique keys the output can be treated as a external-memory associative dictionary. RHIPE can thus be
used as a medium scale (millions of keys) disk based dictionary, which is useful for loading R 
objects into R.

In summary, the objective of RHIPE is to let the user focus on thinking about the data. The 
difficulties in distributing computations and storing data across a cluster are automatically handled
by RHIPE and Hadoop.

    
  



